{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"NLP experiments notebook.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyN4zY6Mv7pLPX6eJ3qcxi/Z"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S2glx4_zyniZ","executionInfo":{"status":"ok","timestamp":1621628334510,"user_tz":-60,"elapsed":13668,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"9af59d91-7ad2-4c1a-d8c1-f59c8562a23b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')#"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zy47tVgO_ovh","executionInfo":{"status":"ok","timestamp":1621628347947,"user_tz":-60,"elapsed":16951,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"b0a6445a-7c67-4676-a1c3-6683bc48eb91"},"source":["!pip install imbalanced-learn\n","!pip install contractions\n","!pip install torch torchvision\n","!pip install bayesian-optimization"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.4.3)\n","Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn) (1.0.1)\n","Collecting contractions\n","  Downloading https://files.pythonhosted.org/packages/11/4d/378ab91284c2c3a06ab475b287721c09b7951d5ecb3edf4ffb0e1e7a568a/contractions-0.0.49-py2.py3-none-any.whl\n","Collecting textsearch>=0.0.21\n","  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n","Collecting anyascii\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n","\u001b[K     |████████████████████████████████| 286kB 10.7MB/s \n","\u001b[?25hCollecting pyahocorasick\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n","\u001b[K     |████████████████████████████████| 327kB 36.1MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n","  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85399 sha256=a57feafdc977f23fe46a80fd11bedd634b362e2f89f6e6b3bc49f5ed82093dd5\n","  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n","Successfully built pyahocorasick\n","Installing collected packages: anyascii, pyahocorasick, textsearch, contractions\n","Successfully installed anyascii-0.2.0 contractions-0.0.49 pyahocorasick-1.4.2 textsearch-0.0.21\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Collecting bayesian-optimization\n","  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n","Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n","Building wheels for collected packages: bayesian-optimization\n","  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp37-none-any.whl size=11687 sha256=b85c5b87fd3cbe2a8d85c5e0627cb18bb6e61a7aff8defca21e8027a7b05ecc2\n","  Stored in directory: /root/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n","Successfully built bayesian-optimization\n","Installing collected packages: bayesian-optimization\n","Successfully installed bayesian-optimization-1.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8JjWcu2vy173","executionInfo":{"status":"ok","timestamp":1621628353271,"user_tz":-60,"elapsed":20769,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"da6a72b5-5cf3-4af7-d829-c323cbe3e045"},"source":["import json\n","import numpy as np\n","import pandas as pd\n","import contractions\n","from sklearn.model_selection import train_test_split\n","from imblearn.under_sampling import RandomUnderSampler\n","import re\n","import matplotlib.pyplot as plt\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer \n","\n","import string\n","import time\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","\n","import gensim\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms\n","\n","torch.manual_seed(0)\n","use_cuda = torch.cuda.is_available()\n","device = torch.device('cuda' if use_cuda else 'cpu')\n","    \n","print(\"Using GPU: {}\".format(use_cuda))\n","\n","from bayes_opt import BayesianOptimization"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","Using GPU: True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PlCKXlXGzM4q"},"source":["with open('/content/drive/My Drive/Individual Project/cleaned_depression_data.json') as f:\n","    depression_data = json.load(f)\n","  \n","with open('/content/drive/My Drive/Individual Project/cleaned_anxiety_data.json') as f:\n","    anxiety_data = json.load(f)\n","\n","with open('/content/drive/My Drive/Individual Project/cleaned_bipolar_data.json') as f:\n","    bipolar_data = json.load(f)\n","\n","with open('/content/drive/My Drive/Individual Project/cleaned_bpd_data.json') as f:\n","    bpd_data = json.load(f)\n","\n","with open('/content/drive/My Drive/Individual Project/cleaned_ocd_data.json') as f:\n","    ocd_data = json.load(f)\n","\n","with open('/content/drive/My Drive/Individual Project/cleaned_control_data.json') as f:\n","    control_data = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OQgfgsHSzXOU","executionInfo":{"elapsed":504,"status":"ok","timestamp":1616705205151,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"1c9eddb0-13f3-42ed-c940-cff0798da48c"},"source":["len(depression_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["121668"]},"metadata":{"tags":[]},"execution_count":195}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfvVIHNQztRa","executionInfo":{"elapsed":473,"status":"ok","timestamp":1616705207469,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"3f550e7b-4632-4ed3-85fa-55956f81fe7e"},"source":["len(anxiety_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["119288"]},"metadata":{"tags":[]},"execution_count":196}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDC5xZP4zzR7","executionInfo":{"elapsed":463,"status":"ok","timestamp":1616705209466,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"cd51504e-00af-412d-a8d1-b498fb135e6b"},"source":["len(bipolar_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["81133"]},"metadata":{"tags":[]},"execution_count":197}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcPWdQrzz4qS","executionInfo":{"elapsed":458,"status":"ok","timestamp":1616705210971,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"315ec560-d4c7-4cbc-814b-01f943e49ab3"},"source":["len(bpd_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["119714"]},"metadata":{"tags":[]},"execution_count":198}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m8BRZanKz82u","executionInfo":{"elapsed":478,"status":"ok","timestamp":1616705212283,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"ad83f5f2-ed3a-44ef-fcd2-f7a4ab9702f0"},"source":["len(ocd_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["55656"]},"metadata":{"tags":[]},"execution_count":199}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMugQz_J0Hq1","executionInfo":{"elapsed":573,"status":"ok","timestamp":1616705509161,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"7df32348-975b-43ee-af26-ebbe6ec36334"},"source":["len(control_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["228974"]},"metadata":{"tags":[]},"execution_count":208}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sr68_5xe2mtS","executionInfo":{"elapsed":667,"status":"ok","timestamp":1616705510758,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"e45a3c4f-4972-447c-97cf-99590b2d9226"},"source":["lengths = []\n","for i in range(0, len(control_data)):\n","  lengths.append(len(control_data[i]['selftext']))\n","\n","print(min(lengths))\n","print(np.percentile(lengths, 25))\n","print(np.percentile(lengths, 50))\n","print(np.percentile(lengths, 75))\n","print(max(lengths))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["51\n","79.0\n","122.0\n","204.0\n","999\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GeYWi1D41Ue","executionInfo":{"elapsed":449,"status":"ok","timestamp":1616701039235,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"e09369f9-9b38-4a72-9fc1-3df80fbf5b24"},"source":["print(ocd_data[678]['selftext'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Every so often, I'll get the idea that I'm being watched on camera in my home. It feels very similar to the Truman Show delusion. However, in my case, instead of feeling like I am on a reality show or that a large audience can see me, I feel that only one specific person is watching me. The specific person has changed over the years, but it's always only one person at a time and is someone that I know well in real life. \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BzoO1Iy7SHo","executionInfo":{"elapsed":915,"status":"ok","timestamp":1616706058951,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"e81e0032-eb49-46e5-9b12-04ddbdb92a73"},"source":["selftext = []\n","subreddit = []\n","\n","for i in range(0, len(depression_data)):\n","  selftext.append(depression_data[i]['selftext'])\n","  subreddit.append('depression')\n","\n","for i in range(0, len(anxiety_data)):\n","  selftext.append(anxiety_data[i]['selftext'])\n","  subreddit.append('anxiety')\n","\n","for i in range(0, len(ocd_data)):\n","  selftext.append(ocd_data[i]['selftext'])\n","  subreddit.append('ocd')\n","\n","for i in range(0, len(bipolar_data)):\n","  selftext.append(bipolar_data[i]['selftext'])\n","  subreddit.append('bipolar')\n","\n","for i in range(0, len(bpd_data)):\n","  selftext.append(bpd_data[i]['selftext'])\n","  subreddit.append('bpd')\n","\n","for i in range(0, len(control_data)):\n","  selftext.append(control_data[i]['selftext'])\n","  subreddit.append('control')\n","\n","print(len(selftext))\n","print(len(subreddit))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["726433\n","726433\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_u1HnT7_7QHO"},"source":["df = pd.DataFrame({'selftext':selftext, 'subreddit':subreddit})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0rhGH2t-kyX"},"source":["df = df.sample(frac = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ehm1xTEk9J0o","executionInfo":{"elapsed":664,"status":"ok","timestamp":1616706063242,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"9a7f7c8b-bef2-40f7-a2df-dbed0679dc1d"},"source":["len(df)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["726433"]},"metadata":{"tags":[]},"execution_count":237}]},{"cell_type":"code","metadata":{"id":"MVCfV8pB-mJ3"},"source":["x_train, x_test, y_train, y_test = train_test_split(df['selftext'], df['subreddit'], test_size=0.2)\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVPNxHxM_uDF"},"source":["train_df = pd.DataFrame({'selftext':x_train, 'subreddit':y_train})\n","val_df = pd.DataFrame({'selftext':x_val, 'subreddit':y_val})\n","test_df = pd.DataFrame({'selftext':x_test, 'subreddit':y_test})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSGDy9cUAGzZ"},"source":["train_df.to_csv('/content/drive/My Drive/Individual Project/train.csv', index=False)\n","val_df.to_csv('/content/drive/My Drive/Individual Project/validation.csv', index=False)\n","test_df.to_csv('/content/drive/My Drive/Individual Project/test.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n81X8z35MPMj"},"source":["new_df = pd.read_csv('/content/drive/My Drive/Individual Project/test.csv', lineterminator='\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kx2rj1Rp_y3d"},"source":["train_df = pd.read_csv('/content/drive/My Drive/Individual Project/train.csv', lineterminator='\\n')\n","val_df = pd.read_csv('/content/drive/My Drive/Individual Project/validation.csv', lineterminator='\\n')\n","test_df = pd.read_csv('/content/drive/My Drive/Individual Project/test.csv', lineterminator='\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"cfauI3FeOjw5","executionInfo":{"elapsed":489,"status":"error","timestamp":1616758574064,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"4b7cf9d4-6235-417f-ad0c-70ebd7c44957"},"source":["print(len(control_data))\n","print(len(anxiety_data))\n","print(len(depression_data))\n","print(len(ocd_data))\n","print(len(bpd_data))\n","print(len(bipolar_data))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-118-df0bb30fd80d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manxiety_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepression_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mocd_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpd_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'control_data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"C_SJ9_N0HWlp"},"source":["def remove_contractions(text):\n","  expanded_words = []    \n","  for word in text.split():\n","    # using contractions.fix to expand the shotened words\n","    expanded_words.append(contractions.fix(word))   \n","    \n","  expanded_text = ' '.join(expanded_words)\n","\n","  return expanded_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WomVfuAQ3cJ"},"source":["def get_wordnet_pos(word):\n","    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","\n","    return tag_dict.get(tag, wordnet.NOUN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PjQ4giWIg53"},"source":["def preprocess_text(text):\n","  text = remove_contractions(text)\n","  tokens = nltk.word_tokenize(text)\n","  tokens = [token.lower() for token in tokens]\n","  tokens = list(filter(lambda x: x not in string.punctuation, tokens))\n","  tokens = list(filter(lambda x: not x.isnumeric(), tokens))\n","  stop_words = set(stopwords.words('english'))\n","  tokens = list(filter(lambda x: x not in stop_words, tokens))\n","  lemmatizer = WordNetLemmatizer()\n","  tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n","\n","  return ' '.join(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-XMXs2Q5yL_"},"source":["# remove all characters which are not letters\n","def sanitise_text(text):\n","  words = text.split(' ')\n","  cleaned_words = []\n","  for word in words:\n","    word = re.sub(\"[^a-zA-Z]+\", \"\", word)\n","    word = word.strip()\n","    cleaned_words.append(word)\n","\n","  return \" \".join(cleaned_words).strip()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFYTy6YDUdm8"},"source":["train_df['preprocessed_selftext'] = train_df['selftext'].map(lambda x: preprocess_text(x))\n","val_df['preprocessed_selftext'] = val_df['selftext'].map(lambda x: preprocess_text(x))\n","test_df['preprocessed_selftext'] = test_df['selftext'].map(lambda x: preprocess_text(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEO3R13YVnlA"},"source":["train_df.to_csv('/content/drive/My Drive/Individual Project/train_preprocessed.csv', index=False)\n","val_df.to_csv('/content/drive/My Drive/Individual Project/validation_preprocessed.csv', index=False)\n","test_df.to_csv('/content/drive/My Drive/Individual Project/test_preprocessed.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GSxC5lDhtavB"},"source":["train_df = pd.read_csv('/content/drive/My Drive/Individual Project/train_preprocessed.csv', lineterminator='\\n')\n","val_df = pd.read_csv('/content/drive/My Drive/Individual Project/validation_preprocessed.csv', lineterminator='\\n')\n","test_df = pd.read_csv('/content/drive/My Drive/Individual Project/test_preprocessed.csv', lineterminator='\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":669},"id":"Yz_4472e1rti","executionInfo":{"status":"ok","timestamp":1621628361569,"user_tz":-60,"elapsed":23986,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"a103846c-808f-4bc3-a9c1-87c164feb03c"},"source":["train_df.head(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>selftext</th>\n","      <th>subreddit</th>\n","      <th>preprocessed_selftext</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>And just when everything was finally starting ...</td>\n","      <td>depression</td>\n","      <td>everything finally start work struggle past ye...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>We care for each other, but they these types o...</td>\n","      <td>bipolar</td>\n","      <td>care type mental illness something childish ha...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Hello everyone!  This seemed like the best pla...</td>\n","      <td>control</td>\n","      <td>hello everyone seem like best place ask advice...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>My best friend is 23. She has repeatedly told ...</td>\n","      <td>control</td>\n","      <td>best friend repeatedly told go mother wish mot...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I have trust issues due to past relationships ...</td>\n","      <td>depression</td>\n","      <td>trust issue due past relationship feel like fi...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Hello All I am currently facing a hearing with...</td>\n","      <td>control</td>\n","      <td>hello currently face hearing allege student co...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>My father died in mid December 33 years ago an...</td>\n","      <td>depression</td>\n","      <td>father die mid december year ago year easy dea...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>In the last 6 years something has broken in me...</td>\n","      <td>depression</td>\n","      <td>last year something broken feel people soon le...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Age: 28  Sex: Female  Height: 5'6\"  Weight: Ob...</td>\n","      <td>control</td>\n","      <td>age sex female height   weight obese sure rele...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Hi Im looking to gain a little weight anyone k...</td>\n","      <td>control</td>\n","      <td>hi look gain little weight anyone know good ap...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>I have always felt that Ive been slightly \"wro...</td>\n","      <td>bpd</td>\n","      <td>always felt slightly  wrong    whole life thou...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Hi! i'm new to reddit and i'm v grateful for f...</td>\n","      <td>anxiety</td>\n","      <td>hi new reddit v grateful find sub suffer w anx...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Hi. I recently have been diagnosed with BPD I ...</td>\n","      <td>bipolar</td>\n","      <td>hi recently diagnose bpd know bipolar anymore ...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>I am about to go to my drs apt and Im freaking...</td>\n","      <td>anxiety</td>\n","      <td>go drs apt freak feel like tell dr second medi...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Last year I met this wonderful woman and we im...</td>\n","      <td>bpd</td>\n","      <td>last year met wonderful woman immediately real...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Today, I was doing legs and during my set of s...</td>\n","      <td>control</td>\n","      <td>today leg set squat back feel straight add lot...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Character names  how do you come up with yours...</td>\n","      <td>control</td>\n","      <td>character name come much think matter think na...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>TW: suicidal thoughts   Ill have days where I ...</td>\n","      <td>bpd</td>\n","      <td>tw suicidal thought ill day cry anxiety roof i...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Hello /r/writing, long time lurker here...  So...</td>\n","      <td>control</td>\n","      <td>hello rwriting long time lurker  know painful ...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Okay, so this is embarrassing and I fucking...</td>\n","      <td>control</td>\n","      <td>okay embarrass fuck hate gp recommend get circ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             selftext  ...                              preprocessed_selftext\n","0   And just when everything was finally starting ...  ...  everything finally start work struggle past ye...\n","1   We care for each other, but they these types o...  ...  care type mental illness something childish ha...\n","2   Hello everyone!  This seemed like the best pla...  ...  hello everyone seem like best place ask advice...\n","3   My best friend is 23. She has repeatedly told ...  ...  best friend repeatedly told go mother wish mot...\n","4   I have trust issues due to past relationships ...  ...  trust issue due past relationship feel like fi...\n","5   Hello All I am currently facing a hearing with...  ...  hello currently face hearing allege student co...\n","6   My father died in mid December 33 years ago an...  ...  father die mid december year ago year easy dea...\n","7   In the last 6 years something has broken in me...  ...  last year something broken feel people soon le...\n","8   Age: 28  Sex: Female  Height: 5'6\"  Weight: Ob...  ...  age sex female height   weight obese sure rele...\n","9   Hi Im looking to gain a little weight anyone k...  ...  hi look gain little weight anyone know good ap...\n","10  I have always felt that Ive been slightly \"wro...  ...  always felt slightly  wrong    whole life thou...\n","11  Hi! i'm new to reddit and i'm v grateful for f...  ...  hi new reddit v grateful find sub suffer w anx...\n","12  Hi. I recently have been diagnosed with BPD I ...  ...  hi recently diagnose bpd know bipolar anymore ...\n","13  I am about to go to my drs apt and Im freaking...  ...  go drs apt freak feel like tell dr second medi...\n","14  Last year I met this wonderful woman and we im...  ...  last year met wonderful woman immediately real...\n","15  Today, I was doing legs and during my set of s...  ...  today leg set squat back feel straight add lot...\n","16  Character names  how do you come up with yours...  ...  character name come much think matter think na...\n","17  TW: suicidal thoughts   Ill have days where I ...  ...  tw suicidal thought ill day cry anxiety roof i...\n","18  Hello /r/writing, long time lurker here...  So...  ...  hello rwriting long time lurker  know painful ...\n","19     Okay, so this is embarrassing and I fucking...  ...  okay embarrass fuck hate gp recommend get circ...\n","\n","[20 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"u0-dbB0k4Scm"},"source":["train_df = train_df.dropna()\n","val_df = val_df.dropna()\n","test_df = test_df.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0DXCFWxRKZxb","executionInfo":{"status":"ok","timestamp":1621628361570,"user_tz":-60,"elapsed":21736,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"aa292416-8ddb-4736-fb48-c1e56ee91923"},"source":["print(len(train_df))\n","print(len(val_df))\n","print(len(test_df))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["223307\n","24812\n","27569\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ViW6wmNo8Vx","executionInfo":{"elapsed":552,"status":"ok","timestamp":1621332291867,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"f9827962-af7c-4147-c8ef-a1bef72a2874"},"source":["print(len(train_df[train_df['subreddit'] == 'control']))\n","print(len(train_df[train_df['subreddit'] == 'anxiety']))\n","print(len(train_df[train_df['subreddit'] == 'depression']))\n","print(len(train_df[train_df['subreddit'] == 'bpd']))\n","print(len(train_df[train_df['subreddit'] == 'bipolar']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["74409\n","36732\n","38752\n","34150\n","39264\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6l1Pc63d7Jpi"},"source":["train_df['preprocessed_selftext'] = train_df['preprocessed_selftext'].map(lambda x: sanitise_text(x))\n","val_df['preprocessed_selftext'] = val_df['preprocessed_selftext'].map(lambda x: sanitise_text(x))\n","test_df['preprocessed_selftext'] = test_df['preprocessed_selftext'].map(lambda x: sanitise_text(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"IpYpMmGu7XaN","executionInfo":{"elapsed":466,"status":"ok","timestamp":1616795980528,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":0},"outputId":"b4fb53a6-25a5-4660-deef-7e3f1c7ce8b3"},"source":["test_df['preprocessed_selftext'].iloc[11]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'impulse kick get anxious like must'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P9dt4Itt-CSq","executionInfo":{"status":"ok","timestamp":1621628367161,"user_tz":-60,"elapsed":1051,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"4b1b6d7f-1e68-480c-811b-38545e7991d9"},"source":["undersample = RandomUnderSampler(sampling_strategy = {'control': 34000, 'depression': 34000, 'anxiety': 34000, 'bpd': 34000, 'bipolar': 34000}, random_state=42)\n","X_under, y_under = undersample.fit_resample(train_df[['preprocessed_selftext']], train_df['subreddit'])\n","train_df_balanced = pd.DataFrame({'preprocessed_selftext':X_under[ : , 0], 'subreddit':y_under})\n","train_df_balanced['label'] = train_df_balanced['subreddit'].map({'control': 0, 'depression': 1, 'anxiety': 2, 'bpd': 3, 'bipolar': 4})\n","train_df_balanced = train_df_balanced.sample(frac = 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"T7bihFjbBXOx","executionInfo":{"status":"ok","timestamp":1621628367562,"user_tz":-60,"elapsed":424,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"bfb6372f-ff78-4463-a5b3-6b21d3d469bc"},"source":["train_df_balanced"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>preprocessed_selftext</th>\n","      <th>subreddit</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>18664</th>\n","      <td>soo even feel fine day lately feel dread body ...</td>\n","      <td>anxiety</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>53972</th>\n","      <td>med really suppress mania like full blown mani...</td>\n","      <td>bipolar</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>94783</th>\n","      <td>since go back therapy little year ago therapis...</td>\n","      <td>bpd</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>137325</th>\n","      <td>start let say post may chaotic probably type w...</td>\n","      <td>depression</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>47992</th>\n","      <td>feel pretty paranoid spa think might root evil...</td>\n","      <td>bipolar</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>97727</th>\n","      <td>last night big mental breakdown felt angry sad...</td>\n","      <td>bpd</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>99082</th>\n","      <td>anybody else experience typical pm talk full b...</td>\n","      <td>bpd</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>15192</th>\n","      <td>totally freak nothing besides eat  sure cause ...</td>\n","      <td>anxiety</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>62745</th>\n","      <td>know people say committedsectioned know manic ...</td>\n","      <td>bipolar</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>20438</th>\n","      <td>recently increase dose citalopram negative sid...</td>\n","      <td>anxiety</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>170000 rows × 3 columns</p>\n","</div>"],"text/plain":["                                    preprocessed_selftext   subreddit  label\n","18664   soo even feel fine day lately feel dread body ...     anxiety      2\n","53972   med really suppress mania like full blown mani...     bipolar      4\n","94783   since go back therapy little year ago therapis...         bpd      3\n","137325  start let say post may chaotic probably type w...  depression      1\n","47992   feel pretty paranoid spa think might root evil...     bipolar      4\n","...                                                   ...         ...    ...\n","97727   last night big mental breakdown felt angry sad...         bpd      3\n","99082   anybody else experience typical pm talk full b...         bpd      3\n","15192   totally freak nothing besides eat  sure cause ...     anxiety      2\n","62745   know people say committedsectioned know manic ...     bipolar      4\n","20438   recently increase dose citalopram negative sid...     anxiety      2\n","\n","[170000 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XyvAhMZbCB1X","executionInfo":{"elapsed":636,"status":"ok","timestamp":1621332318647,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"8e000f27-aa8a-4285-abd7-19b742900fe9"},"source":["print(len(val_df[val_df['subreddit'] == 'control']))\n","print(len(val_df[val_df['subreddit'] == 'anxiety']))\n","print(len(val_df[val_df['subreddit'] == 'depression']))\n","print(len(val_df[val_df['subreddit'] == 'bpd']))\n","print(len(val_df[val_df['subreddit'] == 'bipolar']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8142\n","4138\n","4323\n","3757\n","4452\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkTNDUSSCQxk","executionInfo":{"elapsed":949,"status":"ok","timestamp":1617017789333,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"9709614e-1c3a-4457-f880-2f2018ebf289"},"source":["undersample = RandomUnderSampler(sampling_strategy = {'depression': 4000, 'control': 4000, 'anxiety': 4000, 'ocd': 0, 'bpd': 4000, 'bipolar': 4000}, random_state=42)\n","X_under, y_under = undersample.fit_resample(val_df[['preprocessed_selftext']], val_df['subreddit'])\n","val_df_balanced = pd.DataFrame({'preprocessed_selftext':X_under[ : , 0], 'subreddit':y_under})\n","val_df_balanced['label'] = val_df_balanced['subreddit'].map({'depression': 1, 'control': 0, 'anxiety': 2, 'ocd': 5, 'bpd': 3, 'bipolar': 4})\n","val_df_balanced = val_df_balanced.sample(frac = 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4oelL8O0-3_a","executionInfo":{"status":"ok","timestamp":1621628420580,"user_tz":-60,"elapsed":8739,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"2eb084a5-36d9-47d0-b856-1e8a9ec81e77"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","count_vectorizer = CountVectorizer()\n","count_vectorizer.fit(train_df_balanced['preprocessed_selftext'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n","                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n","                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n","                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n","                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                tokenizer=None, vocabulary=None)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"fSLVBe_n__5G"},"source":["x_train = count_vectorizer.transform(train_df_balanced['preprocessed_selftext'])\n","y_train = train_df_balanced['label'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7dTKmmoPBwq3"},"source":["x_val = count_vectorizer.transform(val_df_balanced['preprocessed_selftext'])\n","y_val = val_df_balanced['label'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDCLnKuRA_u6","executionInfo":{"elapsed":9400,"status":"ok","timestamp":1617016649440,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"a5e14175-b820-4a53-a4b1-9e9dde3be33c"},"source":["x_val.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 81720)"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"mjmLTxNfAToo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621653736845,"user_tz":-60,"elapsed":25289775,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"3ca54c88-d11e-47b1-db75-d2d687ae7775"},"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","\n","classifier = SVC()\n","classifier.fit(x_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n","    max_iter=-1, probability=False, random_state=None, shrinking=True,\n","    tol=0.001, verbose=False)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"nxzTiawQBJKH"},"source":["y_pred = classifier.predict(x_val)\n","print(classification_report(y_val, y_pred))\n","#print(precision_score(y_train, y_pred))\n","#print(recall_score(y_train, y_pred))\n","#print(f1_score(y_train, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGa3LYKdq3H3"},"source":["print(confusion_matrix(y_val, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JH7tOWcmr2lL","executionInfo":{"elapsed":776,"status":"ok","timestamp":1617029378582,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"2e3307bc-9268-4f94-a7da-fbd3a22a1ffd"},"source":["print(len(test_df[test_df['subreddit'] == 'control']))\n","print(len(test_df[test_df['subreddit'] == 'anxiety']))\n","print(len(test_df[test_df['subreddit'] == 'depression']))\n","print(len(test_df[test_df['subreddit'] == 'bpd']))\n","print(len(test_df[test_df['subreddit'] == 'bipolar']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["45698\n","23927\n","24333\n","23975\n","16265\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"id":"jjNqnlYcrxtu","executionInfo":{"status":"ok","timestamp":1621628372680,"user_tz":-60,"elapsed":391,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"9ffb4f6e-a365-430c-8736-c6935441b174"},"source":["undersample = RandomUnderSampler(sampling_strategy = {'control': 4000, 'depression': 4000, 'anxiety': 4000, 'bpd': 4000, 'bipolar': 4000}, random_state=42)\n","X_under, y_under = undersample.fit_resample(test_df[['preprocessed_selftext']], test_df['subreddit'])\n","test_df_balanced = pd.DataFrame({'preprocessed_selftext':X_under[ : , 0], 'subreddit':y_under})\n","test_df_balanced['label'] = test_df_balanced['subreddit'].map({'control': 0, 'depression': 1, 'anxiety': 2, 'bpd': 3, 'bipolar': 4})\n","test_df_balanced = test_df_balanced.sample(frac=1)\n","test_df_balanced"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>preprocessed_selftext</th>\n","      <th>subreddit</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6174</th>\n","      <td>w bf year compatible matespartners great team ...</td>\n","      <td>bipolar</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>6541</th>\n","      <td>although day time fill beautiful thing make li...</td>\n","      <td>bipolar</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>8959</th>\n","      <td>opening someone something personal respond  lo...</td>\n","      <td>bpd</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3526</th>\n","      <td>doctor prescribed paxil mg today say look chan...</td>\n","      <td>anxiety</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>17770</th>\n","      <td>year bad last honest year surround family yet ...</td>\n","      <td>depression</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1508</th>\n","      <td>hi already post relationship know tie anxiety ...</td>\n","      <td>anxiety</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>14582</th>\n","      <td>back may complex flood terribly car flood cert...</td>\n","      <td>control</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19774</th>\n","      <td>edit know post right sub confessional first th...</td>\n","      <td>depression</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2272</th>\n","      <td>tw medication near miss car accident hi everyo...</td>\n","      <td>anxiety</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11802</th>\n","      <td>fp somebody new  seem bring best  hurt really ...</td>\n","      <td>bpd</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20000 rows × 3 columns</p>\n","</div>"],"text/plain":["                                   preprocessed_selftext   subreddit  label\n","6174   w bf year compatible matespartners great team ...     bipolar      4\n","6541   although day time fill beautiful thing make li...     bipolar      4\n","8959   opening someone something personal respond  lo...         bpd      3\n","3526   doctor prescribed paxil mg today say look chan...     anxiety      2\n","17770  year bad last honest year surround family yet ...  depression      1\n","...                                                  ...         ...    ...\n","1508   hi already post relationship know tie anxiety ...     anxiety      2\n","14582  back may complex flood terribly car flood cert...     control      0\n","19774  edit know post right sub confessional first th...  depression      1\n","2272   tw medication near miss car accident hi everyo...     anxiety      2\n","11802  fp somebody new  seem bring best  hurt really ...         bpd      3\n","\n","[20000 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"ETf5DYvPtWro"},"source":["x_test = count_vectorizer.transform(test_df_balanced['preprocessed_selftext'])\n","y_test = test_df_balanced['label'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNHlziHmtRrR","executionInfo":{"status":"ok","timestamp":1621654781525,"user_tz":-60,"elapsed":1044657,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"4cc18dac-fe51-49f4-ef03-e69676dc4962"},"source":["y_pred = classifier.predict(x_test)\n","print(classification_report(y_test, y_pred))\n","#print(precision_score(y_train, y_pred))\n","#print(recall_score(y_train, y_pred))\n","#print(f1_score(y_train, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.80      0.92      0.86      4000\n","           1       0.66      0.78      0.72      4000\n","           2       0.83      0.78      0.80      4000\n","           3       0.79      0.69      0.74      4000\n","           4       0.82      0.71      0.76      4000\n","\n","    accuracy                           0.78     20000\n","   macro avg       0.78      0.78      0.78     20000\n","weighted avg       0.78      0.78      0.78     20000\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ZYHDOW0te22","executionInfo":{"status":"ok","timestamp":1621654781526,"user_tz":-60,"elapsed":1044646,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"830a6a60-3f94-42a1-c339-e09615f631fb"},"source":["print(confusion_matrix(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[3695  128   70   68   39]\n"," [ 226 3122  200  272  180]\n"," [ 227  388 3108  134  143]\n"," [ 220  584  184 2753  259]\n"," [ 236  490  176  252 2846]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xq1c7qYrto5d","executionInfo":{"status":"ok","timestamp":1621654781526,"user_tz":-60,"elapsed":1044637,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"dc73d951-69cf-4799-98a8-54514fa5fb49"},"source":["sum(sum(confusion_matrix(y_test, y_pred)))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20000"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"_RkBmQc1XquL"},"source":["**Feed forward neural network**"]},{"cell_type":"code","metadata":{"id":"qM5VnQX_iTVg"},"source":["model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/Individual Project/GoogleNews-vectors-negative300.bin', binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9InDZjA-jHbv"},"source":["x_train = train_df_balanced['preprocessed_selftext'].values\n","y_train = train_df_balanced['label'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JP7CIsGone7W","executionInfo":{"elapsed":617,"status":"ok","timestamp":1617021778388,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"9324fed5-7ae7-429c-a779-9cf1871c1ca2"},"source":["len(x_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["250000"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"c5uHA_m3k3EJ"},"source":["embeddings = []\n","for i in range(0, len(x_train)):\n","  embedding_sum = np.zeros(300)\n","  words = x_train[i].split(' ')\n","  for word in words:\n","    if word in model:\n","      embedding_sum = embedding_sum + model[word]\n","  embedding_sum = embedding_sum / len(words)\n","\n","  embeddings.append(embedding_sum)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_L9Fm5O6lz2R","executionInfo":{"elapsed":20896,"status":"ok","timestamp":1617021801049,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"c92437f6-8a7f-430a-f681-793d22a48db6"},"source":["len(x_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["250000"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"code","metadata":{"id":"zCyh9g2slri6"},"source":["x_train_embeddings = np.array(embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P7fXysxCl91B","executionInfo":{"elapsed":19408,"status":"ok","timestamp":1617021801706,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"c020793e-5dc6-4da8-982a-ea5673fd4792"},"source":["x_train_embeddings.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(250000, 300)"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXpzfZgFt8JC","executionInfo":{"elapsed":672,"status":"ok","timestamp":1617017954666,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"6926efe8-609f-4d3b-8966-f9e1c30c51c7"},"source":["x_train_embeddings[0][12 : 45]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.02527214,  0.0377664 , -0.05615257,  0.04805621,  0.05744665,\n","        0.04408174, -0.06353401, -0.07883857,  0.06381944,  0.02764803,\n","        0.0613143 ,  0.02490952,  0.0204755 ,  0.00608377, -0.04036937,\n","        0.02257386,  0.03334674, -0.00764735, -0.04194013,  0.03487621,\n","       -0.02846123, -0.02564778, -0.04874555, -0.04200116,  0.03236142,\n","        0.00402114,  0.00728473,  0.09419161,  0.06982422, -0.09816607,\n","        0.19156422,  0.04734892,  0.00405704])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"lDvz8lVktz00"},"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","scaler.fit(x_train_embeddings)\n","\n","x_train_embeddings = scaler.transform(x_train_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UT4dEMWtmefd"},"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class DepressionDataset(Dataset):\n","\n","   def __init__(self, x, y):\n","      self.x = x\n","      self.y = y\n","      \n","   def __len__(self):\n","      return len(self.x)\n","      \n","   def __getitem__(self, idx):\n","      return self.x[idx], self.y[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yoNNhkbxmiGL"},"source":["train = DepressionDataset(x_train_embeddings, y_train)\n","train_dataloader = DataLoader(train, batch_size=207)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zWcf3iLNXuYI"},"source":["class MLP(nn.Module):\n","    # Define entities containing model weights in the constructor.\n","    def __init__(self, n_hidden, dropout_p):\n","        super().__init__()\n","        self.linear1 = nn.Linear(\n","            in_features=300, out_features=n_hidden, bias=True\n","        )\n","        self.linear2 = nn.Linear(\n","            in_features=n_hidden, out_features=n_hidden, bias=True\n","        )\n","        self.linear3 = nn.Linear(\n","            in_features=n_hidden, out_features=5, bias=True\n","        )\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    # Then, all you need to do is implement a `forward` method to define the\n","    # computation that takes place on the forward pass. A corresponding\n","    # `backward` method, which computes gradients, is automatically defined!\n","    def forward(self, inputs):\n","        h = self.linear1(inputs)\n","        h = self.dropout(h)\n","        h = torch.relu(h)\n","        h = self.linear2(h)\n","        h = self.dropout(h)\n","        h = torch.relu(h)\n","        out = self.linear3(h)\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iiXMiTNzfjmx"},"source":["def train_model(model, train_loader, optimizer, epoch, log_interval=500):\n","    \"\"\"\n","    A utility function that performs a basic training loop.\n","\n","    For each batch in the training set, fetched using `train_loader`:\n","        - Zeroes the gradient used by `optimizer`\n","        - Performs forward pass through `model` on the given batch\n","        - Computes loss on batch\n","        - Performs backward pass\n","        - `optimizer` updates model parameters using computed gradient\n","\n","    Prints the training loss on the current batch every `log_interval` batches.\n","    \"\"\"\n","    model.train()\n","\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        # We need to send our batch to the device we are using. If this is not\n","        # it will default to using the CPU.\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        inputs = inputs.float()\n","        targets = targets.float()\n","        \n","        # Zeroes the gradient used by `optimizer`; NOTE: if this is not done,\n","        # then gradients will be accumulated across batches!\n","        optimizer.zero_grad()\n","\n","        # Performs forward pass through `model` on the given batch; equivalent\n","        # to `model.forward(inputs)`. Any information needed to compute\n","        # gradients is automatically thanks to autograd running under the hood.\n","        #print(\"ha\")\n","        outputs = model(inputs)\n","        #print(\"ha\")\n","        #print(inputs.size())\n","        #print(targets.size())\n","        #print(outputs.size())\n","\n","        loss_fn = nn.CrossEntropyLoss()\n","        targets = targets.long()\n","        loss = loss_fn(outputs, targets)\n","\n","        # Performs backward pass; steps backward through the computation graph,\n","        # computing the gradient of the loss wrt model parameters.\n","        loss.backward()\n","\n","        # `optimizer` updates model parameters using computed gradient.\n","        optimizer.step()\n","\n","        # Prints the training loss on the current batch every `log_interval`\n","        # batches.\n","        if batch_idx % log_interval == 0:\n","            print(\n","                \"Train Epoch: {:02d} -- Batch: {:03d} -- Loss: {:.4f}\".format(\n","                    epoch,\n","                    batch_idx,\n","                    # Calling `loss.item()` returns the scalar loss as a Python\n","                    # number.\n","                    loss.item(),\n","                )\n","            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-MN5XKGmt7g","executionInfo":{"elapsed":3650,"status":"ok","timestamp":1617022348598,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"cf803412-aaea-423e-da47-50e3b5d437c7"},"source":["classifier = MLP(n_hidden=199, dropout_p=0.01).to(device)\n","optimizer = optim.SGD(classifier.parameters(), lr=0.07)\n","# Train-test loop\n","for epoch in range(1):\n","  train_model(classifier, train_dataloader, optimizer, epoch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Epoch: 00 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.0962\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.0673\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"94PzRtlfrA7R"},"source":["x_test = test_df_balanced['preprocessed_selftext'].values\n","y_test = test_df_balanced['label'].values\n","\n","embeddings = []\n","for i in range(0, len(x_test)):\n","  embedding_sum = np.zeros(300)\n","  words = x_test[i].split(' ')\n","  for word in words:\n","    if word in model:\n","      embedding_sum = embedding_sum + model[word]\n","  embedding_sum = embedding_sum / len(words)\n","\n","  embeddings.append(embedding_sum)\n","\n","x_test_embeddings = np.array(embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzWpQwRvrZm8","executionInfo":{"elapsed":7115,"status":"ok","timestamp":1617021840567,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"410834d2-589d-41bf-84c4-91b4b1b39dde"},"source":["x_test_embeddings.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(80000, 300)"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"code","metadata":{"id":"B91sDXhnux9G"},"source":["x_test_embeddings = scaler.transform(x_test_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CKeZa_SrbpZ"},"source":["classifier.eval()\n","y_pred = classifier(torch.from_numpy(x_test_embeddings).float().to(device))\n","y_pred = y_pred.cpu().detach().numpy()\n","outputs = []\n","for i in range(0, len(y_pred)):\n","  outputs.append(y_pred[i].argmax())\n","outputs = np.array(outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kw3fK5hXsmr4","executionInfo":{"elapsed":588,"status":"ok","timestamp":1617020996514,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"a1ac727d-a537-4708-853e-cc585b107b7d"},"source":["outputs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3, 2, 1, ..., 0, 2, 0])"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jIh3jOmrtHb","executionInfo":{"elapsed":646,"status":"ok","timestamp":1617022369260,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"98cce399-854a-46e6-9fe5-bb8034eedb0e"},"source":["print(classification_report(y_test, outputs))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.71      0.84      0.77     16000\n","           1       0.46      0.64      0.53     16000\n","           2       0.62      0.55      0.58     16000\n","           3       0.58      0.45      0.51     16000\n","           4       0.62      0.47      0.54     16000\n","\n","    accuracy                           0.59     80000\n","   macro avg       0.60      0.59      0.59     80000\n","weighted avg       0.60      0.59      0.59     80000\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xbwwl1sJ5J6n"},"source":["print(confusion_matrix(y_train, outputs))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BlzrUMyj-L_n"},"source":["def bayesian_train_model(n_hidden, dropout_p, batch_size, lr):\n","  classifier = MLP(n_hidden=int(n_hidden), dropout_p=dropout_p).to(device)\n","  optimizer = optim.SGD(classifier.parameters(), lr=lr)\n","  \n","  train = DepressionDataset(x_train_embeddings, y_train)\n","  train_dataloader = DataLoader(train, batch_size=int(batch_size))\n","\n","  for epoch in range(100):\n","    train_model(classifier, train_dataloader, optimizer, epoch)\n","\n","  classifier.eval()\n","  y_pred = classifier(torch.from_numpy(x_test_embeddings).float().to(device))\n","  y_pred = y_pred.cpu().detach().numpy()\n","  outputs = []\n","  for i in range(0, len(y_pred)):\n","    outputs.append(y_pred[i].argmax())\n","  outputs = np.array(outputs)\n","\n","  return accuracy_score(y_test, outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"04mG7v8__rz4"},"source":["bounds = {\n","    'n_hidden':(50, 250),\n","    'dropout_p': (0, 0.5),\n","    'batch_size':(64, 512),\n","    'lr':(0.001, 0.1)\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0S1P2IBsALle","outputId":"d13fa023-7eef-44a6-a319-f47f39ed854d"},"source":["bayesian_optimizer = BayesianOptimization(\n","    f=bayesian_train_model,\n","    pbounds=bounds,\n","    random_state=1,\n",")\n","bayesian_optimizer.maximize(init_points=10, n_iter=50)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["|   iter    |  target   | batch_... | dropout_p |    lr     | n_hidden  |\n","-------------------------------------------------------------------------\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6205\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.5998\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5949\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.5744\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5789\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.5399\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5538\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5037\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5163\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4709\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4804\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.4311\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4492\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4000\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4252\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.3704\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4248\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.3772\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4345\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.3523\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.3895\n","Train Epoch: 10 -- Batch: 500 -- Loss: 1.3331\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.3712\n","Train Epoch: 11 -- Batch: 500 -- Loss: 1.3223\n","Train Epoch: 12 -- Batch: 000 -- Loss: 1.3613\n","Train Epoch: 12 -- Batch: 500 -- Loss: 1.3005\n","Train Epoch: 13 -- Batch: 000 -- Loss: 1.3575\n","Train Epoch: 13 -- Batch: 500 -- Loss: 1.2973\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.3232\n","Train Epoch: 14 -- Batch: 500 -- Loss: 1.3379\n","Train Epoch: 15 -- Batch: 000 -- Loss: 1.3559\n","Train Epoch: 15 -- Batch: 500 -- Loss: 1.2875\n","Train Epoch: 16 -- Batch: 000 -- Loss: 1.2816\n","Train Epoch: 16 -- Batch: 500 -- Loss: 1.2545\n","Train Epoch: 17 -- Batch: 000 -- Loss: 1.3129\n","Train Epoch: 17 -- Batch: 500 -- Loss: 1.2505\n","Train Epoch: 18 -- Batch: 000 -- Loss: 1.3106\n","Train Epoch: 18 -- Batch: 500 -- Loss: 1.2743\n","Train Epoch: 19 -- Batch: 000 -- Loss: 1.2924\n","Train Epoch: 19 -- Batch: 500 -- Loss: 1.2631\n","Train Epoch: 20 -- Batch: 000 -- Loss: 1.3400\n","Train Epoch: 20 -- Batch: 500 -- Loss: 1.2370\n","Train Epoch: 21 -- Batch: 000 -- Loss: 1.3259\n","Train Epoch: 21 -- Batch: 500 -- Loss: 1.2600\n","Train Epoch: 22 -- Batch: 000 -- Loss: 1.3130\n","Train Epoch: 22 -- Batch: 500 -- Loss: 1.2330\n","Train Epoch: 23 -- Batch: 000 -- Loss: 1.2853\n","Train Epoch: 23 -- Batch: 500 -- Loss: 1.2263\n","Train Epoch: 24 -- Batch: 000 -- Loss: 1.2559\n","Train Epoch: 24 -- Batch: 500 -- Loss: 1.2457\n","Train Epoch: 25 -- Batch: 000 -- Loss: 1.2696\n","Train Epoch: 25 -- Batch: 500 -- Loss: 1.2366\n","Train Epoch: 26 -- Batch: 000 -- Loss: 1.2650\n","Train Epoch: 26 -- Batch: 500 -- Loss: 1.2240\n","Train Epoch: 27 -- Batch: 000 -- Loss: 1.2523\n","Train Epoch: 27 -- Batch: 500 -- Loss: 1.2320\n","Train Epoch: 28 -- Batch: 000 -- Loss: 1.2643\n","Train Epoch: 28 -- Batch: 500 -- Loss: 1.2360\n","Train Epoch: 29 -- Batch: 000 -- Loss: 1.2608\n","Train Epoch: 29 -- Batch: 500 -- Loss: 1.2124\n","Train Epoch: 30 -- Batch: 000 -- Loss: 1.2410\n","Train Epoch: 30 -- Batch: 500 -- Loss: 1.2015\n","Train Epoch: 31 -- Batch: 000 -- Loss: 1.2456\n","Train Epoch: 31 -- Batch: 500 -- Loss: 1.1842\n","Train Epoch: 32 -- Batch: 000 -- Loss: 1.2424\n","Train Epoch: 32 -- Batch: 500 -- Loss: 1.1826\n","Train Epoch: 33 -- Batch: 000 -- Loss: 1.2182\n","Train Epoch: 33 -- Batch: 500 -- Loss: 1.1757\n","Train Epoch: 34 -- Batch: 000 -- Loss: 1.2331\n","Train Epoch: 34 -- Batch: 500 -- Loss: 1.2001\n","Train Epoch: 35 -- Batch: 000 -- Loss: 1.2615\n","Train Epoch: 35 -- Batch: 500 -- Loss: 1.1730\n","Train Epoch: 36 -- Batch: 000 -- Loss: 1.2366\n","Train Epoch: 36 -- Batch: 500 -- Loss: 1.1949\n","Train Epoch: 37 -- Batch: 000 -- Loss: 1.2600\n","Train Epoch: 37 -- Batch: 500 -- Loss: 1.1812\n","Train Epoch: 38 -- Batch: 000 -- Loss: 1.1993\n","Train Epoch: 38 -- Batch: 500 -- Loss: 1.1828\n","Train Epoch: 39 -- Batch: 000 -- Loss: 1.2374\n","Train Epoch: 39 -- Batch: 500 -- Loss: 1.1753\n","Train Epoch: 40 -- Batch: 000 -- Loss: 1.2107\n","Train Epoch: 40 -- Batch: 500 -- Loss: 1.1991\n","Train Epoch: 41 -- Batch: 000 -- Loss: 1.2065\n","Train Epoch: 41 -- Batch: 500 -- Loss: 1.1454\n","Train Epoch: 42 -- Batch: 000 -- Loss: 1.2368\n","Train Epoch: 42 -- Batch: 500 -- Loss: 1.1302\n","Train Epoch: 43 -- Batch: 000 -- Loss: 1.2263\n","Train Epoch: 43 -- Batch: 500 -- Loss: 1.1942\n","Train Epoch: 44 -- Batch: 000 -- Loss: 1.2228\n","Train Epoch: 44 -- Batch: 500 -- Loss: 1.1981\n","Train Epoch: 45 -- Batch: 000 -- Loss: 1.2121\n","Train Epoch: 45 -- Batch: 500 -- Loss: 1.1451\n","Train Epoch: 46 -- Batch: 000 -- Loss: 1.2360\n","Train Epoch: 46 -- Batch: 500 -- Loss: 1.1844\n","Train Epoch: 47 -- Batch: 000 -- Loss: 1.2235\n","Train Epoch: 47 -- Batch: 500 -- Loss: 1.1706\n","Train Epoch: 48 -- Batch: 000 -- Loss: 1.2183\n","Train Epoch: 48 -- Batch: 500 -- Loss: 1.1333\n","Train Epoch: 49 -- Batch: 000 -- Loss: 1.2211\n","Train Epoch: 49 -- Batch: 500 -- Loss: 1.2292\n","Train Epoch: 50 -- Batch: 000 -- Loss: 1.2185\n","Train Epoch: 50 -- Batch: 500 -- Loss: 1.1566\n","Train Epoch: 51 -- Batch: 000 -- Loss: 1.1836\n","Train Epoch: 51 -- Batch: 500 -- Loss: 1.1546\n","Train Epoch: 52 -- Batch: 000 -- Loss: 1.2349\n","Train Epoch: 52 -- Batch: 500 -- Loss: 1.1518\n","Train Epoch: 53 -- Batch: 000 -- Loss: 1.2448\n","Train Epoch: 53 -- Batch: 500 -- Loss: 1.1660\n","Train Epoch: 54 -- Batch: 000 -- Loss: 1.1737\n","Train Epoch: 54 -- Batch: 500 -- Loss: 1.1321\n","Train Epoch: 55 -- Batch: 000 -- Loss: 1.1840\n","Train Epoch: 55 -- Batch: 500 -- Loss: 1.1574\n","Train Epoch: 56 -- Batch: 000 -- Loss: 1.2309\n","Train Epoch: 56 -- Batch: 500 -- Loss: 1.1736\n","Train Epoch: 57 -- Batch: 000 -- Loss: 1.1785\n","Train Epoch: 57 -- Batch: 500 -- Loss: 1.1297\n","Train Epoch: 58 -- Batch: 000 -- Loss: 1.1911\n","Train Epoch: 58 -- Batch: 500 -- Loss: 1.1542\n","Train Epoch: 59 -- Batch: 000 -- Loss: 1.1886\n","Train Epoch: 59 -- Batch: 500 -- Loss: 1.1903\n","Train Epoch: 60 -- Batch: 000 -- Loss: 1.1830\n","Train Epoch: 60 -- Batch: 500 -- Loss: 1.1215\n","Train Epoch: 61 -- Batch: 000 -- Loss: 1.1956\n","Train Epoch: 61 -- Batch: 500 -- Loss: 1.1570\n","Train Epoch: 62 -- Batch: 000 -- Loss: 1.1939\n","Train Epoch: 62 -- Batch: 500 -- Loss: 1.1607\n","Train Epoch: 63 -- Batch: 000 -- Loss: 1.1971\n","Train Epoch: 63 -- Batch: 500 -- Loss: 1.1093\n","Train Epoch: 64 -- Batch: 000 -- Loss: 1.2025\n","Train Epoch: 64 -- Batch: 500 -- Loss: 1.1327\n","Train Epoch: 65 -- Batch: 000 -- Loss: 1.2130\n","Train Epoch: 65 -- Batch: 500 -- Loss: 1.1243\n","Train Epoch: 66 -- Batch: 000 -- Loss: 1.2088\n","Train Epoch: 66 -- Batch: 500 -- Loss: 1.1375\n","Train Epoch: 67 -- Batch: 000 -- Loss: 1.1681\n","Train Epoch: 67 -- Batch: 500 -- Loss: 1.1432\n","Train Epoch: 68 -- Batch: 000 -- Loss: 1.1758\n","Train Epoch: 68 -- Batch: 500 -- Loss: 1.1316\n","Train Epoch: 69 -- Batch: 000 -- Loss: 1.1388\n","Train Epoch: 69 -- Batch: 500 -- Loss: 1.1300\n","Train Epoch: 70 -- Batch: 000 -- Loss: 1.2116\n","Train Epoch: 70 -- Batch: 500 -- Loss: 1.1405\n","Train Epoch: 71 -- Batch: 000 -- Loss: 1.1720\n","Train Epoch: 71 -- Batch: 500 -- Loss: 1.1436\n","Train Epoch: 72 -- Batch: 000 -- Loss: 1.1892\n","Train Epoch: 72 -- Batch: 500 -- Loss: 1.1525\n","Train Epoch: 73 -- Batch: 000 -- Loss: 1.1611\n","Train Epoch: 73 -- Batch: 500 -- Loss: 1.1263\n","Train Epoch: 74 -- Batch: 000 -- Loss: 1.1626\n","Train Epoch: 74 -- Batch: 500 -- Loss: 1.1155\n","Train Epoch: 75 -- Batch: 000 -- Loss: 1.1651\n","Train Epoch: 75 -- Batch: 500 -- Loss: 1.1220\n","Train Epoch: 76 -- Batch: 000 -- Loss: 1.1973\n","Train Epoch: 76 -- Batch: 500 -- Loss: 1.1712\n","Train Epoch: 77 -- Batch: 000 -- Loss: 1.1945\n","Train Epoch: 77 -- Batch: 500 -- Loss: 1.1187\n","Train Epoch: 78 -- Batch: 000 -- Loss: 1.1846\n","Train Epoch: 78 -- Batch: 500 -- Loss: 1.1353\n","Train Epoch: 79 -- Batch: 000 -- Loss: 1.1933\n","Train Epoch: 79 -- Batch: 500 -- Loss: 1.0968\n","Train Epoch: 80 -- Batch: 000 -- Loss: 1.1859\n","Train Epoch: 80 -- Batch: 500 -- Loss: 1.1547\n","Train Epoch: 81 -- Batch: 000 -- Loss: 1.1631\n","Train Epoch: 81 -- Batch: 500 -- Loss: 1.1359\n","Train Epoch: 82 -- Batch: 000 -- Loss: 1.1775\n","Train Epoch: 82 -- Batch: 500 -- Loss: 1.0930\n","Train Epoch: 83 -- Batch: 000 -- Loss: 1.1964\n","Train Epoch: 83 -- Batch: 500 -- Loss: 1.0908\n","Train Epoch: 84 -- Batch: 000 -- Loss: 1.1798\n","Train Epoch: 84 -- Batch: 500 -- Loss: 1.0679\n","Train Epoch: 85 -- Batch: 000 -- Loss: 1.1556\n","Train Epoch: 85 -- Batch: 500 -- Loss: 1.1582\n","Train Epoch: 86 -- Batch: 000 -- Loss: 1.1390\n","Train Epoch: 86 -- Batch: 500 -- Loss: 1.1064\n","Train Epoch: 87 -- Batch: 000 -- Loss: 1.1859\n","Train Epoch: 87 -- Batch: 500 -- Loss: 1.0973\n","Train Epoch: 88 -- Batch: 000 -- Loss: 1.1745\n","Train Epoch: 88 -- Batch: 500 -- Loss: 1.1274\n","Train Epoch: 89 -- Batch: 000 -- Loss: 1.1806\n","Train Epoch: 89 -- Batch: 500 -- Loss: 1.1508\n","Train Epoch: 90 -- Batch: 000 -- Loss: 1.1565\n","Train Epoch: 90 -- Batch: 500 -- Loss: 1.0749\n","Train Epoch: 91 -- Batch: 000 -- Loss: 1.1750\n","Train Epoch: 91 -- Batch: 500 -- Loss: 1.1307\n","Train Epoch: 92 -- Batch: 000 -- Loss: 1.1867\n","Train Epoch: 92 -- Batch: 500 -- Loss: 1.1089\n","Train Epoch: 93 -- Batch: 000 -- Loss: 1.1517\n","Train Epoch: 93 -- Batch: 500 -- Loss: 1.1204\n","Train Epoch: 94 -- Batch: 000 -- Loss: 1.1680\n","Train Epoch: 94 -- Batch: 500 -- Loss: 1.1206\n","Train Epoch: 95 -- Batch: 000 -- Loss: 1.1693\n","Train Epoch: 95 -- Batch: 500 -- Loss: 1.1260\n","Train Epoch: 96 -- Batch: 000 -- Loss: 1.1778\n","Train Epoch: 96 -- Batch: 500 -- Loss: 1.1350\n","Train Epoch: 97 -- Batch: 000 -- Loss: 1.1966\n","Train Epoch: 97 -- Batch: 500 -- Loss: 1.1037\n","Train Epoch: 98 -- Batch: 000 -- Loss: 1.1492\n","Train Epoch: 98 -- Batch: 500 -- Loss: 1.1034\n","Train Epoch: 99 -- Batch: 000 -- Loss: 1.1555\n","Train Epoch: 99 -- Batch: 500 -- Loss: 1.1137\n","| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5897  \u001b[0m | \u001b[0m 250.8   \u001b[0m | \u001b[0m 0.3602  \u001b[0m | \u001b[0m 0.001011\u001b[0m | \u001b[0m 110.5   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6204\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.2862\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.2453\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.1324\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.1392\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.0457\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.1791\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.1081\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.0832\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.0209\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.1495\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.0706\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.0811\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.0377\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.1458\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.0944\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.0376\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.0177\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.1233\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.0760\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.0367\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.9929\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.1173\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.0256\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.0151\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.0140\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.1045\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.0365\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0150\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.0027\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.0640\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.0182\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.9650\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.9617\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.0561\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.0297\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.9613\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.9966\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.0628\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.0220\n","Train Epoch: 10 -- Batch: 000 -- Loss: 0.9659\n","Train Epoch: 10 -- Batch: 500 -- Loss: 0.9899\n","Train Epoch: 10 -- Batch: 1000 -- Loss: 1.0218\n","Train Epoch: 10 -- Batch: 1500 -- Loss: 0.9910\n","Train Epoch: 11 -- Batch: 000 -- Loss: 0.9728\n","Train Epoch: 11 -- Batch: 500 -- Loss: 0.9870\n","Train Epoch: 11 -- Batch: 1000 -- Loss: 1.0213\n","Train Epoch: 11 -- Batch: 1500 -- Loss: 0.9961\n","Train Epoch: 12 -- Batch: 000 -- Loss: 0.9940\n","Train Epoch: 12 -- Batch: 500 -- Loss: 0.9643\n","Train Epoch: 12 -- Batch: 1000 -- Loss: 1.0263\n","Train Epoch: 12 -- Batch: 1500 -- Loss: 1.0139\n","Train Epoch: 13 -- Batch: 000 -- Loss: 0.9451\n","Train Epoch: 13 -- Batch: 500 -- Loss: 0.9961\n","Train Epoch: 13 -- Batch: 1000 -- Loss: 1.0089\n","Train Epoch: 13 -- Batch: 1500 -- Loss: 0.9585\n","Train Epoch: 14 -- Batch: 000 -- Loss: 0.9300\n","Train Epoch: 14 -- Batch: 500 -- Loss: 0.9480\n","Train Epoch: 14 -- Batch: 1000 -- Loss: 0.9834\n","Train Epoch: 14 -- Batch: 1500 -- Loss: 0.9910\n","Train Epoch: 15 -- Batch: 000 -- Loss: 0.9309\n","Train Epoch: 15 -- Batch: 500 -- Loss: 0.9629\n","Train Epoch: 15 -- Batch: 1000 -- Loss: 0.9770\n","Train Epoch: 15 -- Batch: 1500 -- Loss: 0.9901\n","Train Epoch: 16 -- Batch: 000 -- Loss: 0.9287\n","Train Epoch: 16 -- Batch: 500 -- Loss: 0.9615\n","Train Epoch: 16 -- Batch: 1000 -- Loss: 0.9699\n","Train Epoch: 16 -- Batch: 1500 -- Loss: 0.9878\n","Train Epoch: 17 -- Batch: 000 -- Loss: 0.9275\n","Train Epoch: 17 -- Batch: 500 -- Loss: 0.9730\n","Train Epoch: 17 -- Batch: 1000 -- Loss: 1.0014\n","Train Epoch: 17 -- Batch: 1500 -- Loss: 0.9866\n","Train Epoch: 18 -- Batch: 000 -- Loss: 0.9341\n","Train Epoch: 18 -- Batch: 500 -- Loss: 0.9481\n","Train Epoch: 18 -- Batch: 1000 -- Loss: 0.9998\n","Train Epoch: 18 -- Batch: 1500 -- Loss: 0.9760\n","Train Epoch: 19 -- Batch: 000 -- Loss: 0.8998\n","Train Epoch: 19 -- Batch: 500 -- Loss: 0.9324\n","Train Epoch: 19 -- Batch: 1000 -- Loss: 0.9766\n","Train Epoch: 19 -- Batch: 1500 -- Loss: 0.9697\n","Train Epoch: 20 -- Batch: 000 -- Loss: 0.9354\n","Train Epoch: 20 -- Batch: 500 -- Loss: 0.9640\n","Train Epoch: 20 -- Batch: 1000 -- Loss: 0.9940\n","Train Epoch: 20 -- Batch: 1500 -- Loss: 0.9640\n","Train Epoch: 21 -- Batch: 000 -- Loss: 0.8931\n","Train Epoch: 21 -- Batch: 500 -- Loss: 0.9352\n","Train Epoch: 21 -- Batch: 1000 -- Loss: 0.9723\n","Train Epoch: 21 -- Batch: 1500 -- Loss: 0.9377\n","Train Epoch: 22 -- Batch: 000 -- Loss: 0.9027\n","Train Epoch: 22 -- Batch: 500 -- Loss: 0.9439\n","Train Epoch: 22 -- Batch: 1000 -- Loss: 0.9198\n","Train Epoch: 22 -- Batch: 1500 -- Loss: 0.9513\n","Train Epoch: 23 -- Batch: 000 -- Loss: 0.8979\n","Train Epoch: 23 -- Batch: 500 -- Loss: 0.9497\n","Train Epoch: 23 -- Batch: 1000 -- Loss: 0.9775\n","Train Epoch: 23 -- Batch: 1500 -- Loss: 0.9548\n","Train Epoch: 24 -- Batch: 000 -- Loss: 0.9071\n","Train Epoch: 24 -- Batch: 500 -- Loss: 0.9024\n","Train Epoch: 24 -- Batch: 1000 -- Loss: 0.9698\n","Train Epoch: 24 -- Batch: 1500 -- Loss: 0.9468\n","Train Epoch: 25 -- Batch: 000 -- Loss: 0.8827\n","Train Epoch: 25 -- Batch: 500 -- Loss: 0.9007\n","Train Epoch: 25 -- Batch: 1000 -- Loss: 0.9603\n","Train Epoch: 25 -- Batch: 1500 -- Loss: 0.9598\n","Train Epoch: 26 -- Batch: 000 -- Loss: 0.8899\n","Train Epoch: 26 -- Batch: 500 -- Loss: 0.9367\n","Train Epoch: 26 -- Batch: 1000 -- Loss: 0.9705\n","Train Epoch: 26 -- Batch: 1500 -- Loss: 0.9544\n","Train Epoch: 27 -- Batch: 000 -- Loss: 0.8671\n","Train Epoch: 27 -- Batch: 500 -- Loss: 0.9421\n","Train Epoch: 27 -- Batch: 1000 -- Loss: 0.9868\n","Train Epoch: 27 -- Batch: 1500 -- Loss: 0.9287\n","Train Epoch: 28 -- Batch: 000 -- Loss: 0.8608\n","Train Epoch: 28 -- Batch: 500 -- Loss: 0.9448\n","Train Epoch: 28 -- Batch: 1000 -- Loss: 0.9956\n","Train Epoch: 28 -- Batch: 1500 -- Loss: 0.9802\n","Train Epoch: 29 -- Batch: 000 -- Loss: 0.8553\n","Train Epoch: 29 -- Batch: 500 -- Loss: 0.9256\n","Train Epoch: 29 -- Batch: 1000 -- Loss: 0.9809\n","Train Epoch: 29 -- Batch: 1500 -- Loss: 0.9732\n","Train Epoch: 30 -- Batch: 000 -- Loss: 0.8728\n","Train Epoch: 30 -- Batch: 500 -- Loss: 0.9208\n","Train Epoch: 30 -- Batch: 1000 -- Loss: 0.9375\n","Train Epoch: 30 -- Batch: 1500 -- Loss: 0.9598\n","Train Epoch: 31 -- Batch: 000 -- Loss: 0.8532\n","Train Epoch: 31 -- Batch: 500 -- Loss: 0.9483\n","Train Epoch: 31 -- Batch: 1000 -- Loss: 0.9445\n","Train Epoch: 31 -- Batch: 1500 -- Loss: 0.9307\n","Train Epoch: 32 -- Batch: 000 -- Loss: 0.8440\n","Train Epoch: 32 -- Batch: 500 -- Loss: 0.9359\n","Train Epoch: 32 -- Batch: 1000 -- Loss: 0.9845\n","Train Epoch: 32 -- Batch: 1500 -- Loss: 0.9452\n","Train Epoch: 33 -- Batch: 000 -- Loss: 0.8672\n","Train Epoch: 33 -- Batch: 500 -- Loss: 0.9315\n","Train Epoch: 33 -- Batch: 1000 -- Loss: 0.9298\n","Train Epoch: 33 -- Batch: 1500 -- Loss: 0.9572\n","Train Epoch: 34 -- Batch: 000 -- Loss: 0.8532\n","Train Epoch: 34 -- Batch: 500 -- Loss: 0.9483\n","Train Epoch: 34 -- Batch: 1000 -- Loss: 0.9320\n","Train Epoch: 34 -- Batch: 1500 -- Loss: 0.9687\n","Train Epoch: 35 -- Batch: 000 -- Loss: 0.8119\n","Train Epoch: 35 -- Batch: 500 -- Loss: 0.9285\n","Train Epoch: 35 -- Batch: 1000 -- Loss: 0.9201\n","Train Epoch: 35 -- Batch: 1500 -- Loss: 0.9314\n","Train Epoch: 36 -- Batch: 000 -- Loss: 0.8212\n","Train Epoch: 36 -- Batch: 500 -- Loss: 0.9352\n","Train Epoch: 36 -- Batch: 1000 -- Loss: 0.9289\n","Train Epoch: 36 -- Batch: 1500 -- Loss: 0.9258\n","Train Epoch: 37 -- Batch: 000 -- Loss: 0.8317\n","Train Epoch: 37 -- Batch: 500 -- Loss: 0.9173\n","Train Epoch: 37 -- Batch: 1000 -- Loss: 0.9316\n","Train Epoch: 37 -- Batch: 1500 -- Loss: 0.9527\n","Train Epoch: 38 -- Batch: 000 -- Loss: 0.8124\n","Train Epoch: 38 -- Batch: 500 -- Loss: 0.9461\n","Train Epoch: 38 -- Batch: 1000 -- Loss: 0.9525\n","Train Epoch: 38 -- Batch: 1500 -- Loss: 0.9360\n","Train Epoch: 39 -- Batch: 000 -- Loss: 0.8467\n","Train Epoch: 39 -- Batch: 500 -- Loss: 0.9425\n","Train Epoch: 39 -- Batch: 1000 -- Loss: 0.9357\n","Train Epoch: 39 -- Batch: 1500 -- Loss: 0.9166\n","Train Epoch: 40 -- Batch: 000 -- Loss: 0.8664\n","Train Epoch: 40 -- Batch: 500 -- Loss: 0.9299\n","Train Epoch: 40 -- Batch: 1000 -- Loss: 0.9243\n","Train Epoch: 40 -- Batch: 1500 -- Loss: 0.9393\n","Train Epoch: 41 -- Batch: 000 -- Loss: 0.8513\n","Train Epoch: 41 -- Batch: 500 -- Loss: 0.9579\n","Train Epoch: 41 -- Batch: 1000 -- Loss: 0.9636\n","Train Epoch: 41 -- Batch: 1500 -- Loss: 0.9097\n","Train Epoch: 42 -- Batch: 000 -- Loss: 0.8225\n","Train Epoch: 42 -- Batch: 500 -- Loss: 0.9811\n","Train Epoch: 42 -- Batch: 1000 -- Loss: 0.9713\n","Train Epoch: 42 -- Batch: 1500 -- Loss: 0.9057\n","Train Epoch: 43 -- Batch: 000 -- Loss: 0.8624\n","Train Epoch: 43 -- Batch: 500 -- Loss: 0.9352\n","Train Epoch: 43 -- Batch: 1000 -- Loss: 0.9059\n","Train Epoch: 43 -- Batch: 1500 -- Loss: 0.8988\n","Train Epoch: 44 -- Batch: 000 -- Loss: 0.8173\n","Train Epoch: 44 -- Batch: 500 -- Loss: 0.9121\n","Train Epoch: 44 -- Batch: 1000 -- Loss: 0.9782\n","Train Epoch: 44 -- Batch: 1500 -- Loss: 0.9766\n","Train Epoch: 45 -- Batch: 000 -- Loss: 0.8289\n","Train Epoch: 45 -- Batch: 500 -- Loss: 0.9331\n","Train Epoch: 45 -- Batch: 1000 -- Loss: 0.9463\n","Train Epoch: 45 -- Batch: 1500 -- Loss: 0.9389\n","Train Epoch: 46 -- Batch: 000 -- Loss: 0.8253\n","Train Epoch: 46 -- Batch: 500 -- Loss: 0.9360\n","Train Epoch: 46 -- Batch: 1000 -- Loss: 0.9268\n","Train Epoch: 46 -- Batch: 1500 -- Loss: 0.9177\n","Train Epoch: 47 -- Batch: 000 -- Loss: 0.8590\n","Train Epoch: 47 -- Batch: 500 -- Loss: 0.9129\n","Train Epoch: 47 -- Batch: 1000 -- Loss: 0.9305\n","Train Epoch: 47 -- Batch: 1500 -- Loss: 0.9186\n","Train Epoch: 48 -- Batch: 000 -- Loss: 0.8028\n","Train Epoch: 48 -- Batch: 500 -- Loss: 0.9286\n","Train Epoch: 48 -- Batch: 1000 -- Loss: 0.9155\n","Train Epoch: 48 -- Batch: 1500 -- Loss: 0.8994\n","Train Epoch: 49 -- Batch: 000 -- Loss: 0.8209\n","Train Epoch: 49 -- Batch: 500 -- Loss: 0.9696\n","Train Epoch: 49 -- Batch: 1000 -- Loss: 0.9154\n","Train Epoch: 49 -- Batch: 1500 -- Loss: 0.9078\n","Train Epoch: 50 -- Batch: 000 -- Loss: 0.8061\n","Train Epoch: 50 -- Batch: 500 -- Loss: 0.9223\n","Train Epoch: 50 -- Batch: 1000 -- Loss: 0.9468\n","Train Epoch: 50 -- Batch: 1500 -- Loss: 0.9347\n","Train Epoch: 51 -- Batch: 000 -- Loss: 0.8186\n","Train Epoch: 51 -- Batch: 500 -- Loss: 0.9247\n","Train Epoch: 51 -- Batch: 1000 -- Loss: 0.8972\n","Train Epoch: 51 -- Batch: 1500 -- Loss: 0.9028\n","Train Epoch: 52 -- Batch: 000 -- Loss: 0.8526\n","Train Epoch: 52 -- Batch: 500 -- Loss: 0.9409\n","Train Epoch: 52 -- Batch: 1000 -- Loss: 0.9437\n","Train Epoch: 52 -- Batch: 1500 -- Loss: 0.9211\n","Train Epoch: 53 -- Batch: 000 -- Loss: 0.8152\n","Train Epoch: 53 -- Batch: 500 -- Loss: 0.8969\n","Train Epoch: 53 -- Batch: 1000 -- Loss: 0.9323\n","Train Epoch: 53 -- Batch: 1500 -- Loss: 0.9463\n","Train Epoch: 54 -- Batch: 000 -- Loss: 0.8287\n","Train Epoch: 54 -- Batch: 500 -- Loss: 0.9111\n","Train Epoch: 54 -- Batch: 1000 -- Loss: 0.8995\n","Train Epoch: 54 -- Batch: 1500 -- Loss: 0.9236\n","Train Epoch: 55 -- Batch: 000 -- Loss: 0.8061\n","Train Epoch: 55 -- Batch: 500 -- Loss: 0.9202\n","Train Epoch: 55 -- Batch: 1000 -- Loss: 0.9248\n","Train Epoch: 55 -- Batch: 1500 -- Loss: 0.8966\n","Train Epoch: 56 -- Batch: 000 -- Loss: 0.8193\n","Train Epoch: 56 -- Batch: 500 -- Loss: 0.9161\n","Train Epoch: 56 -- Batch: 1000 -- Loss: 0.9392\n","Train Epoch: 56 -- Batch: 1500 -- Loss: 0.8979\n","Train Epoch: 57 -- Batch: 000 -- Loss: 0.8156\n","Train Epoch: 57 -- Batch: 500 -- Loss: 0.8968\n","Train Epoch: 57 -- Batch: 1000 -- Loss: 0.9260\n","Train Epoch: 57 -- Batch: 1500 -- Loss: 0.9431\n","Train Epoch: 58 -- Batch: 000 -- Loss: 0.8369\n","Train Epoch: 58 -- Batch: 500 -- Loss: 0.8876\n","Train Epoch: 58 -- Batch: 1000 -- Loss: 0.9210\n","Train Epoch: 58 -- Batch: 1500 -- Loss: 0.9276\n","Train Epoch: 59 -- Batch: 000 -- Loss: 0.8540\n","Train Epoch: 59 -- Batch: 500 -- Loss: 0.9041\n","Train Epoch: 59 -- Batch: 1000 -- Loss: 0.8903\n","Train Epoch: 59 -- Batch: 1500 -- Loss: 0.8968\n","Train Epoch: 60 -- Batch: 000 -- Loss: 0.8251\n","Train Epoch: 60 -- Batch: 500 -- Loss: 0.9149\n","Train Epoch: 60 -- Batch: 1000 -- Loss: 0.8939\n","Train Epoch: 60 -- Batch: 1500 -- Loss: 0.9026\n","Train Epoch: 61 -- Batch: 000 -- Loss: 0.8162\n","Train Epoch: 61 -- Batch: 500 -- Loss: 0.9195\n","Train Epoch: 61 -- Batch: 1000 -- Loss: 0.9125\n","Train Epoch: 61 -- Batch: 1500 -- Loss: 0.8769\n","Train Epoch: 62 -- Batch: 000 -- Loss: 0.8253\n","Train Epoch: 62 -- Batch: 500 -- Loss: 0.9166\n","Train Epoch: 62 -- Batch: 1000 -- Loss: 0.9118\n","Train Epoch: 62 -- Batch: 1500 -- Loss: 0.9292\n","Train Epoch: 63 -- Batch: 000 -- Loss: 0.8281\n","Train Epoch: 63 -- Batch: 500 -- Loss: 0.9377\n","Train Epoch: 63 -- Batch: 1000 -- Loss: 0.9104\n","Train Epoch: 63 -- Batch: 1500 -- Loss: 0.9230\n","Train Epoch: 64 -- Batch: 000 -- Loss: 0.7988\n","Train Epoch: 64 -- Batch: 500 -- Loss: 0.9200\n","Train Epoch: 64 -- Batch: 1000 -- Loss: 0.8690\n","Train Epoch: 64 -- Batch: 1500 -- Loss: 0.8864\n","Train Epoch: 65 -- Batch: 000 -- Loss: 0.8128\n","Train Epoch: 65 -- Batch: 500 -- Loss: 0.9397\n","Train Epoch: 65 -- Batch: 1000 -- Loss: 0.8799\n","Train Epoch: 65 -- Batch: 1500 -- Loss: 0.8825\n","Train Epoch: 66 -- Batch: 000 -- Loss: 0.7852\n","Train Epoch: 66 -- Batch: 500 -- Loss: 0.8961\n","Train Epoch: 66 -- Batch: 1000 -- Loss: 0.8645\n","Train Epoch: 66 -- Batch: 1500 -- Loss: 0.9539\n","Train Epoch: 67 -- Batch: 000 -- Loss: 0.8200\n","Train Epoch: 67 -- Batch: 500 -- Loss: 0.8930\n","Train Epoch: 67 -- Batch: 1000 -- Loss: 0.8612\n","Train Epoch: 67 -- Batch: 1500 -- Loss: 0.8936\n","Train Epoch: 68 -- Batch: 000 -- Loss: 0.8355\n","Train Epoch: 68 -- Batch: 500 -- Loss: 0.9196\n","Train Epoch: 68 -- Batch: 1000 -- Loss: 0.9224\n","Train Epoch: 68 -- Batch: 1500 -- Loss: 0.9155\n","Train Epoch: 69 -- Batch: 000 -- Loss: 0.8360\n","Train Epoch: 69 -- Batch: 500 -- Loss: 0.9312\n","Train Epoch: 69 -- Batch: 1000 -- Loss: 0.9058\n","Train Epoch: 69 -- Batch: 1500 -- Loss: 0.9009\n","Train Epoch: 70 -- Batch: 000 -- Loss: 0.8537\n","Train Epoch: 70 -- Batch: 500 -- Loss: 0.9350\n","Train Epoch: 70 -- Batch: 1000 -- Loss: 0.8503\n","Train Epoch: 70 -- Batch: 1500 -- Loss: 0.8676\n","Train Epoch: 71 -- Batch: 000 -- Loss: 0.7988\n","Train Epoch: 71 -- Batch: 500 -- Loss: 0.9425\n","Train Epoch: 71 -- Batch: 1000 -- Loss: 0.8974\n","Train Epoch: 71 -- Batch: 1500 -- Loss: 0.9312\n","Train Epoch: 72 -- Batch: 000 -- Loss: 0.8316\n","Train Epoch: 72 -- Batch: 500 -- Loss: 0.8939\n","Train Epoch: 72 -- Batch: 1000 -- Loss: 0.8852\n","Train Epoch: 72 -- Batch: 1500 -- Loss: 0.8879\n","Train Epoch: 73 -- Batch: 000 -- Loss: 0.7520\n","Train Epoch: 73 -- Batch: 500 -- Loss: 0.8969\n","Train Epoch: 73 -- Batch: 1000 -- Loss: 0.9250\n","Train Epoch: 73 -- Batch: 1500 -- Loss: 0.8764\n","Train Epoch: 74 -- Batch: 000 -- Loss: 0.7929\n","Train Epoch: 74 -- Batch: 500 -- Loss: 0.8895\n","Train Epoch: 74 -- Batch: 1000 -- Loss: 0.8667\n","Train Epoch: 74 -- Batch: 1500 -- Loss: 0.8769\n","Train Epoch: 75 -- Batch: 000 -- Loss: 0.7782\n","Train Epoch: 75 -- Batch: 500 -- Loss: 0.9022\n","Train Epoch: 75 -- Batch: 1000 -- Loss: 0.9115\n","Train Epoch: 75 -- Batch: 1500 -- Loss: 0.9036\n","Train Epoch: 76 -- Batch: 000 -- Loss: 0.7823\n","Train Epoch: 76 -- Batch: 500 -- Loss: 0.8957\n","Train Epoch: 76 -- Batch: 1000 -- Loss: 0.9022\n","Train Epoch: 76 -- Batch: 1500 -- Loss: 0.9143\n","Train Epoch: 77 -- Batch: 000 -- Loss: 0.7768\n","Train Epoch: 77 -- Batch: 500 -- Loss: 0.8759\n","Train Epoch: 77 -- Batch: 1000 -- Loss: 0.9035\n","Train Epoch: 77 -- Batch: 1500 -- Loss: 0.8557\n","Train Epoch: 78 -- Batch: 000 -- Loss: 0.8136\n","Train Epoch: 78 -- Batch: 500 -- Loss: 0.8609\n","Train Epoch: 78 -- Batch: 1000 -- Loss: 0.8484\n","Train Epoch: 78 -- Batch: 1500 -- Loss: 0.8755\n","Train Epoch: 79 -- Batch: 000 -- Loss: 0.8045\n","Train Epoch: 79 -- Batch: 500 -- Loss: 0.9076\n","Train Epoch: 79 -- Batch: 1000 -- Loss: 0.8792\n","Train Epoch: 79 -- Batch: 1500 -- Loss: 0.8776\n","Train Epoch: 80 -- Batch: 000 -- Loss: 0.7911\n","Train Epoch: 80 -- Batch: 500 -- Loss: 0.8724\n","Train Epoch: 80 -- Batch: 1000 -- Loss: 0.8554\n","Train Epoch: 80 -- Batch: 1500 -- Loss: 0.8623\n","Train Epoch: 81 -- Batch: 000 -- Loss: 0.8278\n","Train Epoch: 81 -- Batch: 500 -- Loss: 0.9053\n","Train Epoch: 81 -- Batch: 1000 -- Loss: 0.8623\n","Train Epoch: 81 -- Batch: 1500 -- Loss: 0.8881\n","Train Epoch: 82 -- Batch: 000 -- Loss: 0.7902\n","Train Epoch: 82 -- Batch: 500 -- Loss: 0.9322\n","Train Epoch: 82 -- Batch: 1000 -- Loss: 0.8478\n","Train Epoch: 82 -- Batch: 1500 -- Loss: 0.9117\n","Train Epoch: 83 -- Batch: 000 -- Loss: 0.8123\n","Train Epoch: 83 -- Batch: 500 -- Loss: 0.8817\n","Train Epoch: 83 -- Batch: 1000 -- Loss: 0.8627\n","Train Epoch: 83 -- Batch: 1500 -- Loss: 0.8711\n","Train Epoch: 84 -- Batch: 000 -- Loss: 0.8090\n","Train Epoch: 84 -- Batch: 500 -- Loss: 0.8923\n","Train Epoch: 84 -- Batch: 1000 -- Loss: 0.8760\n","Train Epoch: 84 -- Batch: 1500 -- Loss: 0.8701\n","Train Epoch: 85 -- Batch: 000 -- Loss: 0.7993\n","Train Epoch: 85 -- Batch: 500 -- Loss: 0.8826\n","Train Epoch: 85 -- Batch: 1000 -- Loss: 0.8343\n","Train Epoch: 85 -- Batch: 1500 -- Loss: 0.8793\n","Train Epoch: 86 -- Batch: 000 -- Loss: 0.7857\n","Train Epoch: 86 -- Batch: 500 -- Loss: 0.9002\n","Train Epoch: 86 -- Batch: 1000 -- Loss: 0.8524\n","Train Epoch: 86 -- Batch: 1500 -- Loss: 0.8679\n","Train Epoch: 87 -- Batch: 000 -- Loss: 0.7914\n","Train Epoch: 87 -- Batch: 500 -- Loss: 0.8676\n","Train Epoch: 87 -- Batch: 1000 -- Loss: 0.8734\n","Train Epoch: 87 -- Batch: 1500 -- Loss: 0.9196\n","Train Epoch: 88 -- Batch: 000 -- Loss: 0.7984\n","Train Epoch: 88 -- Batch: 500 -- Loss: 0.8338\n","Train Epoch: 88 -- Batch: 1000 -- Loss: 0.8698\n","Train Epoch: 88 -- Batch: 1500 -- Loss: 0.8000\n","Train Epoch: 89 -- Batch: 000 -- Loss: 0.7727\n","Train Epoch: 89 -- Batch: 500 -- Loss: 0.8746\n","Train Epoch: 89 -- Batch: 1000 -- Loss: 0.8684\n","Train Epoch: 89 -- Batch: 1500 -- Loss: 0.8582\n","Train Epoch: 90 -- Batch: 000 -- Loss: 0.7980\n","Train Epoch: 90 -- Batch: 500 -- Loss: 0.8610\n","Train Epoch: 90 -- Batch: 1000 -- Loss: 0.8423\n","Train Epoch: 90 -- Batch: 1500 -- Loss: 0.8424\n","Train Epoch: 91 -- Batch: 000 -- Loss: 0.7817\n","Train Epoch: 91 -- Batch: 500 -- Loss: 0.8451\n","Train Epoch: 91 -- Batch: 1000 -- Loss: 0.8435\n","Train Epoch: 91 -- Batch: 1500 -- Loss: 0.8413\n","Train Epoch: 92 -- Batch: 000 -- Loss: 0.7911\n","Train Epoch: 92 -- Batch: 500 -- Loss: 0.8598\n","Train Epoch: 92 -- Batch: 1000 -- Loss: 0.8365\n","Train Epoch: 92 -- Batch: 1500 -- Loss: 0.8932\n","Train Epoch: 93 -- Batch: 000 -- Loss: 0.8098\n","Train Epoch: 93 -- Batch: 500 -- Loss: 0.8484\n","Train Epoch: 93 -- Batch: 1000 -- Loss: 0.8587\n","Train Epoch: 93 -- Batch: 1500 -- Loss: 0.8283\n","Train Epoch: 94 -- Batch: 000 -- Loss: 0.7853\n","Train Epoch: 94 -- Batch: 500 -- Loss: 0.8849\n","Train Epoch: 94 -- Batch: 1000 -- Loss: 0.8554\n","Train Epoch: 94 -- Batch: 1500 -- Loss: 0.8849\n","Train Epoch: 95 -- Batch: 000 -- Loss: 0.7930\n","Train Epoch: 95 -- Batch: 500 -- Loss: 0.8822\n","Train Epoch: 95 -- Batch: 1000 -- Loss: 0.8773\n","Train Epoch: 95 -- Batch: 1500 -- Loss: 0.8718\n","Train Epoch: 96 -- Batch: 000 -- Loss: 0.8052\n","Train Epoch: 96 -- Batch: 500 -- Loss: 0.8654\n","Train Epoch: 96 -- Batch: 1000 -- Loss: 0.8596\n","Train Epoch: 96 -- Batch: 1500 -- Loss: 0.9035\n","Train Epoch: 97 -- Batch: 000 -- Loss: 0.8344\n","Train Epoch: 97 -- Batch: 500 -- Loss: 0.8715\n","Train Epoch: 97 -- Batch: 1000 -- Loss: 0.8155\n","Train Epoch: 97 -- Batch: 1500 -- Loss: 0.8222\n","Train Epoch: 98 -- Batch: 000 -- Loss: 0.7985\n","Train Epoch: 98 -- Batch: 500 -- Loss: 0.8797\n","Train Epoch: 98 -- Batch: 1000 -- Loss: 0.8529\n","Train Epoch: 98 -- Batch: 1500 -- Loss: 0.9086\n","Train Epoch: 99 -- Batch: 000 -- Loss: 0.7705\n","Train Epoch: 99 -- Batch: 500 -- Loss: 0.8036\n","Train Epoch: 99 -- Batch: 1000 -- Loss: 0.8326\n","Train Epoch: 99 -- Batch: 1500 -- Loss: 0.8197\n","| \u001b[95m 2       \u001b[0m | \u001b[95m 0.6002  \u001b[0m | \u001b[95m 129.7   \u001b[0m | \u001b[95m 0.04617 \u001b[0m | \u001b[95m 0.01944 \u001b[0m | \u001b[95m 119.1   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6120\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.2996\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.1883\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.2060\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.2504\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.1287\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.1597\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.2008\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.0887\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.1608\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.1441\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.0737\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.1388\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.1494\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.0378\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.1116\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.1003\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.0381\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.0972\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.1117\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.0666\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.1267\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.1273\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.0242\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.0809\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.0965\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.0241\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.0746\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.1074\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.0215\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.0817\n","Train Epoch: 10 -- Batch: 500 -- Loss: 1.0902\n","Train Epoch: 10 -- Batch: 1000 -- Loss: 1.0172\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.0458\n","Train Epoch: 11 -- Batch: 500 -- Loss: 1.0784\n","Train Epoch: 11 -- Batch: 1000 -- Loss: 1.0313\n","Train Epoch: 12 -- Batch: 000 -- Loss: 1.0550\n","Train Epoch: 12 -- Batch: 500 -- Loss: 1.0849\n","Train Epoch: 12 -- Batch: 1000 -- Loss: 1.0135\n","Train Epoch: 13 -- Batch: 000 -- Loss: 1.0106\n","Train Epoch: 13 -- Batch: 500 -- Loss: 1.0570\n","Train Epoch: 13 -- Batch: 1000 -- Loss: 1.0171\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.0353\n","Train Epoch: 14 -- Batch: 500 -- Loss: 1.0584\n","Train Epoch: 14 -- Batch: 1000 -- Loss: 0.9876\n","Train Epoch: 15 -- Batch: 000 -- Loss: 1.0307\n","Train Epoch: 15 -- Batch: 500 -- Loss: 1.0493\n","Train Epoch: 15 -- Batch: 1000 -- Loss: 0.9850\n","Train Epoch: 16 -- Batch: 000 -- Loss: 1.0300\n","Train Epoch: 16 -- Batch: 500 -- Loss: 1.0370\n","Train Epoch: 16 -- Batch: 1000 -- Loss: 0.9620\n","Train Epoch: 17 -- Batch: 000 -- Loss: 1.0230\n","Train Epoch: 17 -- Batch: 500 -- Loss: 1.0532\n","Train Epoch: 17 -- Batch: 1000 -- Loss: 0.9776\n","Train Epoch: 18 -- Batch: 000 -- Loss: 1.0511\n","Train Epoch: 18 -- Batch: 500 -- Loss: 1.0771\n","Train Epoch: 18 -- Batch: 1000 -- Loss: 0.9821\n","Train Epoch: 19 -- Batch: 000 -- Loss: 1.0507\n","Train Epoch: 19 -- Batch: 500 -- Loss: 1.0189\n","Train Epoch: 19 -- Batch: 1000 -- Loss: 0.9875\n","Train Epoch: 20 -- Batch: 000 -- Loss: 1.0381\n","Train Epoch: 20 -- Batch: 500 -- Loss: 1.0572\n","Train Epoch: 20 -- Batch: 1000 -- Loss: 0.9884\n","Train Epoch: 21 -- Batch: 000 -- Loss: 1.0418\n","Train Epoch: 21 -- Batch: 500 -- Loss: 1.0221\n","Train Epoch: 21 -- Batch: 1000 -- Loss: 1.0000\n","Train Epoch: 22 -- Batch: 000 -- Loss: 1.0233\n","Train Epoch: 22 -- Batch: 500 -- Loss: 1.0599\n","Train Epoch: 22 -- Batch: 1000 -- Loss: 0.9926\n","Train Epoch: 23 -- Batch: 000 -- Loss: 1.0178\n","Train Epoch: 23 -- Batch: 500 -- Loss: 1.0452\n","Train Epoch: 23 -- Batch: 1000 -- Loss: 0.9610\n","Train Epoch: 24 -- Batch: 000 -- Loss: 0.9867\n","Train Epoch: 24 -- Batch: 500 -- Loss: 1.0087\n","Train Epoch: 24 -- Batch: 1000 -- Loss: 0.9703\n","Train Epoch: 25 -- Batch: 000 -- Loss: 1.0321\n","Train Epoch: 25 -- Batch: 500 -- Loss: 1.0309\n","Train Epoch: 25 -- Batch: 1000 -- Loss: 0.9670\n","Train Epoch: 26 -- Batch: 000 -- Loss: 1.0008\n","Train Epoch: 26 -- Batch: 500 -- Loss: 1.0361\n","Train Epoch: 26 -- Batch: 1000 -- Loss: 0.9420\n","Train Epoch: 27 -- Batch: 000 -- Loss: 0.9977\n","Train Epoch: 27 -- Batch: 500 -- Loss: 1.0598\n","Train Epoch: 27 -- Batch: 1000 -- Loss: 0.9676\n","Train Epoch: 28 -- Batch: 000 -- Loss: 1.0121\n","Train Epoch: 28 -- Batch: 500 -- Loss: 0.9876\n","Train Epoch: 28 -- Batch: 1000 -- Loss: 0.9740\n","Train Epoch: 29 -- Batch: 000 -- Loss: 1.0058\n","Train Epoch: 29 -- Batch: 500 -- Loss: 1.0589\n","Train Epoch: 29 -- Batch: 1000 -- Loss: 0.9688\n","Train Epoch: 30 -- Batch: 000 -- Loss: 1.0280\n","Train Epoch: 30 -- Batch: 500 -- Loss: 1.0338\n","Train Epoch: 30 -- Batch: 1000 -- Loss: 0.9468\n","Train Epoch: 31 -- Batch: 000 -- Loss: 0.9953\n","Train Epoch: 31 -- Batch: 500 -- Loss: 1.0083\n","Train Epoch: 31 -- Batch: 1000 -- Loss: 0.9580\n","Train Epoch: 32 -- Batch: 000 -- Loss: 0.9977\n","Train Epoch: 32 -- Batch: 500 -- Loss: 1.0033\n","Train Epoch: 32 -- Batch: 1000 -- Loss: 0.9376\n","Train Epoch: 33 -- Batch: 000 -- Loss: 0.9847\n","Train Epoch: 33 -- Batch: 500 -- Loss: 1.0254\n","Train Epoch: 33 -- Batch: 1000 -- Loss: 0.9621\n","Train Epoch: 34 -- Batch: 000 -- Loss: 0.9769\n","Train Epoch: 34 -- Batch: 500 -- Loss: 0.9940\n","Train Epoch: 34 -- Batch: 1000 -- Loss: 0.9617\n","Train Epoch: 35 -- Batch: 000 -- Loss: 0.9823\n","Train Epoch: 35 -- Batch: 500 -- Loss: 1.0089\n","Train Epoch: 35 -- Batch: 1000 -- Loss: 0.9690\n","Train Epoch: 36 -- Batch: 000 -- Loss: 0.9962\n","Train Epoch: 36 -- Batch: 500 -- Loss: 1.0078\n","Train Epoch: 36 -- Batch: 1000 -- Loss: 0.9759\n","Train Epoch: 37 -- Batch: 000 -- Loss: 0.9792\n","Train Epoch: 37 -- Batch: 500 -- Loss: 1.0051\n","Train Epoch: 37 -- Batch: 1000 -- Loss: 0.9323\n","Train Epoch: 38 -- Batch: 000 -- Loss: 1.0108\n","Train Epoch: 38 -- Batch: 500 -- Loss: 1.0047\n","Train Epoch: 38 -- Batch: 1000 -- Loss: 0.9543\n","Train Epoch: 39 -- Batch: 000 -- Loss: 0.9991\n","Train Epoch: 39 -- Batch: 500 -- Loss: 0.9909\n","Train Epoch: 39 -- Batch: 1000 -- Loss: 0.9289\n","Train Epoch: 40 -- Batch: 000 -- Loss: 0.9730\n","Train Epoch: 40 -- Batch: 500 -- Loss: 1.0336\n","Train Epoch: 40 -- Batch: 1000 -- Loss: 0.9276\n","Train Epoch: 41 -- Batch: 000 -- Loss: 1.0058\n","Train Epoch: 41 -- Batch: 500 -- Loss: 1.0189\n","Train Epoch: 41 -- Batch: 1000 -- Loss: 0.9364\n","Train Epoch: 42 -- Batch: 000 -- Loss: 0.9660\n","Train Epoch: 42 -- Batch: 500 -- Loss: 1.0056\n","Train Epoch: 42 -- Batch: 1000 -- Loss: 0.9365\n","Train Epoch: 43 -- Batch: 000 -- Loss: 1.0038\n","Train Epoch: 43 -- Batch: 500 -- Loss: 0.9601\n","Train Epoch: 43 -- Batch: 1000 -- Loss: 0.9184\n","Train Epoch: 44 -- Batch: 000 -- Loss: 0.9969\n","Train Epoch: 44 -- Batch: 500 -- Loss: 0.9869\n","Train Epoch: 44 -- Batch: 1000 -- Loss: 0.9333\n","Train Epoch: 45 -- Batch: 000 -- Loss: 1.0030\n","Train Epoch: 45 -- Batch: 500 -- Loss: 0.9589\n","Train Epoch: 45 -- Batch: 1000 -- Loss: 0.9164\n","Train Epoch: 46 -- Batch: 000 -- Loss: 1.0228\n","Train Epoch: 46 -- Batch: 500 -- Loss: 0.9920\n","Train Epoch: 46 -- Batch: 1000 -- Loss: 0.8786\n","Train Epoch: 47 -- Batch: 000 -- Loss: 0.9913\n","Train Epoch: 47 -- Batch: 500 -- Loss: 0.9777\n","Train Epoch: 47 -- Batch: 1000 -- Loss: 0.9398\n","Train Epoch: 48 -- Batch: 000 -- Loss: 0.9669\n","Train Epoch: 48 -- Batch: 500 -- Loss: 1.0035\n","Train Epoch: 48 -- Batch: 1000 -- Loss: 0.9437\n","Train Epoch: 49 -- Batch: 000 -- Loss: 0.9932\n","Train Epoch: 49 -- Batch: 500 -- Loss: 1.0191\n","Train Epoch: 49 -- Batch: 1000 -- Loss: 0.9288\n","Train Epoch: 50 -- Batch: 000 -- Loss: 0.9651\n","Train Epoch: 50 -- Batch: 500 -- Loss: 0.9907\n","Train Epoch: 50 -- Batch: 1000 -- Loss: 0.9600\n","Train Epoch: 51 -- Batch: 000 -- Loss: 0.9740\n","Train Epoch: 51 -- Batch: 500 -- Loss: 0.9650\n","Train Epoch: 51 -- Batch: 1000 -- Loss: 0.9538\n","Train Epoch: 52 -- Batch: 000 -- Loss: 0.9996\n","Train Epoch: 52 -- Batch: 500 -- Loss: 0.9830\n","Train Epoch: 52 -- Batch: 1000 -- Loss: 0.9762\n","Train Epoch: 53 -- Batch: 000 -- Loss: 0.9840\n","Train Epoch: 53 -- Batch: 500 -- Loss: 1.0314\n","Train Epoch: 53 -- Batch: 1000 -- Loss: 0.9054\n","Train Epoch: 54 -- Batch: 000 -- Loss: 0.9488\n","Train Epoch: 54 -- Batch: 500 -- Loss: 1.0077\n","Train Epoch: 54 -- Batch: 1000 -- Loss: 0.9037\n","Train Epoch: 55 -- Batch: 000 -- Loss: 0.9882\n","Train Epoch: 55 -- Batch: 500 -- Loss: 0.9639\n","Train Epoch: 55 -- Batch: 1000 -- Loss: 0.8915\n","Train Epoch: 56 -- Batch: 000 -- Loss: 0.9784\n","Train Epoch: 56 -- Batch: 500 -- Loss: 0.9888\n","Train Epoch: 56 -- Batch: 1000 -- Loss: 0.9486\n","Train Epoch: 57 -- Batch: 000 -- Loss: 1.0066\n","Train Epoch: 57 -- Batch: 500 -- Loss: 0.9590\n","Train Epoch: 57 -- Batch: 1000 -- Loss: 0.9529\n","Train Epoch: 58 -- Batch: 000 -- Loss: 0.9826\n","Train Epoch: 58 -- Batch: 500 -- Loss: 0.9782\n","Train Epoch: 58 -- Batch: 1000 -- Loss: 0.9887\n","Train Epoch: 59 -- Batch: 000 -- Loss: 0.9345\n","Train Epoch: 59 -- Batch: 500 -- Loss: 0.9365\n","Train Epoch: 59 -- Batch: 1000 -- Loss: 0.9026\n","Train Epoch: 60 -- Batch: 000 -- Loss: 0.9962\n","Train Epoch: 60 -- Batch: 500 -- Loss: 0.9907\n","Train Epoch: 60 -- Batch: 1000 -- Loss: 0.9125\n","Train Epoch: 61 -- Batch: 000 -- Loss: 0.9580\n","Train Epoch: 61 -- Batch: 500 -- Loss: 0.9637\n","Train Epoch: 61 -- Batch: 1000 -- Loss: 0.9249\n","Train Epoch: 62 -- Batch: 000 -- Loss: 0.9618\n","Train Epoch: 62 -- Batch: 500 -- Loss: 0.9809\n","Train Epoch: 62 -- Batch: 1000 -- Loss: 0.8668\n","Train Epoch: 63 -- Batch: 000 -- Loss: 0.9586\n","Train Epoch: 63 -- Batch: 500 -- Loss: 0.9586\n","Train Epoch: 63 -- Batch: 1000 -- Loss: 0.9125\n","Train Epoch: 64 -- Batch: 000 -- Loss: 0.9735\n","Train Epoch: 64 -- Batch: 500 -- Loss: 0.9785\n","Train Epoch: 64 -- Batch: 1000 -- Loss: 0.8840\n","Train Epoch: 65 -- Batch: 000 -- Loss: 0.9818\n","Train Epoch: 65 -- Batch: 500 -- Loss: 0.9604\n","Train Epoch: 65 -- Batch: 1000 -- Loss: 0.9015\n","Train Epoch: 66 -- Batch: 000 -- Loss: 0.9541\n","Train Epoch: 66 -- Batch: 500 -- Loss: 1.0164\n","Train Epoch: 66 -- Batch: 1000 -- Loss: 0.9194\n","Train Epoch: 67 -- Batch: 000 -- Loss: 0.9776\n","Train Epoch: 67 -- Batch: 500 -- Loss: 0.9291\n","Train Epoch: 67 -- Batch: 1000 -- Loss: 0.9337\n","Train Epoch: 68 -- Batch: 000 -- Loss: 0.9630\n","Train Epoch: 68 -- Batch: 500 -- Loss: 0.9895\n","Train Epoch: 68 -- Batch: 1000 -- Loss: 0.9159\n","Train Epoch: 69 -- Batch: 000 -- Loss: 0.9577\n","Train Epoch: 69 -- Batch: 500 -- Loss: 0.9694\n","Train Epoch: 69 -- Batch: 1000 -- Loss: 0.9253\n","Train Epoch: 70 -- Batch: 000 -- Loss: 0.9355\n","Train Epoch: 70 -- Batch: 500 -- Loss: 0.9795\n","Train Epoch: 70 -- Batch: 1000 -- Loss: 0.9113\n","Train Epoch: 71 -- Batch: 000 -- Loss: 0.9522\n","Train Epoch: 71 -- Batch: 500 -- Loss: 0.9290\n","Train Epoch: 71 -- Batch: 1000 -- Loss: 0.8979\n","Train Epoch: 72 -- Batch: 000 -- Loss: 0.9776\n","Train Epoch: 72 -- Batch: 500 -- Loss: 0.9454\n","Train Epoch: 72 -- Batch: 1000 -- Loss: 0.9225\n","Train Epoch: 73 -- Batch: 000 -- Loss: 0.9496\n","Train Epoch: 73 -- Batch: 500 -- Loss: 0.9811\n","Train Epoch: 73 -- Batch: 1000 -- Loss: 0.9097\n","Train Epoch: 74 -- Batch: 000 -- Loss: 0.9253\n","Train Epoch: 74 -- Batch: 500 -- Loss: 1.0043\n","Train Epoch: 74 -- Batch: 1000 -- Loss: 0.8832\n","Train Epoch: 75 -- Batch: 000 -- Loss: 0.9553\n","Train Epoch: 75 -- Batch: 500 -- Loss: 0.9717\n","Train Epoch: 75 -- Batch: 1000 -- Loss: 0.8765\n","Train Epoch: 76 -- Batch: 000 -- Loss: 0.9765\n","Train Epoch: 76 -- Batch: 500 -- Loss: 0.9892\n","Train Epoch: 76 -- Batch: 1000 -- Loss: 0.9306\n","Train Epoch: 77 -- Batch: 000 -- Loss: 0.9602\n","Train Epoch: 77 -- Batch: 500 -- Loss: 0.9689\n","Train Epoch: 77 -- Batch: 1000 -- Loss: 0.8659\n","Train Epoch: 78 -- Batch: 000 -- Loss: 0.9952\n","Train Epoch: 78 -- Batch: 500 -- Loss: 0.9651\n","Train Epoch: 78 -- Batch: 1000 -- Loss: 0.9329\n","Train Epoch: 79 -- Batch: 000 -- Loss: 0.9766\n","Train Epoch: 79 -- Batch: 500 -- Loss: 0.9327\n","Train Epoch: 79 -- Batch: 1000 -- Loss: 0.8820\n","Train Epoch: 80 -- Batch: 000 -- Loss: 0.9439\n","Train Epoch: 80 -- Batch: 500 -- Loss: 0.9936\n","Train Epoch: 80 -- Batch: 1000 -- Loss: 0.9077\n","Train Epoch: 81 -- Batch: 000 -- Loss: 0.9584\n","Train Epoch: 81 -- Batch: 500 -- Loss: 0.9793\n","Train Epoch: 81 -- Batch: 1000 -- Loss: 0.8967\n","Train Epoch: 82 -- Batch: 000 -- Loss: 0.9577\n","Train Epoch: 82 -- Batch: 500 -- Loss: 0.9972\n","Train Epoch: 82 -- Batch: 1000 -- Loss: 0.9266\n","Train Epoch: 83 -- Batch: 000 -- Loss: 0.9330\n","Train Epoch: 83 -- Batch: 500 -- Loss: 0.9074\n","Train Epoch: 83 -- Batch: 1000 -- Loss: 0.9152\n","Train Epoch: 84 -- Batch: 000 -- Loss: 0.9495\n","Train Epoch: 84 -- Batch: 500 -- Loss: 0.9673\n","Train Epoch: 84 -- Batch: 1000 -- Loss: 0.8952\n","Train Epoch: 85 -- Batch: 000 -- Loss: 0.9457\n","Train Epoch: 85 -- Batch: 500 -- Loss: 0.9875\n","Train Epoch: 85 -- Batch: 1000 -- Loss: 0.9106\n","Train Epoch: 86 -- Batch: 000 -- Loss: 0.9825\n","Train Epoch: 86 -- Batch: 500 -- Loss: 0.9548\n","Train Epoch: 86 -- Batch: 1000 -- Loss: 0.8889\n","Train Epoch: 87 -- Batch: 000 -- Loss: 0.9323\n","Train Epoch: 87 -- Batch: 500 -- Loss: 0.9619\n","Train Epoch: 87 -- Batch: 1000 -- Loss: 0.9590\n","Train Epoch: 88 -- Batch: 000 -- Loss: 0.9736\n","Train Epoch: 88 -- Batch: 500 -- Loss: 0.9914\n","Train Epoch: 88 -- Batch: 1000 -- Loss: 0.8955\n","Train Epoch: 89 -- Batch: 000 -- Loss: 0.9395\n","Train Epoch: 89 -- Batch: 500 -- Loss: 0.9212\n","Train Epoch: 89 -- Batch: 1000 -- Loss: 0.9466\n","Train Epoch: 90 -- Batch: 000 -- Loss: 0.9709\n","Train Epoch: 90 -- Batch: 500 -- Loss: 0.9490\n","Train Epoch: 90 -- Batch: 1000 -- Loss: 0.9242\n","Train Epoch: 91 -- Batch: 000 -- Loss: 0.9105\n","Train Epoch: 91 -- Batch: 500 -- Loss: 0.9328\n","Train Epoch: 91 -- Batch: 1000 -- Loss: 0.8845\n","Train Epoch: 92 -- Batch: 000 -- Loss: 0.8949\n","Train Epoch: 92 -- Batch: 500 -- Loss: 0.9874\n","Train Epoch: 92 -- Batch: 1000 -- Loss: 0.9096\n","Train Epoch: 93 -- Batch: 000 -- Loss: 0.9560\n","Train Epoch: 93 -- Batch: 500 -- Loss: 0.9675\n","Train Epoch: 93 -- Batch: 1000 -- Loss: 0.8519\n","Train Epoch: 94 -- Batch: 000 -- Loss: 0.9645\n","Train Epoch: 94 -- Batch: 500 -- Loss: 0.9605\n","Train Epoch: 94 -- Batch: 1000 -- Loss: 0.8886\n","Train Epoch: 95 -- Batch: 000 -- Loss: 0.9603\n","Train Epoch: 95 -- Batch: 500 -- Loss: 0.9361\n","Train Epoch: 95 -- Batch: 1000 -- Loss: 0.8883\n","Train Epoch: 96 -- Batch: 000 -- Loss: 0.9539\n","Train Epoch: 96 -- Batch: 500 -- Loss: 0.9462\n","Train Epoch: 96 -- Batch: 1000 -- Loss: 0.9056\n","Train Epoch: 97 -- Batch: 000 -- Loss: 0.9722\n","Train Epoch: 97 -- Batch: 500 -- Loss: 0.9249\n","Train Epoch: 97 -- Batch: 1000 -- Loss: 0.8956\n","Train Epoch: 98 -- Batch: 000 -- Loss: 0.9529\n","Train Epoch: 98 -- Batch: 500 -- Loss: 0.9735\n","Train Epoch: 98 -- Batch: 1000 -- Loss: 0.9078\n","Train Epoch: 99 -- Batch: 000 -- Loss: 0.9770\n","Train Epoch: 99 -- Batch: 500 -- Loss: 0.9326\n","Train Epoch: 99 -- Batch: 1000 -- Loss: 0.9538\n","| \u001b[95m 3       \u001b[0m | \u001b[95m 0.6156  \u001b[0m | \u001b[95m 241.8   \u001b[0m | \u001b[95m 0.2694  \u001b[0m | \u001b[95m 0.0425  \u001b[0m | \u001b[95m 187.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6311\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.5716\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.4874\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.4844\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.4397\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.4042\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.3033\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.4092\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.3183\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.3360\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.2464\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.3484\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.2724\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.2490\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.2006\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.2773\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.2372\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.1687\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.2192\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.2162\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.1956\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.2007\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.1913\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.2152\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2103\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.2324\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.1548\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.1914\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.2114\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.2410\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.1335\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.1426\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.2205\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.1883\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.1292\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.1835\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.1857\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.1605\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.1370\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.1614\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.1256\n","Train Epoch: 10 -- Batch: 500 -- Loss: 1.1827\n","Train Epoch: 10 -- Batch: 1000 -- Loss: 1.1354\n","Train Epoch: 10 -- Batch: 1500 -- Loss: 1.1737\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.1939\n","Train Epoch: 11 -- Batch: 500 -- Loss: 1.1114\n","Train Epoch: 11 -- Batch: 1000 -- Loss: 1.1128\n","Train Epoch: 11 -- Batch: 1500 -- Loss: 1.1071\n","Train Epoch: 12 -- Batch: 000 -- Loss: 1.2009\n","Train Epoch: 12 -- Batch: 500 -- Loss: 1.1637\n","Train Epoch: 12 -- Batch: 1000 -- Loss: 1.1591\n","Train Epoch: 12 -- Batch: 1500 -- Loss: 1.1569\n","Train Epoch: 13 -- Batch: 000 -- Loss: 1.1607\n","Train Epoch: 13 -- Batch: 500 -- Loss: 1.1576\n","Train Epoch: 13 -- Batch: 1000 -- Loss: 1.1331\n","Train Epoch: 13 -- Batch: 1500 -- Loss: 1.1295\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.1984\n","Train Epoch: 14 -- Batch: 500 -- Loss: 1.1148\n","Train Epoch: 14 -- Batch: 1000 -- Loss: 1.1273\n","Train Epoch: 14 -- Batch: 1500 -- Loss: 1.1722\n","Train Epoch: 15 -- Batch: 000 -- Loss: 1.1233\n","Train Epoch: 15 -- Batch: 500 -- Loss: 1.1960\n","Train Epoch: 15 -- Batch: 1000 -- Loss: 1.1260\n","Train Epoch: 15 -- Batch: 1500 -- Loss: 1.1660\n","Train Epoch: 16 -- Batch: 000 -- Loss: 1.1247\n","Train Epoch: 16 -- Batch: 500 -- Loss: 1.1396\n","Train Epoch: 16 -- Batch: 1000 -- Loss: 1.0830\n","Train Epoch: 16 -- Batch: 1500 -- Loss: 1.1407\n","Train Epoch: 17 -- Batch: 000 -- Loss: 1.1582\n","Train Epoch: 17 -- Batch: 500 -- Loss: 1.1277\n","Train Epoch: 17 -- Batch: 1000 -- Loss: 1.1336\n","Train Epoch: 17 -- Batch: 1500 -- Loss: 1.1002\n","Train Epoch: 18 -- Batch: 000 -- Loss: 1.1188\n","Train Epoch: 18 -- Batch: 500 -- Loss: 1.1563\n","Train Epoch: 18 -- Batch: 1000 -- Loss: 1.1082\n","Train Epoch: 18 -- Batch: 1500 -- Loss: 1.1313\n","Train Epoch: 19 -- Batch: 000 -- Loss: 1.1313\n","Train Epoch: 19 -- Batch: 500 -- Loss: 1.1484\n","Train Epoch: 19 -- Batch: 1000 -- Loss: 1.1236\n","Train Epoch: 19 -- Batch: 1500 -- Loss: 1.1205\n","Train Epoch: 20 -- Batch: 000 -- Loss: 1.1502\n","Train Epoch: 20 -- Batch: 500 -- Loss: 1.1533\n","Train Epoch: 20 -- Batch: 1000 -- Loss: 1.0602\n","Train Epoch: 20 -- Batch: 1500 -- Loss: 1.1597\n","Train Epoch: 21 -- Batch: 000 -- Loss: 1.1372\n","Train Epoch: 21 -- Batch: 500 -- Loss: 1.1614\n","Train Epoch: 21 -- Batch: 1000 -- Loss: 1.1303\n","Train Epoch: 21 -- Batch: 1500 -- Loss: 1.1092\n","Train Epoch: 22 -- Batch: 000 -- Loss: 1.1065\n","Train Epoch: 22 -- Batch: 500 -- Loss: 1.0686\n","Train Epoch: 22 -- Batch: 1000 -- Loss: 1.1285\n","Train Epoch: 22 -- Batch: 1500 -- Loss: 1.0980\n","Train Epoch: 23 -- Batch: 000 -- Loss: 1.1413\n","Train Epoch: 23 -- Batch: 500 -- Loss: 1.1289\n","Train Epoch: 23 -- Batch: 1000 -- Loss: 1.0973\n","Train Epoch: 23 -- Batch: 1500 -- Loss: 1.1124\n","Train Epoch: 24 -- Batch: 000 -- Loss: 1.1224\n","Train Epoch: 24 -- Batch: 500 -- Loss: 1.1297\n","Train Epoch: 24 -- Batch: 1000 -- Loss: 1.0960\n","Train Epoch: 24 -- Batch: 1500 -- Loss: 1.1057\n","Train Epoch: 25 -- Batch: 000 -- Loss: 1.1573\n","Train Epoch: 25 -- Batch: 500 -- Loss: 1.1296\n","Train Epoch: 25 -- Batch: 1000 -- Loss: 1.1001\n","Train Epoch: 25 -- Batch: 1500 -- Loss: 1.0525\n","Train Epoch: 26 -- Batch: 000 -- Loss: 1.1303\n","Train Epoch: 26 -- Batch: 500 -- Loss: 1.1647\n","Train Epoch: 26 -- Batch: 1000 -- Loss: 1.1371\n","Train Epoch: 26 -- Batch: 1500 -- Loss: 1.1011\n","Train Epoch: 27 -- Batch: 000 -- Loss: 1.0926\n","Train Epoch: 27 -- Batch: 500 -- Loss: 1.1496\n","Train Epoch: 27 -- Batch: 1000 -- Loss: 1.0982\n","Train Epoch: 27 -- Batch: 1500 -- Loss: 1.1170\n","Train Epoch: 28 -- Batch: 000 -- Loss: 1.0964\n","Train Epoch: 28 -- Batch: 500 -- Loss: 1.1174\n","Train Epoch: 28 -- Batch: 1000 -- Loss: 1.1141\n","Train Epoch: 28 -- Batch: 1500 -- Loss: 1.0830\n","Train Epoch: 29 -- Batch: 000 -- Loss: 1.1030\n","Train Epoch: 29 -- Batch: 500 -- Loss: 1.1206\n","Train Epoch: 29 -- Batch: 1000 -- Loss: 1.0995\n","Train Epoch: 29 -- Batch: 1500 -- Loss: 1.1016\n","Train Epoch: 30 -- Batch: 000 -- Loss: 1.1125\n","Train Epoch: 30 -- Batch: 500 -- Loss: 1.1034\n","Train Epoch: 30 -- Batch: 1000 -- Loss: 1.0720\n","Train Epoch: 30 -- Batch: 1500 -- Loss: 1.0797\n","Train Epoch: 31 -- Batch: 000 -- Loss: 1.1631\n","Train Epoch: 31 -- Batch: 500 -- Loss: 1.1421\n","Train Epoch: 31 -- Batch: 1000 -- Loss: 1.1127\n","Train Epoch: 31 -- Batch: 1500 -- Loss: 1.0842\n","Train Epoch: 32 -- Batch: 000 -- Loss: 1.0894\n","Train Epoch: 32 -- Batch: 500 -- Loss: 1.1176\n","Train Epoch: 32 -- Batch: 1000 -- Loss: 1.1246\n","Train Epoch: 32 -- Batch: 1500 -- Loss: 1.0585\n","Train Epoch: 33 -- Batch: 000 -- Loss: 1.1058\n","Train Epoch: 33 -- Batch: 500 -- Loss: 1.0795\n","Train Epoch: 33 -- Batch: 1000 -- Loss: 1.1032\n","Train Epoch: 33 -- Batch: 1500 -- Loss: 1.1133\n","Train Epoch: 34 -- Batch: 000 -- Loss: 1.0898\n","Train Epoch: 34 -- Batch: 500 -- Loss: 1.1092\n","Train Epoch: 34 -- Batch: 1000 -- Loss: 1.0921\n","Train Epoch: 34 -- Batch: 1500 -- Loss: 1.0689\n","Train Epoch: 35 -- Batch: 000 -- Loss: 1.0979\n","Train Epoch: 35 -- Batch: 500 -- Loss: 1.1137\n","Train Epoch: 35 -- Batch: 1000 -- Loss: 1.0964\n","Train Epoch: 35 -- Batch: 1500 -- Loss: 1.1025\n","Train Epoch: 36 -- Batch: 000 -- Loss: 1.0720\n","Train Epoch: 36 -- Batch: 500 -- Loss: 1.1268\n","Train Epoch: 36 -- Batch: 1000 -- Loss: 1.1281\n","Train Epoch: 36 -- Batch: 1500 -- Loss: 1.0769\n","Train Epoch: 37 -- Batch: 000 -- Loss: 1.0453\n","Train Epoch: 37 -- Batch: 500 -- Loss: 1.1358\n","Train Epoch: 37 -- Batch: 1000 -- Loss: 1.0743\n","Train Epoch: 37 -- Batch: 1500 -- Loss: 1.0907\n","Train Epoch: 38 -- Batch: 000 -- Loss: 1.1236\n","Train Epoch: 38 -- Batch: 500 -- Loss: 1.1071\n","Train Epoch: 38 -- Batch: 1000 -- Loss: 1.1081\n","Train Epoch: 38 -- Batch: 1500 -- Loss: 1.1129\n","Train Epoch: 39 -- Batch: 000 -- Loss: 1.1035\n","Train Epoch: 39 -- Batch: 500 -- Loss: 1.0964\n","Train Epoch: 39 -- Batch: 1000 -- Loss: 1.1398\n","Train Epoch: 39 -- Batch: 1500 -- Loss: 1.0505\n","Train Epoch: 40 -- Batch: 000 -- Loss: 1.1068\n","Train Epoch: 40 -- Batch: 500 -- Loss: 1.1422\n","Train Epoch: 40 -- Batch: 1000 -- Loss: 1.0808\n","Train Epoch: 40 -- Batch: 1500 -- Loss: 1.0641\n","Train Epoch: 41 -- Batch: 000 -- Loss: 1.0475\n","Train Epoch: 41 -- Batch: 500 -- Loss: 1.0776\n","Train Epoch: 41 -- Batch: 1000 -- Loss: 1.1175\n","Train Epoch: 41 -- Batch: 1500 -- Loss: 1.0605\n","Train Epoch: 42 -- Batch: 000 -- Loss: 1.0999\n","Train Epoch: 42 -- Batch: 500 -- Loss: 1.0714\n","Train Epoch: 42 -- Batch: 1000 -- Loss: 1.1239\n","Train Epoch: 42 -- Batch: 1500 -- Loss: 1.0750\n","Train Epoch: 43 -- Batch: 000 -- Loss: 1.1392\n","Train Epoch: 43 -- Batch: 500 -- Loss: 1.0752\n","Train Epoch: 43 -- Batch: 1000 -- Loss: 1.1092\n","Train Epoch: 43 -- Batch: 1500 -- Loss: 1.0942\n","Train Epoch: 44 -- Batch: 000 -- Loss: 1.0909\n","Train Epoch: 44 -- Batch: 500 -- Loss: 1.1435\n","Train Epoch: 44 -- Batch: 1000 -- Loss: 1.0849\n","Train Epoch: 44 -- Batch: 1500 -- Loss: 1.0541\n","Train Epoch: 45 -- Batch: 000 -- Loss: 1.1125\n","Train Epoch: 45 -- Batch: 500 -- Loss: 1.1067\n","Train Epoch: 45 -- Batch: 1000 -- Loss: 1.0826\n","Train Epoch: 45 -- Batch: 1500 -- Loss: 1.0688\n","Train Epoch: 46 -- Batch: 000 -- Loss: 1.0747\n","Train Epoch: 46 -- Batch: 500 -- Loss: 1.1047\n","Train Epoch: 46 -- Batch: 1000 -- Loss: 1.1016\n","Train Epoch: 46 -- Batch: 1500 -- Loss: 1.0606\n","Train Epoch: 47 -- Batch: 000 -- Loss: 1.0972\n","Train Epoch: 47 -- Batch: 500 -- Loss: 1.1109\n","Train Epoch: 47 -- Batch: 1000 -- Loss: 1.0884\n","Train Epoch: 47 -- Batch: 1500 -- Loss: 1.0591\n","Train Epoch: 48 -- Batch: 000 -- Loss: 1.0883\n","Train Epoch: 48 -- Batch: 500 -- Loss: 1.0906\n","Train Epoch: 48 -- Batch: 1000 -- Loss: 1.1103\n","Train Epoch: 48 -- Batch: 1500 -- Loss: 1.0594\n","Train Epoch: 49 -- Batch: 000 -- Loss: 1.0631\n","Train Epoch: 49 -- Batch: 500 -- Loss: 1.0899\n","Train Epoch: 49 -- Batch: 1000 -- Loss: 1.0720\n","Train Epoch: 49 -- Batch: 1500 -- Loss: 1.0794\n","Train Epoch: 50 -- Batch: 000 -- Loss: 1.0688\n","Train Epoch: 50 -- Batch: 500 -- Loss: 1.1109\n","Train Epoch: 50 -- Batch: 1000 -- Loss: 1.0835\n","Train Epoch: 50 -- Batch: 1500 -- Loss: 1.0621\n","Train Epoch: 51 -- Batch: 000 -- Loss: 1.0491\n","Train Epoch: 51 -- Batch: 500 -- Loss: 1.1521\n","Train Epoch: 51 -- Batch: 1000 -- Loss: 1.1050\n","Train Epoch: 51 -- Batch: 1500 -- Loss: 1.0674\n","Train Epoch: 52 -- Batch: 000 -- Loss: 1.0907\n","Train Epoch: 52 -- Batch: 500 -- Loss: 1.1197\n","Train Epoch: 52 -- Batch: 1000 -- Loss: 1.1040\n","Train Epoch: 52 -- Batch: 1500 -- Loss: 1.0563\n","Train Epoch: 53 -- Batch: 000 -- Loss: 1.1123\n","Train Epoch: 53 -- Batch: 500 -- Loss: 1.0957\n","Train Epoch: 53 -- Batch: 1000 -- Loss: 1.0487\n","Train Epoch: 53 -- Batch: 1500 -- Loss: 1.0323\n","Train Epoch: 54 -- Batch: 000 -- Loss: 1.0693\n","Train Epoch: 54 -- Batch: 500 -- Loss: 1.1334\n","Train Epoch: 54 -- Batch: 1000 -- Loss: 1.0937\n","Train Epoch: 54 -- Batch: 1500 -- Loss: 1.0607\n","Train Epoch: 55 -- Batch: 000 -- Loss: 1.0472\n","Train Epoch: 55 -- Batch: 500 -- Loss: 1.0956\n","Train Epoch: 55 -- Batch: 1000 -- Loss: 1.0554\n","Train Epoch: 55 -- Batch: 1500 -- Loss: 1.0801\n","Train Epoch: 56 -- Batch: 000 -- Loss: 1.0663\n","Train Epoch: 56 -- Batch: 500 -- Loss: 1.0703\n","Train Epoch: 56 -- Batch: 1000 -- Loss: 1.1106\n","Train Epoch: 56 -- Batch: 1500 -- Loss: 1.0955\n","Train Epoch: 57 -- Batch: 000 -- Loss: 1.1164\n","Train Epoch: 57 -- Batch: 500 -- Loss: 1.1106\n","Train Epoch: 57 -- Batch: 1000 -- Loss: 1.1167\n","Train Epoch: 57 -- Batch: 1500 -- Loss: 1.0255\n","Train Epoch: 58 -- Batch: 000 -- Loss: 1.0635\n","Train Epoch: 58 -- Batch: 500 -- Loss: 1.1176\n","Train Epoch: 58 -- Batch: 1000 -- Loss: 1.1126\n","Train Epoch: 58 -- Batch: 1500 -- Loss: 1.0389\n","Train Epoch: 59 -- Batch: 000 -- Loss: 1.0477\n","Train Epoch: 59 -- Batch: 500 -- Loss: 1.1149\n","Train Epoch: 59 -- Batch: 1000 -- Loss: 1.1033\n","Train Epoch: 59 -- Batch: 1500 -- Loss: 1.0414\n","Train Epoch: 60 -- Batch: 000 -- Loss: 1.1176\n","Train Epoch: 60 -- Batch: 500 -- Loss: 1.0972\n","Train Epoch: 60 -- Batch: 1000 -- Loss: 1.1025\n","Train Epoch: 60 -- Batch: 1500 -- Loss: 1.0322\n","Train Epoch: 61 -- Batch: 000 -- Loss: 1.1040\n","Train Epoch: 61 -- Batch: 500 -- Loss: 1.0222\n","Train Epoch: 61 -- Batch: 1000 -- Loss: 1.1291\n","Train Epoch: 61 -- Batch: 1500 -- Loss: 1.0542\n","Train Epoch: 62 -- Batch: 000 -- Loss: 1.0453\n","Train Epoch: 62 -- Batch: 500 -- Loss: 1.0885\n","Train Epoch: 62 -- Batch: 1000 -- Loss: 1.0902\n","Train Epoch: 62 -- Batch: 1500 -- Loss: 1.0675\n","Train Epoch: 63 -- Batch: 000 -- Loss: 1.0754\n","Train Epoch: 63 -- Batch: 500 -- Loss: 1.0795\n","Train Epoch: 63 -- Batch: 1000 -- Loss: 1.0535\n","Train Epoch: 63 -- Batch: 1500 -- Loss: 1.0748\n","Train Epoch: 64 -- Batch: 000 -- Loss: 1.1021\n","Train Epoch: 64 -- Batch: 500 -- Loss: 1.0998\n","Train Epoch: 64 -- Batch: 1000 -- Loss: 1.1223\n","Train Epoch: 64 -- Batch: 1500 -- Loss: 1.0598\n","Train Epoch: 65 -- Batch: 000 -- Loss: 1.0742\n","Train Epoch: 65 -- Batch: 500 -- Loss: 1.0991\n","Train Epoch: 65 -- Batch: 1000 -- Loss: 1.1319\n","Train Epoch: 65 -- Batch: 1500 -- Loss: 1.0661\n","Train Epoch: 66 -- Batch: 000 -- Loss: 1.0504\n","Train Epoch: 66 -- Batch: 500 -- Loss: 1.0848\n","Train Epoch: 66 -- Batch: 1000 -- Loss: 1.0605\n","Train Epoch: 66 -- Batch: 1500 -- Loss: 1.0528\n","Train Epoch: 67 -- Batch: 000 -- Loss: 1.0587\n","Train Epoch: 67 -- Batch: 500 -- Loss: 1.0688\n","Train Epoch: 67 -- Batch: 1000 -- Loss: 1.1062\n","Train Epoch: 67 -- Batch: 1500 -- Loss: 1.0191\n","Train Epoch: 68 -- Batch: 000 -- Loss: 1.0750\n","Train Epoch: 68 -- Batch: 500 -- Loss: 1.0780\n","Train Epoch: 68 -- Batch: 1000 -- Loss: 1.0583\n","Train Epoch: 68 -- Batch: 1500 -- Loss: 1.0436\n","Train Epoch: 69 -- Batch: 000 -- Loss: 1.1005\n","Train Epoch: 69 -- Batch: 500 -- Loss: 1.0647\n","Train Epoch: 69 -- Batch: 1000 -- Loss: 1.0815\n","Train Epoch: 69 -- Batch: 1500 -- Loss: 1.0343\n","Train Epoch: 70 -- Batch: 000 -- Loss: 1.0426\n","Train Epoch: 70 -- Batch: 500 -- Loss: 1.1096\n","Train Epoch: 70 -- Batch: 1000 -- Loss: 1.0617\n","Train Epoch: 70 -- Batch: 1500 -- Loss: 1.0218\n","Train Epoch: 71 -- Batch: 000 -- Loss: 1.0643\n","Train Epoch: 71 -- Batch: 500 -- Loss: 1.0862\n","Train Epoch: 71 -- Batch: 1000 -- Loss: 1.0535\n","Train Epoch: 71 -- Batch: 1500 -- Loss: 1.0827\n","Train Epoch: 72 -- Batch: 000 -- Loss: 1.0642\n","Train Epoch: 72 -- Batch: 500 -- Loss: 1.0753\n","Train Epoch: 72 -- Batch: 1000 -- Loss: 1.1005\n","Train Epoch: 72 -- Batch: 1500 -- Loss: 1.0452\n","Train Epoch: 73 -- Batch: 000 -- Loss: 1.0269\n","Train Epoch: 73 -- Batch: 500 -- Loss: 1.0886\n","Train Epoch: 73 -- Batch: 1000 -- Loss: 1.0885\n","Train Epoch: 73 -- Batch: 1500 -- Loss: 1.0112\n","Train Epoch: 74 -- Batch: 000 -- Loss: 1.0430\n","Train Epoch: 74 -- Batch: 500 -- Loss: 1.1253\n","Train Epoch: 74 -- Batch: 1000 -- Loss: 1.0488\n","Train Epoch: 74 -- Batch: 1500 -- Loss: 1.0426\n","Train Epoch: 75 -- Batch: 000 -- Loss: 1.0237\n","Train Epoch: 75 -- Batch: 500 -- Loss: 1.1059\n","Train Epoch: 75 -- Batch: 1000 -- Loss: 1.0648\n","Train Epoch: 75 -- Batch: 1500 -- Loss: 1.0490\n","Train Epoch: 76 -- Batch: 000 -- Loss: 1.0590\n","Train Epoch: 76 -- Batch: 500 -- Loss: 1.1236\n","Train Epoch: 76 -- Batch: 1000 -- Loss: 1.0985\n","Train Epoch: 76 -- Batch: 1500 -- Loss: 1.0077\n","Train Epoch: 77 -- Batch: 000 -- Loss: 1.0452\n","Train Epoch: 77 -- Batch: 500 -- Loss: 1.0683\n","Train Epoch: 77 -- Batch: 1000 -- Loss: 1.0281\n","Train Epoch: 77 -- Batch: 1500 -- Loss: 1.0413\n","Train Epoch: 78 -- Batch: 000 -- Loss: 1.0241\n","Train Epoch: 78 -- Batch: 500 -- Loss: 1.0818\n","Train Epoch: 78 -- Batch: 1000 -- Loss: 1.0825\n","Train Epoch: 78 -- Batch: 1500 -- Loss: 1.0227\n","Train Epoch: 79 -- Batch: 000 -- Loss: 1.0087\n","Train Epoch: 79 -- Batch: 500 -- Loss: 1.0821\n","Train Epoch: 79 -- Batch: 1000 -- Loss: 1.0685\n","Train Epoch: 79 -- Batch: 1500 -- Loss: 1.0592\n","Train Epoch: 80 -- Batch: 000 -- Loss: 1.0574\n","Train Epoch: 80 -- Batch: 500 -- Loss: 1.0971\n","Train Epoch: 80 -- Batch: 1000 -- Loss: 1.1319\n","Train Epoch: 80 -- Batch: 1500 -- Loss: 1.0171\n","Train Epoch: 81 -- Batch: 000 -- Loss: 1.0516\n","Train Epoch: 81 -- Batch: 500 -- Loss: 1.0645\n","Train Epoch: 81 -- Batch: 1000 -- Loss: 1.0759\n","Train Epoch: 81 -- Batch: 1500 -- Loss: 1.0140\n","Train Epoch: 82 -- Batch: 000 -- Loss: 1.0562\n","Train Epoch: 82 -- Batch: 500 -- Loss: 1.0252\n","Train Epoch: 82 -- Batch: 1000 -- Loss: 1.0853\n","Train Epoch: 82 -- Batch: 1500 -- Loss: 1.0329\n","Train Epoch: 83 -- Batch: 000 -- Loss: 1.0637\n","Train Epoch: 83 -- Batch: 500 -- Loss: 1.1021\n","Train Epoch: 83 -- Batch: 1000 -- Loss: 1.1004\n","Train Epoch: 83 -- Batch: 1500 -- Loss: 1.0187\n","Train Epoch: 84 -- Batch: 000 -- Loss: 1.0634\n","Train Epoch: 84 -- Batch: 500 -- Loss: 1.0648\n","Train Epoch: 84 -- Batch: 1000 -- Loss: 1.0803\n","Train Epoch: 84 -- Batch: 1500 -- Loss: 1.0049\n","Train Epoch: 85 -- Batch: 000 -- Loss: 1.0451\n","Train Epoch: 85 -- Batch: 500 -- Loss: 1.1021\n","Train Epoch: 85 -- Batch: 1000 -- Loss: 1.1161\n","Train Epoch: 85 -- Batch: 1500 -- Loss: 1.0086\n","Train Epoch: 86 -- Batch: 000 -- Loss: 0.9915\n","Train Epoch: 86 -- Batch: 500 -- Loss: 1.0208\n","Train Epoch: 86 -- Batch: 1000 -- Loss: 1.0767\n","Train Epoch: 86 -- Batch: 1500 -- Loss: 0.9973\n","Train Epoch: 87 -- Batch: 000 -- Loss: 1.0253\n","Train Epoch: 87 -- Batch: 500 -- Loss: 1.0911\n","Train Epoch: 87 -- Batch: 1000 -- Loss: 1.0320\n","Train Epoch: 87 -- Batch: 1500 -- Loss: 1.0340\n","Train Epoch: 88 -- Batch: 000 -- Loss: 1.0203\n","Train Epoch: 88 -- Batch: 500 -- Loss: 1.0877\n","Train Epoch: 88 -- Batch: 1000 -- Loss: 1.0953\n","Train Epoch: 88 -- Batch: 1500 -- Loss: 1.0103\n","Train Epoch: 89 -- Batch: 000 -- Loss: 1.1139\n","Train Epoch: 89 -- Batch: 500 -- Loss: 1.0869\n","Train Epoch: 89 -- Batch: 1000 -- Loss: 1.1149\n","Train Epoch: 89 -- Batch: 1500 -- Loss: 1.0405\n","Train Epoch: 90 -- Batch: 000 -- Loss: 1.0205\n","Train Epoch: 90 -- Batch: 500 -- Loss: 1.1249\n","Train Epoch: 90 -- Batch: 1000 -- Loss: 1.1091\n","Train Epoch: 90 -- Batch: 1500 -- Loss: 1.0488\n","Train Epoch: 91 -- Batch: 000 -- Loss: 1.0159\n","Train Epoch: 91 -- Batch: 500 -- Loss: 1.0601\n","Train Epoch: 91 -- Batch: 1000 -- Loss: 1.1101\n","Train Epoch: 91 -- Batch: 1500 -- Loss: 1.0401\n","Train Epoch: 92 -- Batch: 000 -- Loss: 1.0559\n","Train Epoch: 92 -- Batch: 500 -- Loss: 1.0328\n","Train Epoch: 92 -- Batch: 1000 -- Loss: 1.0803\n","Train Epoch: 92 -- Batch: 1500 -- Loss: 0.9968\n","Train Epoch: 93 -- Batch: 000 -- Loss: 1.0053\n","Train Epoch: 93 -- Batch: 500 -- Loss: 1.0296\n","Train Epoch: 93 -- Batch: 1000 -- Loss: 1.0711\n","Train Epoch: 93 -- Batch: 1500 -- Loss: 1.0573\n","Train Epoch: 94 -- Batch: 000 -- Loss: 1.0213\n","Train Epoch: 94 -- Batch: 500 -- Loss: 1.0777\n","Train Epoch: 94 -- Batch: 1000 -- Loss: 1.0704\n","Train Epoch: 94 -- Batch: 1500 -- Loss: 1.0466\n","Train Epoch: 95 -- Batch: 000 -- Loss: 1.0436\n","Train Epoch: 95 -- Batch: 500 -- Loss: 1.0771\n","Train Epoch: 95 -- Batch: 1000 -- Loss: 1.0562\n","Train Epoch: 95 -- Batch: 1500 -- Loss: 0.9831\n","Train Epoch: 96 -- Batch: 000 -- Loss: 1.0624\n","Train Epoch: 96 -- Batch: 500 -- Loss: 1.1325\n","Train Epoch: 96 -- Batch: 1000 -- Loss: 1.0348\n","Train Epoch: 96 -- Batch: 1500 -- Loss: 1.0540\n","Train Epoch: 97 -- Batch: 000 -- Loss: 1.0411\n","Train Epoch: 97 -- Batch: 500 -- Loss: 1.0716\n","Train Epoch: 97 -- Batch: 1000 -- Loss: 1.0829\n","Train Epoch: 97 -- Batch: 1500 -- Loss: 1.0162\n","Train Epoch: 98 -- Batch: 000 -- Loss: 1.0569\n","Train Epoch: 98 -- Batch: 500 -- Loss: 1.0159\n","Train Epoch: 98 -- Batch: 1000 -- Loss: 1.1089\n","Train Epoch: 98 -- Batch: 1500 -- Loss: 1.0555\n","Train Epoch: 99 -- Batch: 000 -- Loss: 1.0461\n","Train Epoch: 99 -- Batch: 500 -- Loss: 1.0785\n","Train Epoch: 99 -- Batch: 1000 -- Loss: 1.0731\n","Train Epoch: 99 -- Batch: 1500 -- Loss: 1.0115\n","| \u001b[0m 4       \u001b[0m | \u001b[0m 0.6095  \u001b[0m | \u001b[0m 155.6   \u001b[0m | \u001b[0m 0.4391  \u001b[0m | \u001b[0m 0.003711\u001b[0m | \u001b[0m 184.1   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6039\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.3518\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.2990\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.1970\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.2333\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.1393\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.1984\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.1227\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.2063\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.1082\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.1722\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.0524\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.1493\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.1151\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.1362\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.1171\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.1592\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.0719\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.1271\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.0791\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.1224\n","Train Epoch: 10 -- Batch: 500 -- Loss: 1.0298\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.1666\n","Train Epoch: 11 -- Batch: 500 -- Loss: 1.0696\n","Train Epoch: 12 -- Batch: 000 -- Loss: 1.1319\n","Train Epoch: 12 -- Batch: 500 -- Loss: 1.0700\n","Train Epoch: 13 -- Batch: 000 -- Loss: 1.1223\n","Train Epoch: 13 -- Batch: 500 -- Loss: 1.0902\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.1148\n","Train Epoch: 14 -- Batch: 500 -- Loss: 1.0695\n","Train Epoch: 15 -- Batch: 000 -- Loss: 1.0962\n","Train Epoch: 15 -- Batch: 500 -- Loss: 1.0533\n","Train Epoch: 16 -- Batch: 000 -- Loss: 1.1162\n","Train Epoch: 16 -- Batch: 500 -- Loss: 1.0858\n","Train Epoch: 17 -- Batch: 000 -- Loss: 1.0830\n","Train Epoch: 17 -- Batch: 500 -- Loss: 1.0606\n","Train Epoch: 18 -- Batch: 000 -- Loss: 1.1043\n","Train Epoch: 18 -- Batch: 500 -- Loss: 1.0434\n","Train Epoch: 19 -- Batch: 000 -- Loss: 1.1139\n","Train Epoch: 19 -- Batch: 500 -- Loss: 1.0395\n","Train Epoch: 20 -- Batch: 000 -- Loss: 1.0817\n","Train Epoch: 20 -- Batch: 500 -- Loss: 1.0581\n","Train Epoch: 21 -- Batch: 000 -- Loss: 1.0712\n","Train Epoch: 21 -- Batch: 500 -- Loss: 1.0282\n","Train Epoch: 22 -- Batch: 000 -- Loss: 1.0888\n","Train Epoch: 22 -- Batch: 500 -- Loss: 1.0176\n","Train Epoch: 23 -- Batch: 000 -- Loss: 1.0909\n","Train Epoch: 23 -- Batch: 500 -- Loss: 1.0321\n","Train Epoch: 24 -- Batch: 000 -- Loss: 1.0834\n","Train Epoch: 24 -- Batch: 500 -- Loss: 1.0202\n","Train Epoch: 25 -- Batch: 000 -- Loss: 1.0909\n","Train Epoch: 25 -- Batch: 500 -- Loss: 1.0375\n","Train Epoch: 26 -- Batch: 000 -- Loss: 1.0761\n","Train Epoch: 26 -- Batch: 500 -- Loss: 1.0346\n","Train Epoch: 27 -- Batch: 000 -- Loss: 1.0867\n","Train Epoch: 27 -- Batch: 500 -- Loss: 1.0349\n","Train Epoch: 28 -- Batch: 000 -- Loss: 1.0649\n","Train Epoch: 28 -- Batch: 500 -- Loss: 1.0180\n","Train Epoch: 29 -- Batch: 000 -- Loss: 1.0812\n","Train Epoch: 29 -- Batch: 500 -- Loss: 1.0090\n","Train Epoch: 30 -- Batch: 000 -- Loss: 1.0878\n","Train Epoch: 30 -- Batch: 500 -- Loss: 1.0270\n","Train Epoch: 31 -- Batch: 000 -- Loss: 1.0781\n","Train Epoch: 31 -- Batch: 500 -- Loss: 1.0416\n","Train Epoch: 32 -- Batch: 000 -- Loss: 1.0673\n","Train Epoch: 32 -- Batch: 500 -- Loss: 0.9877\n","Train Epoch: 33 -- Batch: 000 -- Loss: 1.0790\n","Train Epoch: 33 -- Batch: 500 -- Loss: 1.0037\n","Train Epoch: 34 -- Batch: 000 -- Loss: 1.0347\n","Train Epoch: 34 -- Batch: 500 -- Loss: 0.9882\n","Train Epoch: 35 -- Batch: 000 -- Loss: 1.0854\n","Train Epoch: 35 -- Batch: 500 -- Loss: 1.0269\n","Train Epoch: 36 -- Batch: 000 -- Loss: 1.0644\n","Train Epoch: 36 -- Batch: 500 -- Loss: 1.0120\n","Train Epoch: 37 -- Batch: 000 -- Loss: 1.0766\n","Train Epoch: 37 -- Batch: 500 -- Loss: 0.9875\n","Train Epoch: 38 -- Batch: 000 -- Loss: 1.0720\n","Train Epoch: 38 -- Batch: 500 -- Loss: 0.9767\n","Train Epoch: 39 -- Batch: 000 -- Loss: 1.0563\n","Train Epoch: 39 -- Batch: 500 -- Loss: 1.0097\n","Train Epoch: 40 -- Batch: 000 -- Loss: 1.0497\n","Train Epoch: 40 -- Batch: 500 -- Loss: 0.9761\n","Train Epoch: 41 -- Batch: 000 -- Loss: 1.0518\n","Train Epoch: 41 -- Batch: 500 -- Loss: 0.9991\n","Train Epoch: 42 -- Batch: 000 -- Loss: 1.0486\n","Train Epoch: 42 -- Batch: 500 -- Loss: 1.0046\n","Train Epoch: 43 -- Batch: 000 -- Loss: 1.0645\n","Train Epoch: 43 -- Batch: 500 -- Loss: 1.0141\n","Train Epoch: 44 -- Batch: 000 -- Loss: 1.0457\n","Train Epoch: 44 -- Batch: 500 -- Loss: 1.0100\n","Train Epoch: 45 -- Batch: 000 -- Loss: 1.0564\n","Train Epoch: 45 -- Batch: 500 -- Loss: 0.9782\n","Train Epoch: 46 -- Batch: 000 -- Loss: 1.0822\n","Train Epoch: 46 -- Batch: 500 -- Loss: 1.0201\n","Train Epoch: 47 -- Batch: 000 -- Loss: 1.0748\n","Train Epoch: 47 -- Batch: 500 -- Loss: 0.9900\n","Train Epoch: 48 -- Batch: 000 -- Loss: 1.0337\n","Train Epoch: 48 -- Batch: 500 -- Loss: 0.9852\n","Train Epoch: 49 -- Batch: 000 -- Loss: 1.0611\n","Train Epoch: 49 -- Batch: 500 -- Loss: 1.0256\n","Train Epoch: 50 -- Batch: 000 -- Loss: 1.0354\n","Train Epoch: 50 -- Batch: 500 -- Loss: 1.0041\n","Train Epoch: 51 -- Batch: 000 -- Loss: 1.0370\n","Train Epoch: 51 -- Batch: 500 -- Loss: 0.9921\n","Train Epoch: 52 -- Batch: 000 -- Loss: 1.0650\n","Train Epoch: 52 -- Batch: 500 -- Loss: 0.9982\n","Train Epoch: 53 -- Batch: 000 -- Loss: 1.0218\n","Train Epoch: 53 -- Batch: 500 -- Loss: 0.9951\n","Train Epoch: 54 -- Batch: 000 -- Loss: 1.0744\n","Train Epoch: 54 -- Batch: 500 -- Loss: 0.9771\n","Train Epoch: 55 -- Batch: 000 -- Loss: 1.0347\n","Train Epoch: 55 -- Batch: 500 -- Loss: 0.9984\n","Train Epoch: 56 -- Batch: 000 -- Loss: 1.0567\n","Train Epoch: 56 -- Batch: 500 -- Loss: 0.9987\n","Train Epoch: 57 -- Batch: 000 -- Loss: 1.0581\n","Train Epoch: 57 -- Batch: 500 -- Loss: 1.0134\n","Train Epoch: 58 -- Batch: 000 -- Loss: 1.0506\n","Train Epoch: 58 -- Batch: 500 -- Loss: 0.9888\n","Train Epoch: 59 -- Batch: 000 -- Loss: 1.0222\n","Train Epoch: 59 -- Batch: 500 -- Loss: 1.0320\n","Train Epoch: 60 -- Batch: 000 -- Loss: 1.0347\n","Train Epoch: 60 -- Batch: 500 -- Loss: 0.9897\n","Train Epoch: 61 -- Batch: 000 -- Loss: 1.0104\n","Train Epoch: 61 -- Batch: 500 -- Loss: 0.9916\n","Train Epoch: 62 -- Batch: 000 -- Loss: 1.0908\n","Train Epoch: 62 -- Batch: 500 -- Loss: 0.9982\n","Train Epoch: 63 -- Batch: 000 -- Loss: 1.0387\n","Train Epoch: 63 -- Batch: 500 -- Loss: 0.9564\n","Train Epoch: 64 -- Batch: 000 -- Loss: 1.0426\n","Train Epoch: 64 -- Batch: 500 -- Loss: 0.9821\n","Train Epoch: 65 -- Batch: 000 -- Loss: 1.0292\n","Train Epoch: 65 -- Batch: 500 -- Loss: 0.9718\n","Train Epoch: 66 -- Batch: 000 -- Loss: 1.0359\n","Train Epoch: 66 -- Batch: 500 -- Loss: 0.9616\n","Train Epoch: 67 -- Batch: 000 -- Loss: 1.0413\n","Train Epoch: 67 -- Batch: 500 -- Loss: 0.9817\n","Train Epoch: 68 -- Batch: 000 -- Loss: 1.0253\n","Train Epoch: 68 -- Batch: 500 -- Loss: 1.0157\n","Train Epoch: 69 -- Batch: 000 -- Loss: 1.0256\n","Train Epoch: 69 -- Batch: 500 -- Loss: 0.9661\n","Train Epoch: 70 -- Batch: 000 -- Loss: 1.0605\n","Train Epoch: 70 -- Batch: 500 -- Loss: 0.9678\n","Train Epoch: 71 -- Batch: 000 -- Loss: 1.0355\n","Train Epoch: 71 -- Batch: 500 -- Loss: 0.9630\n","Train Epoch: 72 -- Batch: 000 -- Loss: 1.0340\n","Train Epoch: 72 -- Batch: 500 -- Loss: 0.9839\n","Train Epoch: 73 -- Batch: 000 -- Loss: 1.0600\n","Train Epoch: 73 -- Batch: 500 -- Loss: 0.9709\n","Train Epoch: 74 -- Batch: 000 -- Loss: 1.0514\n","Train Epoch: 74 -- Batch: 500 -- Loss: 0.9582\n","Train Epoch: 75 -- Batch: 000 -- Loss: 1.0349\n","Train Epoch: 75 -- Batch: 500 -- Loss: 0.9377\n","Train Epoch: 76 -- Batch: 000 -- Loss: 1.0136\n","Train Epoch: 76 -- Batch: 500 -- Loss: 0.9822\n","Train Epoch: 77 -- Batch: 000 -- Loss: 1.0696\n","Train Epoch: 77 -- Batch: 500 -- Loss: 0.9820\n","Train Epoch: 78 -- Batch: 000 -- Loss: 1.0281\n","Train Epoch: 78 -- Batch: 500 -- Loss: 0.9690\n","Train Epoch: 79 -- Batch: 000 -- Loss: 1.0424\n","Train Epoch: 79 -- Batch: 500 -- Loss: 1.0064\n","Train Epoch: 80 -- Batch: 000 -- Loss: 1.0232\n","Train Epoch: 80 -- Batch: 500 -- Loss: 0.9721\n","Train Epoch: 81 -- Batch: 000 -- Loss: 1.0418\n","Train Epoch: 81 -- Batch: 500 -- Loss: 0.9646\n","Train Epoch: 82 -- Batch: 000 -- Loss: 1.0193\n","Train Epoch: 82 -- Batch: 500 -- Loss: 0.9510\n","Train Epoch: 83 -- Batch: 000 -- Loss: 1.0237\n","Train Epoch: 83 -- Batch: 500 -- Loss: 0.9690\n","Train Epoch: 84 -- Batch: 000 -- Loss: 1.0021\n","Train Epoch: 84 -- Batch: 500 -- Loss: 1.0116\n","Train Epoch: 85 -- Batch: 000 -- Loss: 1.0140\n","Train Epoch: 85 -- Batch: 500 -- Loss: 0.9823\n","Train Epoch: 86 -- Batch: 000 -- Loss: 1.0021\n","Train Epoch: 86 -- Batch: 500 -- Loss: 0.9808\n","Train Epoch: 87 -- Batch: 000 -- Loss: 1.0073\n","Train Epoch: 87 -- Batch: 500 -- Loss: 0.9832\n","Train Epoch: 88 -- Batch: 000 -- Loss: 1.0357\n","Train Epoch: 88 -- Batch: 500 -- Loss: 0.9492\n","Train Epoch: 89 -- Batch: 000 -- Loss: 1.0323\n","Train Epoch: 89 -- Batch: 500 -- Loss: 0.9915\n","Train Epoch: 90 -- Batch: 000 -- Loss: 1.0276\n","Train Epoch: 90 -- Batch: 500 -- Loss: 0.9876\n","Train Epoch: 91 -- Batch: 000 -- Loss: 1.0379\n","Train Epoch: 91 -- Batch: 500 -- Loss: 0.9712\n","Train Epoch: 92 -- Batch: 000 -- Loss: 1.0109\n","Train Epoch: 92 -- Batch: 500 -- Loss: 0.9740\n","Train Epoch: 93 -- Batch: 000 -- Loss: 1.0249\n","Train Epoch: 93 -- Batch: 500 -- Loss: 0.9556\n","Train Epoch: 94 -- Batch: 000 -- Loss: 1.0066\n","Train Epoch: 94 -- Batch: 500 -- Loss: 0.9578\n","Train Epoch: 95 -- Batch: 000 -- Loss: 1.0366\n","Train Epoch: 95 -- Batch: 500 -- Loss: 0.9595\n","Train Epoch: 96 -- Batch: 000 -- Loss: 1.0302\n","Train Epoch: 96 -- Batch: 500 -- Loss: 0.9834\n","Train Epoch: 97 -- Batch: 000 -- Loss: 1.0448\n","Train Epoch: 97 -- Batch: 500 -- Loss: 1.0098\n","Train Epoch: 98 -- Batch: 000 -- Loss: 1.0255\n","Train Epoch: 98 -- Batch: 500 -- Loss: 0.9704\n","Train Epoch: 99 -- Batch: 000 -- Loss: 1.0136\n","Train Epoch: 99 -- Batch: 500 -- Loss: 0.9437\n","| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6138  \u001b[0m | \u001b[0m 251.0   \u001b[0m | \u001b[0m 0.2793  \u001b[0m | \u001b[0m 0.0149  \u001b[0m | \u001b[0m 89.62   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6156\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.3027\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.2950\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.2232\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.2219\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.1836\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.1401\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.1660\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.1327\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.1767\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.1189\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.1622\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.1406\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.1436\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.1122\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.1428\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.0949\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.1332\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.1261\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.1189\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.1254\n","Train Epoch: 10 -- Batch: 500 -- Loss: 1.1289\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.1169\n","Train Epoch: 11 -- Batch: 500 -- Loss: 1.1433\n","Train Epoch: 12 -- Batch: 000 -- Loss: 1.1072\n","Train Epoch: 12 -- Batch: 500 -- Loss: 1.1147\n","Train Epoch: 13 -- Batch: 000 -- Loss: 1.0848\n","Train Epoch: 13 -- Batch: 500 -- Loss: 1.1162\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.1072\n","Train Epoch: 14 -- Batch: 500 -- Loss: 1.0959\n","Train Epoch: 15 -- Batch: 000 -- Loss: 1.0917\n","Train Epoch: 15 -- Batch: 500 -- Loss: 1.1235\n","Train Epoch: 16 -- Batch: 000 -- Loss: 1.0746\n","Train Epoch: 16 -- Batch: 500 -- Loss: 1.1125\n","Train Epoch: 17 -- Batch: 000 -- Loss: 1.0797\n","Train Epoch: 17 -- Batch: 500 -- Loss: 1.0863\n","Train Epoch: 18 -- Batch: 000 -- Loss: 1.1150\n","Train Epoch: 18 -- Batch: 500 -- Loss: 1.0961\n","Train Epoch: 19 -- Batch: 000 -- Loss: 1.0810\n","Train Epoch: 19 -- Batch: 500 -- Loss: 1.1094\n","Train Epoch: 20 -- Batch: 000 -- Loss: 1.0626\n","Train Epoch: 20 -- Batch: 500 -- Loss: 1.1145\n","Train Epoch: 21 -- Batch: 000 -- Loss: 1.0741\n","Train Epoch: 21 -- Batch: 500 -- Loss: 1.0982\n","Train Epoch: 22 -- Batch: 000 -- Loss: 1.0674\n","Train Epoch: 22 -- Batch: 500 -- Loss: 1.0849\n","Train Epoch: 23 -- Batch: 000 -- Loss: 1.0519\n","Train Epoch: 23 -- Batch: 500 -- Loss: 1.0684\n","Train Epoch: 24 -- Batch: 000 -- Loss: 1.0732\n","Train Epoch: 24 -- Batch: 500 -- Loss: 1.1013\n","Train Epoch: 25 -- Batch: 000 -- Loss: 1.0554\n","Train Epoch: 25 -- Batch: 500 -- Loss: 1.0749\n","Train Epoch: 26 -- Batch: 000 -- Loss: 1.0805\n","Train Epoch: 26 -- Batch: 500 -- Loss: 1.0935\n","Train Epoch: 27 -- Batch: 000 -- Loss: 1.0839\n","Train Epoch: 27 -- Batch: 500 -- Loss: 1.0781\n","Train Epoch: 28 -- Batch: 000 -- Loss: 1.0357\n","Train Epoch: 28 -- Batch: 500 -- Loss: 1.0677\n","Train Epoch: 29 -- Batch: 000 -- Loss: 1.0411\n","Train Epoch: 29 -- Batch: 500 -- Loss: 1.0881\n","Train Epoch: 30 -- Batch: 000 -- Loss: 1.0585\n","Train Epoch: 30 -- Batch: 500 -- Loss: 1.0697\n","Train Epoch: 31 -- Batch: 000 -- Loss: 1.0624\n","Train Epoch: 31 -- Batch: 500 -- Loss: 1.0861\n","Train Epoch: 32 -- Batch: 000 -- Loss: 1.0412\n","Train Epoch: 32 -- Batch: 500 -- Loss: 1.0596\n","Train Epoch: 33 -- Batch: 000 -- Loss: 1.0468\n","Train Epoch: 33 -- Batch: 500 -- Loss: 1.0870\n","Train Epoch: 34 -- Batch: 000 -- Loss: 1.0195\n","Train Epoch: 34 -- Batch: 500 -- Loss: 1.0430\n","Train Epoch: 35 -- Batch: 000 -- Loss: 1.0472\n","Train Epoch: 35 -- Batch: 500 -- Loss: 1.0568\n","Train Epoch: 36 -- Batch: 000 -- Loss: 1.0459\n","Train Epoch: 36 -- Batch: 500 -- Loss: 1.0619\n","Train Epoch: 37 -- Batch: 000 -- Loss: 1.0411\n","Train Epoch: 37 -- Batch: 500 -- Loss: 1.0495\n","Train Epoch: 38 -- Batch: 000 -- Loss: 1.0393\n","Train Epoch: 38 -- Batch: 500 -- Loss: 1.0456\n","Train Epoch: 39 -- Batch: 000 -- Loss: 1.0169\n","Train Epoch: 39 -- Batch: 500 -- Loss: 1.0687\n","Train Epoch: 40 -- Batch: 000 -- Loss: 1.0106\n","Train Epoch: 40 -- Batch: 500 -- Loss: 1.0595\n","Train Epoch: 41 -- Batch: 000 -- Loss: 1.0382\n","Train Epoch: 41 -- Batch: 500 -- Loss: 1.0371\n","Train Epoch: 42 -- Batch: 000 -- Loss: 1.0447\n","Train Epoch: 42 -- Batch: 500 -- Loss: 1.0445\n","Train Epoch: 43 -- Batch: 000 -- Loss: 1.0268\n","Train Epoch: 43 -- Batch: 500 -- Loss: 1.0518\n","Train Epoch: 44 -- Batch: 000 -- Loss: 0.9900\n","Train Epoch: 44 -- Batch: 500 -- Loss: 1.0481\n","Train Epoch: 45 -- Batch: 000 -- Loss: 1.0107\n","Train Epoch: 45 -- Batch: 500 -- Loss: 1.0325\n","Train Epoch: 46 -- Batch: 000 -- Loss: 1.0117\n","Train Epoch: 46 -- Batch: 500 -- Loss: 1.0747\n","Train Epoch: 47 -- Batch: 000 -- Loss: 1.0013\n","Train Epoch: 47 -- Batch: 500 -- Loss: 1.0291\n","Train Epoch: 48 -- Batch: 000 -- Loss: 0.9941\n","Train Epoch: 48 -- Batch: 500 -- Loss: 1.0348\n","Train Epoch: 49 -- Batch: 000 -- Loss: 1.0048\n","Train Epoch: 49 -- Batch: 500 -- Loss: 1.0702\n","Train Epoch: 50 -- Batch: 000 -- Loss: 1.0318\n","Train Epoch: 50 -- Batch: 500 -- Loss: 1.0335\n","Train Epoch: 51 -- Batch: 000 -- Loss: 1.0313\n","Train Epoch: 51 -- Batch: 500 -- Loss: 1.0422\n","Train Epoch: 52 -- Batch: 000 -- Loss: 1.0004\n","Train Epoch: 52 -- Batch: 500 -- Loss: 1.0460\n","Train Epoch: 53 -- Batch: 000 -- Loss: 1.0143\n","Train Epoch: 53 -- Batch: 500 -- Loss: 1.0269\n","Train Epoch: 54 -- Batch: 000 -- Loss: 1.0263\n","Train Epoch: 54 -- Batch: 500 -- Loss: 1.0388\n","Train Epoch: 55 -- Batch: 000 -- Loss: 1.0377\n","Train Epoch: 55 -- Batch: 500 -- Loss: 1.0349\n","Train Epoch: 56 -- Batch: 000 -- Loss: 1.0259\n","Train Epoch: 56 -- Batch: 500 -- Loss: 1.0214\n","Train Epoch: 57 -- Batch: 000 -- Loss: 1.0089\n","Train Epoch: 57 -- Batch: 500 -- Loss: 1.0270\n","Train Epoch: 58 -- Batch: 000 -- Loss: 1.0147\n","Train Epoch: 58 -- Batch: 500 -- Loss: 1.0482\n","Train Epoch: 59 -- Batch: 000 -- Loss: 1.0153\n","Train Epoch: 59 -- Batch: 500 -- Loss: 1.0398\n","Train Epoch: 60 -- Batch: 000 -- Loss: 0.9922\n","Train Epoch: 60 -- Batch: 500 -- Loss: 1.0482\n","Train Epoch: 61 -- Batch: 000 -- Loss: 1.0128\n","Train Epoch: 61 -- Batch: 500 -- Loss: 1.0176\n","Train Epoch: 62 -- Batch: 000 -- Loss: 1.0146\n","Train Epoch: 62 -- Batch: 500 -- Loss: 1.0151\n","Train Epoch: 63 -- Batch: 000 -- Loss: 0.9997\n","Train Epoch: 63 -- Batch: 500 -- Loss: 1.0432\n","Train Epoch: 64 -- Batch: 000 -- Loss: 0.9931\n","Train Epoch: 64 -- Batch: 500 -- Loss: 1.0348\n","Train Epoch: 65 -- Batch: 000 -- Loss: 0.9757\n","Train Epoch: 65 -- Batch: 500 -- Loss: 1.0723\n","Train Epoch: 66 -- Batch: 000 -- Loss: 1.0137\n","Train Epoch: 66 -- Batch: 500 -- Loss: 1.0242\n","Train Epoch: 67 -- Batch: 000 -- Loss: 0.9807\n","Train Epoch: 67 -- Batch: 500 -- Loss: 0.9954\n","Train Epoch: 68 -- Batch: 000 -- Loss: 0.9922\n","Train Epoch: 68 -- Batch: 500 -- Loss: 1.0155\n","Train Epoch: 69 -- Batch: 000 -- Loss: 0.9977\n","Train Epoch: 69 -- Batch: 500 -- Loss: 1.0326\n","Train Epoch: 70 -- Batch: 000 -- Loss: 0.9854\n","Train Epoch: 70 -- Batch: 500 -- Loss: 1.0364\n","Train Epoch: 71 -- Batch: 000 -- Loss: 0.9814\n","Train Epoch: 71 -- Batch: 500 -- Loss: 1.0105\n","Train Epoch: 72 -- Batch: 000 -- Loss: 0.9844\n","Train Epoch: 72 -- Batch: 500 -- Loss: 1.0117\n","Train Epoch: 73 -- Batch: 000 -- Loss: 0.9857\n","Train Epoch: 73 -- Batch: 500 -- Loss: 1.0137\n","Train Epoch: 74 -- Batch: 000 -- Loss: 0.9839\n","Train Epoch: 74 -- Batch: 500 -- Loss: 1.0000\n","Train Epoch: 75 -- Batch: 000 -- Loss: 0.9838\n","Train Epoch: 75 -- Batch: 500 -- Loss: 1.0371\n","Train Epoch: 76 -- Batch: 000 -- Loss: 0.9991\n","Train Epoch: 76 -- Batch: 500 -- Loss: 1.0243\n","Train Epoch: 77 -- Batch: 000 -- Loss: 0.9794\n","Train Epoch: 77 -- Batch: 500 -- Loss: 1.0085\n","Train Epoch: 78 -- Batch: 000 -- Loss: 0.9873\n","Train Epoch: 78 -- Batch: 500 -- Loss: 1.0179\n","Train Epoch: 79 -- Batch: 000 -- Loss: 0.9719\n","Train Epoch: 79 -- Batch: 500 -- Loss: 0.9951\n","Train Epoch: 80 -- Batch: 000 -- Loss: 0.9870\n","Train Epoch: 80 -- Batch: 500 -- Loss: 1.0119\n","Train Epoch: 81 -- Batch: 000 -- Loss: 0.9744\n","Train Epoch: 81 -- Batch: 500 -- Loss: 1.0302\n","Train Epoch: 82 -- Batch: 000 -- Loss: 0.9823\n","Train Epoch: 82 -- Batch: 500 -- Loss: 1.0256\n","Train Epoch: 83 -- Batch: 000 -- Loss: 1.0181\n","Train Epoch: 83 -- Batch: 500 -- Loss: 1.0207\n","Train Epoch: 84 -- Batch: 000 -- Loss: 0.9746\n","Train Epoch: 84 -- Batch: 500 -- Loss: 1.0105\n","Train Epoch: 85 -- Batch: 000 -- Loss: 0.9984\n","Train Epoch: 85 -- Batch: 500 -- Loss: 1.0277\n","Train Epoch: 86 -- Batch: 000 -- Loss: 0.9662\n","Train Epoch: 86 -- Batch: 500 -- Loss: 1.0090\n","Train Epoch: 87 -- Batch: 000 -- Loss: 0.9714\n","Train Epoch: 87 -- Batch: 500 -- Loss: 1.0136\n","Train Epoch: 88 -- Batch: 000 -- Loss: 0.9931\n","Train Epoch: 88 -- Batch: 500 -- Loss: 1.0172\n","Train Epoch: 89 -- Batch: 000 -- Loss: 1.0172\n","Train Epoch: 89 -- Batch: 500 -- Loss: 1.0166\n","Train Epoch: 90 -- Batch: 000 -- Loss: 0.9930\n","Train Epoch: 90 -- Batch: 500 -- Loss: 1.0049\n","Train Epoch: 91 -- Batch: 000 -- Loss: 0.9817\n","Train Epoch: 91 -- Batch: 500 -- Loss: 1.0063\n","Train Epoch: 92 -- Batch: 000 -- Loss: 0.9796\n","Train Epoch: 92 -- Batch: 500 -- Loss: 0.9901\n","Train Epoch: 93 -- Batch: 000 -- Loss: 0.9702\n","Train Epoch: 93 -- Batch: 500 -- Loss: 1.0112\n","Train Epoch: 94 -- Batch: 000 -- Loss: 0.9934\n","Train Epoch: 94 -- Batch: 500 -- Loss: 0.9848\n","Train Epoch: 95 -- Batch: 000 -- Loss: 0.9616\n","Train Epoch: 95 -- Batch: 500 -- Loss: 0.9988\n","Train Epoch: 96 -- Batch: 000 -- Loss: 1.0025\n","Train Epoch: 96 -- Batch: 500 -- Loss: 1.0125\n","Train Epoch: 97 -- Batch: 000 -- Loss: 0.9509\n","Train Epoch: 97 -- Batch: 500 -- Loss: 0.9931\n","Train Epoch: 98 -- Batch: 000 -- Loss: 0.9808\n","Train Epoch: 98 -- Batch: 500 -- Loss: 1.0238\n","Train Epoch: 99 -- Batch: 000 -- Loss: 1.0089\n","Train Epoch: 99 -- Batch: 500 -- Loss: 1.0288\n","| \u001b[95m 6       \u001b[0m | \u001b[95m 0.6165  \u001b[0m | \u001b[95m 422.7   \u001b[0m | \u001b[95m 0.4841  \u001b[0m | \u001b[95m 0.03203 \u001b[0m | \u001b[95m 188.5   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6224\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.5119\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.4879\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.4032\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.4139\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.3496\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.3553\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.3228\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.3513\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.3035\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.2852\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.2699\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2962\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.2820\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.2296\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.2705\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.2634\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.2101\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.2040\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.2871\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.2327\n","Train Epoch: 10 -- Batch: 500 -- Loss: 1.2338\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.2292\n","Train Epoch: 11 -- Batch: 500 -- Loss: 1.2478\n","Train Epoch: 12 -- Batch: 000 -- Loss: 1.2056\n","Train Epoch: 12 -- Batch: 500 -- Loss: 1.2316\n","Train Epoch: 13 -- Batch: 000 -- Loss: 1.1875\n","Train Epoch: 13 -- Batch: 500 -- Loss: 1.2290\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.1909\n","Train Epoch: 14 -- Batch: 500 -- Loss: 1.2426\n","Train Epoch: 15 -- Batch: 000 -- Loss: 1.1794\n","Train Epoch: 15 -- Batch: 500 -- Loss: 1.2330\n","Train Epoch: 16 -- Batch: 000 -- Loss: 1.2107\n","Train Epoch: 16 -- Batch: 500 -- Loss: 1.2279\n","Train Epoch: 17 -- Batch: 000 -- Loss: 1.1908\n","Train Epoch: 17 -- Batch: 500 -- Loss: 1.2689\n","Train Epoch: 18 -- Batch: 000 -- Loss: 1.1654\n","Train Epoch: 18 -- Batch: 500 -- Loss: 1.2004\n","Train Epoch: 19 -- Batch: 000 -- Loss: 1.2184\n","Train Epoch: 19 -- Batch: 500 -- Loss: 1.2211\n","Train Epoch: 20 -- Batch: 000 -- Loss: 1.1782\n","Train Epoch: 20 -- Batch: 500 -- Loss: 1.2344\n","Train Epoch: 21 -- Batch: 000 -- Loss: 1.1653\n","Train Epoch: 21 -- Batch: 500 -- Loss: 1.1899\n","Train Epoch: 22 -- Batch: 000 -- Loss: 1.1958\n","Train Epoch: 22 -- Batch: 500 -- Loss: 1.1994\n","Train Epoch: 23 -- Batch: 000 -- Loss: 1.1675\n","Train Epoch: 23 -- Batch: 500 -- Loss: 1.2003\n","Train Epoch: 24 -- Batch: 000 -- Loss: 1.1825\n","Train Epoch: 24 -- Batch: 500 -- Loss: 1.1907\n","Train Epoch: 25 -- Batch: 000 -- Loss: 1.1707\n","Train Epoch: 25 -- Batch: 500 -- Loss: 1.1988\n","Train Epoch: 26 -- Batch: 000 -- Loss: 1.2049\n","Train Epoch: 26 -- Batch: 500 -- Loss: 1.1951\n","Train Epoch: 27 -- Batch: 000 -- Loss: 1.1338\n","Train Epoch: 27 -- Batch: 500 -- Loss: 1.2138\n","Train Epoch: 28 -- Batch: 000 -- Loss: 1.1714\n","Train Epoch: 28 -- Batch: 500 -- Loss: 1.1517\n","Train Epoch: 29 -- Batch: 000 -- Loss: 1.1315\n","Train Epoch: 29 -- Batch: 500 -- Loss: 1.1492\n","Train Epoch: 30 -- Batch: 000 -- Loss: 1.1600\n","Train Epoch: 30 -- Batch: 500 -- Loss: 1.1896\n","Train Epoch: 31 -- Batch: 000 -- Loss: 1.1709\n","Train Epoch: 31 -- Batch: 500 -- Loss: 1.1660\n","Train Epoch: 32 -- Batch: 000 -- Loss: 1.1593\n","Train Epoch: 32 -- Batch: 500 -- Loss: 1.2126\n","Train Epoch: 33 -- Batch: 000 -- Loss: 1.1799\n","Train Epoch: 33 -- Batch: 500 -- Loss: 1.2088\n","Train Epoch: 34 -- Batch: 000 -- Loss: 1.1668\n","Train Epoch: 34 -- Batch: 500 -- Loss: 1.2026\n","Train Epoch: 35 -- Batch: 000 -- Loss: 1.1717\n","Train Epoch: 35 -- Batch: 500 -- Loss: 1.1607\n","Train Epoch: 36 -- Batch: 000 -- Loss: 1.1382\n","Train Epoch: 36 -- Batch: 500 -- Loss: 1.1987\n","Train Epoch: 37 -- Batch: 000 -- Loss: 1.1506\n","Train Epoch: 37 -- Batch: 500 -- Loss: 1.1769\n","Train Epoch: 38 -- Batch: 000 -- Loss: 1.1486\n","Train Epoch: 38 -- Batch: 500 -- Loss: 1.2022\n","Train Epoch: 39 -- Batch: 000 -- Loss: 1.1535\n","Train Epoch: 39 -- Batch: 500 -- Loss: 1.1624\n","Train Epoch: 40 -- Batch: 000 -- Loss: 1.1568\n","Train Epoch: 40 -- Batch: 500 -- Loss: 1.1463\n","Train Epoch: 41 -- Batch: 000 -- Loss: 1.1473\n","Train Epoch: 41 -- Batch: 500 -- Loss: 1.1842\n","Train Epoch: 42 -- Batch: 000 -- Loss: 1.1456\n","Train Epoch: 42 -- Batch: 500 -- Loss: 1.1788\n","Train Epoch: 43 -- Batch: 000 -- Loss: 1.1393\n","Train Epoch: 43 -- Batch: 500 -- Loss: 1.1889\n","Train Epoch: 44 -- Batch: 000 -- Loss: 1.1566\n","Train Epoch: 44 -- Batch: 500 -- Loss: 1.1743\n","Train Epoch: 45 -- Batch: 000 -- Loss: 1.1505\n","Train Epoch: 45 -- Batch: 500 -- Loss: 1.1912\n","Train Epoch: 46 -- Batch: 000 -- Loss: 1.1737\n","Train Epoch: 46 -- Batch: 500 -- Loss: 1.1577\n","Train Epoch: 47 -- Batch: 000 -- Loss: 1.1420\n","Train Epoch: 47 -- Batch: 500 -- Loss: 1.1639\n","Train Epoch: 48 -- Batch: 000 -- Loss: 1.1392\n","Train Epoch: 48 -- Batch: 500 -- Loss: 1.1632\n","Train Epoch: 49 -- Batch: 000 -- Loss: 1.1698\n","Train Epoch: 49 -- Batch: 500 -- Loss: 1.1665\n","Train Epoch: 50 -- Batch: 000 -- Loss: 1.1302\n","Train Epoch: 50 -- Batch: 500 -- Loss: 1.1775\n","Train Epoch: 51 -- Batch: 000 -- Loss: 1.1193\n","Train Epoch: 51 -- Batch: 500 -- Loss: 1.1880\n","Train Epoch: 52 -- Batch: 000 -- Loss: 1.1593\n","Train Epoch: 52 -- Batch: 500 -- Loss: 1.1992\n","Train Epoch: 53 -- Batch: 000 -- Loss: 1.1477\n","Train Epoch: 53 -- Batch: 500 -- Loss: 1.1446\n","Train Epoch: 54 -- Batch: 000 -- Loss: 1.1328\n","Train Epoch: 54 -- Batch: 500 -- Loss: 1.1735\n","Train Epoch: 55 -- Batch: 000 -- Loss: 1.1438\n","Train Epoch: 55 -- Batch: 500 -- Loss: 1.1820\n","Train Epoch: 56 -- Batch: 000 -- Loss: 1.1082\n","Train Epoch: 56 -- Batch: 500 -- Loss: 1.1559\n","Train Epoch: 57 -- Batch: 000 -- Loss: 1.1446\n","Train Epoch: 57 -- Batch: 500 -- Loss: 1.1401\n","Train Epoch: 58 -- Batch: 000 -- Loss: 1.1264\n","Train Epoch: 58 -- Batch: 500 -- Loss: 1.2098\n","Train Epoch: 59 -- Batch: 000 -- Loss: 1.1463\n","Train Epoch: 59 -- Batch: 500 -- Loss: 1.1646\n","Train Epoch: 60 -- Batch: 000 -- Loss: 1.1419\n","Train Epoch: 60 -- Batch: 500 -- Loss: 1.1685\n","Train Epoch: 61 -- Batch: 000 -- Loss: 1.1372\n","Train Epoch: 61 -- Batch: 500 -- Loss: 1.1599\n","Train Epoch: 62 -- Batch: 000 -- Loss: 1.1329\n","Train Epoch: 62 -- Batch: 500 -- Loss: 1.1526\n","Train Epoch: 63 -- Batch: 000 -- Loss: 1.1317\n","Train Epoch: 63 -- Batch: 500 -- Loss: 1.1471\n","Train Epoch: 64 -- Batch: 000 -- Loss: 1.1113\n","Train Epoch: 64 -- Batch: 500 -- Loss: 1.1510\n","Train Epoch: 65 -- Batch: 000 -- Loss: 1.1151\n","Train Epoch: 65 -- Batch: 500 -- Loss: 1.1489\n","Train Epoch: 66 -- Batch: 000 -- Loss: 1.1276\n","Train Epoch: 66 -- Batch: 500 -- Loss: 1.1447\n","Train Epoch: 67 -- Batch: 000 -- Loss: 1.1302\n","Train Epoch: 67 -- Batch: 500 -- Loss: 1.1320\n","Train Epoch: 68 -- Batch: 000 -- Loss: 1.1318\n","Train Epoch: 68 -- Batch: 500 -- Loss: 1.1541\n","Train Epoch: 69 -- Batch: 000 -- Loss: 1.1007\n","Train Epoch: 69 -- Batch: 500 -- Loss: 1.1412\n","Train Epoch: 70 -- Batch: 000 -- Loss: 1.1229\n","Train Epoch: 70 -- Batch: 500 -- Loss: 1.1504\n","Train Epoch: 71 -- Batch: 000 -- Loss: 1.1166\n","Train Epoch: 71 -- Batch: 500 -- Loss: 1.1519\n","Train Epoch: 72 -- Batch: 000 -- Loss: 1.1155\n","Train Epoch: 72 -- Batch: 500 -- Loss: 1.1978\n","Train Epoch: 73 -- Batch: 000 -- Loss: 1.1469\n","Train Epoch: 73 -- Batch: 500 -- Loss: 1.1799\n","Train Epoch: 74 -- Batch: 000 -- Loss: 1.1521\n","Train Epoch: 74 -- Batch: 500 -- Loss: 1.1466\n","Train Epoch: 75 -- Batch: 000 -- Loss: 1.1468\n","Train Epoch: 75 -- Batch: 500 -- Loss: 1.1484\n","Train Epoch: 76 -- Batch: 000 -- Loss: 1.1331\n","Train Epoch: 76 -- Batch: 500 -- Loss: 1.1347\n","Train Epoch: 77 -- Batch: 000 -- Loss: 1.1432\n","Train Epoch: 77 -- Batch: 500 -- Loss: 1.1267\n","Train Epoch: 78 -- Batch: 000 -- Loss: 1.1241\n","Train Epoch: 78 -- Batch: 500 -- Loss: 1.1684\n","Train Epoch: 79 -- Batch: 000 -- Loss: 1.1114\n","Train Epoch: 79 -- Batch: 500 -- Loss: 1.1668\n","Train Epoch: 80 -- Batch: 000 -- Loss: 1.1333\n","Train Epoch: 80 -- Batch: 500 -- Loss: 1.1381\n","Train Epoch: 81 -- Batch: 000 -- Loss: 1.1396\n","Train Epoch: 81 -- Batch: 500 -- Loss: 1.1616\n","Train Epoch: 82 -- Batch: 000 -- Loss: 1.1574\n","Train Epoch: 82 -- Batch: 500 -- Loss: 1.1428\n","Train Epoch: 83 -- Batch: 000 -- Loss: 1.1247\n","Train Epoch: 83 -- Batch: 500 -- Loss: 1.1373\n","Train Epoch: 84 -- Batch: 000 -- Loss: 1.1417\n","Train Epoch: 84 -- Batch: 500 -- Loss: 1.1147\n","Train Epoch: 85 -- Batch: 000 -- Loss: 1.1377\n","Train Epoch: 85 -- Batch: 500 -- Loss: 1.1600\n","Train Epoch: 86 -- Batch: 000 -- Loss: 1.0903\n","Train Epoch: 86 -- Batch: 500 -- Loss: 1.1409\n","Train Epoch: 87 -- Batch: 000 -- Loss: 1.1341\n","Train Epoch: 87 -- Batch: 500 -- Loss: 1.1161\n","Train Epoch: 88 -- Batch: 000 -- Loss: 1.1355\n","Train Epoch: 88 -- Batch: 500 -- Loss: 1.1603\n","Train Epoch: 89 -- Batch: 000 -- Loss: 1.1040\n","Train Epoch: 89 -- Batch: 500 -- Loss: 1.1501\n","Train Epoch: 90 -- Batch: 000 -- Loss: 1.1169\n","Train Epoch: 90 -- Batch: 500 -- Loss: 1.1272\n","Train Epoch: 91 -- Batch: 000 -- Loss: 1.0912\n","Train Epoch: 91 -- Batch: 500 -- Loss: 1.1296\n","Train Epoch: 92 -- Batch: 000 -- Loss: 1.1324\n","Train Epoch: 92 -- Batch: 500 -- Loss: 1.1428\n","Train Epoch: 93 -- Batch: 000 -- Loss: 1.1035\n","Train Epoch: 93 -- Batch: 500 -- Loss: 1.1363\n","Train Epoch: 94 -- Batch: 000 -- Loss: 1.1026\n","Train Epoch: 94 -- Batch: 500 -- Loss: 1.1427\n","Train Epoch: 95 -- Batch: 000 -- Loss: 1.1256\n","Train Epoch: 95 -- Batch: 500 -- Loss: 1.1118\n","Train Epoch: 96 -- Batch: 000 -- Loss: 1.1031\n","Train Epoch: 96 -- Batch: 500 -- Loss: 1.1320\n","Train Epoch: 97 -- Batch: 000 -- Loss: 1.1021\n","Train Epoch: 97 -- Batch: 500 -- Loss: 1.1371\n","Train Epoch: 98 -- Batch: 000 -- Loss: 1.1429\n","Train Epoch: 98 -- Batch: 500 -- Loss: 1.1152\n","Train Epoch: 99 -- Batch: 000 -- Loss: 1.1026\n","Train Epoch: 99 -- Batch: 500 -- Loss: 1.1462\n","| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5971  \u001b[0m | \u001b[0m 456.6   \u001b[0m | \u001b[0m 0.4473  \u001b[0m | \u001b[0m 0.009419\u001b[0m | \u001b[0m 57.81   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.5968\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.4533\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.3062\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.3539\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.3247\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.2457\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.1217\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.2173\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.2784\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.1735\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.2414\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.2504\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.1955\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.1866\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.1326\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.1863\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.1977\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.1820\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.1480\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.2494\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.1792\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.1170\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.1501\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.2214\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.1614\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.1830\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.1227\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.2372\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.1460\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.1263\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.1146\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.1843\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.1052\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.1016\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.1061\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.2085\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.1539\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.1436\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.0965\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.2365\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.1279\n","Train Epoch: 10 -- Batch: 500 -- Loss: 1.0508\n","Train Epoch: 10 -- Batch: 1000 -- Loss: 1.1523\n","Train Epoch: 10 -- Batch: 1500 -- Loss: 1.1820\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.1579\n","Train Epoch: 11 -- Batch: 500 -- Loss: 1.1050\n","Train Epoch: 11 -- Batch: 1000 -- Loss: 1.1357\n","Train Epoch: 11 -- Batch: 1500 -- Loss: 1.1910\n","Train Epoch: 12 -- Batch: 000 -- Loss: 1.1436\n","Train Epoch: 12 -- Batch: 500 -- Loss: 1.0440\n","Train Epoch: 12 -- Batch: 1000 -- Loss: 1.1429\n","Train Epoch: 12 -- Batch: 1500 -- Loss: 1.1821\n","Train Epoch: 13 -- Batch: 000 -- Loss: 1.1302\n","Train Epoch: 13 -- Batch: 500 -- Loss: 1.1233\n","Train Epoch: 13 -- Batch: 1000 -- Loss: 1.0829\n","Train Epoch: 13 -- Batch: 1500 -- Loss: 1.1411\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.1198\n","Train Epoch: 14 -- Batch: 500 -- Loss: 1.0684\n","Train Epoch: 14 -- Batch: 1000 -- Loss: 1.1234\n","Train Epoch: 14 -- Batch: 1500 -- Loss: 1.1753\n","Train Epoch: 15 -- Batch: 000 -- Loss: 1.1198\n","Train Epoch: 15 -- Batch: 500 -- Loss: 1.0376\n","Train Epoch: 15 -- Batch: 1000 -- Loss: 1.0938\n","Train Epoch: 15 -- Batch: 1500 -- Loss: 1.1520\n","Train Epoch: 16 -- Batch: 000 -- Loss: 1.1386\n","Train Epoch: 16 -- Batch: 500 -- Loss: 1.0709\n","Train Epoch: 16 -- Batch: 1000 -- Loss: 1.1580\n","Train Epoch: 16 -- Batch: 1500 -- Loss: 1.1392\n","Train Epoch: 17 -- Batch: 000 -- Loss: 1.1576\n","Train Epoch: 17 -- Batch: 500 -- Loss: 1.0867\n","Train Epoch: 17 -- Batch: 1000 -- Loss: 1.1329\n","Train Epoch: 17 -- Batch: 1500 -- Loss: 1.1480\n","Train Epoch: 18 -- Batch: 000 -- Loss: 1.0911\n","Train Epoch: 18 -- Batch: 500 -- Loss: 1.0487\n","Train Epoch: 18 -- Batch: 1000 -- Loss: 1.0805\n","Train Epoch: 18 -- Batch: 1500 -- Loss: 1.1336\n","Train Epoch: 19 -- Batch: 000 -- Loss: 1.0831\n","Train Epoch: 19 -- Batch: 500 -- Loss: 1.0697\n","Train Epoch: 19 -- Batch: 1000 -- Loss: 1.0800\n","Train Epoch: 19 -- Batch: 1500 -- Loss: 1.1293\n","Train Epoch: 20 -- Batch: 000 -- Loss: 1.0497\n","Train Epoch: 20 -- Batch: 500 -- Loss: 1.0458\n","Train Epoch: 20 -- Batch: 1000 -- Loss: 1.1092\n","Train Epoch: 20 -- Batch: 1500 -- Loss: 1.1521\n","Train Epoch: 21 -- Batch: 000 -- Loss: 1.0759\n","Train Epoch: 21 -- Batch: 500 -- Loss: 1.0300\n","Train Epoch: 21 -- Batch: 1000 -- Loss: 1.1158\n","Train Epoch: 21 -- Batch: 1500 -- Loss: 1.1688\n","Train Epoch: 22 -- Batch: 000 -- Loss: 1.0898\n","Train Epoch: 22 -- Batch: 500 -- Loss: 1.1110\n","Train Epoch: 22 -- Batch: 1000 -- Loss: 1.1330\n","Train Epoch: 22 -- Batch: 1500 -- Loss: 1.1077\n","Train Epoch: 23 -- Batch: 000 -- Loss: 1.1034\n","Train Epoch: 23 -- Batch: 500 -- Loss: 1.0768\n","Train Epoch: 23 -- Batch: 1000 -- Loss: 1.1277\n","Train Epoch: 23 -- Batch: 1500 -- Loss: 1.0999\n","Train Epoch: 24 -- Batch: 000 -- Loss: 1.0936\n","Train Epoch: 24 -- Batch: 500 -- Loss: 1.0882\n","Train Epoch: 24 -- Batch: 1000 -- Loss: 1.0500\n","Train Epoch: 24 -- Batch: 1500 -- Loss: 1.1294\n","Train Epoch: 25 -- Batch: 000 -- Loss: 1.1160\n","Train Epoch: 25 -- Batch: 500 -- Loss: 1.0992\n","Train Epoch: 25 -- Batch: 1000 -- Loss: 1.0497\n","Train Epoch: 25 -- Batch: 1500 -- Loss: 1.1009\n","Train Epoch: 26 -- Batch: 000 -- Loss: 1.0738\n","Train Epoch: 26 -- Batch: 500 -- Loss: 1.0417\n","Train Epoch: 26 -- Batch: 1000 -- Loss: 1.1316\n","Train Epoch: 26 -- Batch: 1500 -- Loss: 1.1374\n","Train Epoch: 27 -- Batch: 000 -- Loss: 1.0841\n","Train Epoch: 27 -- Batch: 500 -- Loss: 1.0626\n","Train Epoch: 27 -- Batch: 1000 -- Loss: 1.0631\n","Train Epoch: 27 -- Batch: 1500 -- Loss: 1.1266\n","Train Epoch: 28 -- Batch: 000 -- Loss: 1.0956\n","Train Epoch: 28 -- Batch: 500 -- Loss: 0.9979\n","Train Epoch: 28 -- Batch: 1000 -- Loss: 1.0733\n","Train Epoch: 28 -- Batch: 1500 -- Loss: 1.1265\n","Train Epoch: 29 -- Batch: 000 -- Loss: 1.0858\n","Train Epoch: 29 -- Batch: 500 -- Loss: 1.0296\n","Train Epoch: 29 -- Batch: 1000 -- Loss: 1.0962\n","Train Epoch: 29 -- Batch: 1500 -- Loss: 1.1204\n","Train Epoch: 30 -- Batch: 000 -- Loss: 1.0576\n","Train Epoch: 30 -- Batch: 500 -- Loss: 1.0358\n","Train Epoch: 30 -- Batch: 1000 -- Loss: 1.0501\n","Train Epoch: 30 -- Batch: 1500 -- Loss: 1.1237\n","Train Epoch: 31 -- Batch: 000 -- Loss: 1.0901\n","Train Epoch: 31 -- Batch: 500 -- Loss: 0.9998\n","Train Epoch: 31 -- Batch: 1000 -- Loss: 1.0592\n","Train Epoch: 31 -- Batch: 1500 -- Loss: 1.1191\n","Train Epoch: 32 -- Batch: 000 -- Loss: 1.0785\n","Train Epoch: 32 -- Batch: 500 -- Loss: 1.0604\n","Train Epoch: 32 -- Batch: 1000 -- Loss: 1.0882\n","Train Epoch: 32 -- Batch: 1500 -- Loss: 1.1311\n","Train Epoch: 33 -- Batch: 000 -- Loss: 1.0478\n","Train Epoch: 33 -- Batch: 500 -- Loss: 1.0378\n","Train Epoch: 33 -- Batch: 1000 -- Loss: 1.0869\n","Train Epoch: 33 -- Batch: 1500 -- Loss: 1.0993\n","Train Epoch: 34 -- Batch: 000 -- Loss: 1.0371\n","Train Epoch: 34 -- Batch: 500 -- Loss: 1.0265\n","Train Epoch: 34 -- Batch: 1000 -- Loss: 1.0501\n","Train Epoch: 34 -- Batch: 1500 -- Loss: 1.1063\n","Train Epoch: 35 -- Batch: 000 -- Loss: 1.0909\n","Train Epoch: 35 -- Batch: 500 -- Loss: 1.0828\n","Train Epoch: 35 -- Batch: 1000 -- Loss: 1.1135\n","Train Epoch: 35 -- Batch: 1500 -- Loss: 1.0917\n","Train Epoch: 36 -- Batch: 000 -- Loss: 1.0898\n","Train Epoch: 36 -- Batch: 500 -- Loss: 1.0477\n","Train Epoch: 36 -- Batch: 1000 -- Loss: 1.0740\n","Train Epoch: 36 -- Batch: 1500 -- Loss: 1.1000\n","Train Epoch: 37 -- Batch: 000 -- Loss: 1.0572\n","Train Epoch: 37 -- Batch: 500 -- Loss: 1.0155\n","Train Epoch: 37 -- Batch: 1000 -- Loss: 1.0665\n","Train Epoch: 37 -- Batch: 1500 -- Loss: 1.1106\n","Train Epoch: 38 -- Batch: 000 -- Loss: 1.1065\n","Train Epoch: 38 -- Batch: 500 -- Loss: 1.0244\n","Train Epoch: 38 -- Batch: 1000 -- Loss: 1.0782\n","Train Epoch: 38 -- Batch: 1500 -- Loss: 1.0936\n","Train Epoch: 39 -- Batch: 000 -- Loss: 1.0494\n","Train Epoch: 39 -- Batch: 500 -- Loss: 1.0728\n","Train Epoch: 39 -- Batch: 1000 -- Loss: 1.1143\n","Train Epoch: 39 -- Batch: 1500 -- Loss: 1.0923\n","Train Epoch: 40 -- Batch: 000 -- Loss: 1.0783\n","Train Epoch: 40 -- Batch: 500 -- Loss: 1.0242\n","Train Epoch: 40 -- Batch: 1000 -- Loss: 1.0781\n","Train Epoch: 40 -- Batch: 1500 -- Loss: 1.1154\n","Train Epoch: 41 -- Batch: 000 -- Loss: 1.0774\n","Train Epoch: 41 -- Batch: 500 -- Loss: 0.9852\n","Train Epoch: 41 -- Batch: 1000 -- Loss: 1.0440\n","Train Epoch: 41 -- Batch: 1500 -- Loss: 1.0850\n","Train Epoch: 42 -- Batch: 000 -- Loss: 1.0864\n","Train Epoch: 42 -- Batch: 500 -- Loss: 1.0123\n","Train Epoch: 42 -- Batch: 1000 -- Loss: 1.0528\n","Train Epoch: 42 -- Batch: 1500 -- Loss: 1.1077\n","Train Epoch: 43 -- Batch: 000 -- Loss: 1.0557\n","Train Epoch: 43 -- Batch: 500 -- Loss: 1.0510\n","Train Epoch: 43 -- Batch: 1000 -- Loss: 1.0278\n","Train Epoch: 43 -- Batch: 1500 -- Loss: 1.1092\n","Train Epoch: 44 -- Batch: 000 -- Loss: 1.1069\n","Train Epoch: 44 -- Batch: 500 -- Loss: 1.0177\n","Train Epoch: 44 -- Batch: 1000 -- Loss: 1.1042\n","Train Epoch: 44 -- Batch: 1500 -- Loss: 1.0389\n","Train Epoch: 45 -- Batch: 000 -- Loss: 1.0597\n","Train Epoch: 45 -- Batch: 500 -- Loss: 0.9832\n","Train Epoch: 45 -- Batch: 1000 -- Loss: 1.0601\n","Train Epoch: 45 -- Batch: 1500 -- Loss: 1.1037\n","Train Epoch: 46 -- Batch: 000 -- Loss: 1.0593\n","Train Epoch: 46 -- Batch: 500 -- Loss: 1.0610\n","Train Epoch: 46 -- Batch: 1000 -- Loss: 1.0677\n","Train Epoch: 46 -- Batch: 1500 -- Loss: 1.0831\n","Train Epoch: 47 -- Batch: 000 -- Loss: 1.0862\n","Train Epoch: 47 -- Batch: 500 -- Loss: 1.0328\n","Train Epoch: 47 -- Batch: 1000 -- Loss: 1.0109\n","Train Epoch: 47 -- Batch: 1500 -- Loss: 1.0538\n","Train Epoch: 48 -- Batch: 000 -- Loss: 1.0562\n","Train Epoch: 48 -- Batch: 500 -- Loss: 0.9635\n","Train Epoch: 48 -- Batch: 1000 -- Loss: 1.0249\n","Train Epoch: 48 -- Batch: 1500 -- Loss: 1.0931\n","Train Epoch: 49 -- Batch: 000 -- Loss: 1.0550\n","Train Epoch: 49 -- Batch: 500 -- Loss: 1.0145\n","Train Epoch: 49 -- Batch: 1000 -- Loss: 1.0648\n","Train Epoch: 49 -- Batch: 1500 -- Loss: 1.0627\n","Train Epoch: 50 -- Batch: 000 -- Loss: 1.0861\n","Train Epoch: 50 -- Batch: 500 -- Loss: 1.0283\n","Train Epoch: 50 -- Batch: 1000 -- Loss: 1.0386\n","Train Epoch: 50 -- Batch: 1500 -- Loss: 1.0949\n","Train Epoch: 51 -- Batch: 000 -- Loss: 1.0714\n","Train Epoch: 51 -- Batch: 500 -- Loss: 1.0769\n","Train Epoch: 51 -- Batch: 1000 -- Loss: 1.0317\n","Train Epoch: 51 -- Batch: 1500 -- Loss: 1.0533\n","Train Epoch: 52 -- Batch: 000 -- Loss: 1.0477\n","Train Epoch: 52 -- Batch: 500 -- Loss: 1.0159\n","Train Epoch: 52 -- Batch: 1000 -- Loss: 1.0409\n","Train Epoch: 52 -- Batch: 1500 -- Loss: 1.0665\n","Train Epoch: 53 -- Batch: 000 -- Loss: 1.0656\n","Train Epoch: 53 -- Batch: 500 -- Loss: 0.9826\n","Train Epoch: 53 -- Batch: 1000 -- Loss: 1.0408\n","Train Epoch: 53 -- Batch: 1500 -- Loss: 1.0583\n","Train Epoch: 54 -- Batch: 000 -- Loss: 1.0622\n","Train Epoch: 54 -- Batch: 500 -- Loss: 0.9964\n","Train Epoch: 54 -- Batch: 1000 -- Loss: 1.0651\n","Train Epoch: 54 -- Batch: 1500 -- Loss: 1.0597\n","Train Epoch: 55 -- Batch: 000 -- Loss: 1.0693\n","Train Epoch: 55 -- Batch: 500 -- Loss: 1.0238\n","Train Epoch: 55 -- Batch: 1000 -- Loss: 1.0665\n","Train Epoch: 55 -- Batch: 1500 -- Loss: 1.1007\n","Train Epoch: 56 -- Batch: 000 -- Loss: 1.0845\n","Train Epoch: 56 -- Batch: 500 -- Loss: 1.0449\n","Train Epoch: 56 -- Batch: 1000 -- Loss: 1.0631\n","Train Epoch: 56 -- Batch: 1500 -- Loss: 1.0660\n","Train Epoch: 57 -- Batch: 000 -- Loss: 1.1018\n","Train Epoch: 57 -- Batch: 500 -- Loss: 1.0254\n","Train Epoch: 57 -- Batch: 1000 -- Loss: 1.0591\n","Train Epoch: 57 -- Batch: 1500 -- Loss: 1.0752\n","Train Epoch: 58 -- Batch: 000 -- Loss: 1.0149\n","Train Epoch: 58 -- Batch: 500 -- Loss: 1.0333\n","Train Epoch: 58 -- Batch: 1000 -- Loss: 1.0621\n","Train Epoch: 58 -- Batch: 1500 -- Loss: 1.0665\n","Train Epoch: 59 -- Batch: 000 -- Loss: 1.0481\n","Train Epoch: 59 -- Batch: 500 -- Loss: 1.0312\n","Train Epoch: 59 -- Batch: 1000 -- Loss: 1.0719\n","Train Epoch: 59 -- Batch: 1500 -- Loss: 1.0196\n","Train Epoch: 60 -- Batch: 000 -- Loss: 1.0419\n","Train Epoch: 60 -- Batch: 500 -- Loss: 0.9975\n","Train Epoch: 60 -- Batch: 1000 -- Loss: 1.0715\n","Train Epoch: 60 -- Batch: 1500 -- Loss: 1.0705\n","Train Epoch: 61 -- Batch: 000 -- Loss: 1.0476\n","Train Epoch: 61 -- Batch: 500 -- Loss: 1.0027\n","Train Epoch: 61 -- Batch: 1000 -- Loss: 1.0647\n","Train Epoch: 61 -- Batch: 1500 -- Loss: 1.0803\n","Train Epoch: 62 -- Batch: 000 -- Loss: 1.0563\n","Train Epoch: 62 -- Batch: 500 -- Loss: 1.0242\n","Train Epoch: 62 -- Batch: 1000 -- Loss: 1.0821\n","Train Epoch: 62 -- Batch: 1500 -- Loss: 1.0845\n","Train Epoch: 63 -- Batch: 000 -- Loss: 1.0508\n","Train Epoch: 63 -- Batch: 500 -- Loss: 1.0427\n","Train Epoch: 63 -- Batch: 1000 -- Loss: 1.0559\n","Train Epoch: 63 -- Batch: 1500 -- Loss: 1.0316\n","Train Epoch: 64 -- Batch: 000 -- Loss: 1.0343\n","Train Epoch: 64 -- Batch: 500 -- Loss: 0.9839\n","Train Epoch: 64 -- Batch: 1000 -- Loss: 1.0753\n","Train Epoch: 64 -- Batch: 1500 -- Loss: 1.0357\n","Train Epoch: 65 -- Batch: 000 -- Loss: 1.0014\n","Train Epoch: 65 -- Batch: 500 -- Loss: 1.0313\n","Train Epoch: 65 -- Batch: 1000 -- Loss: 1.0885\n","Train Epoch: 65 -- Batch: 1500 -- Loss: 1.0680\n","Train Epoch: 66 -- Batch: 000 -- Loss: 1.0360\n","Train Epoch: 66 -- Batch: 500 -- Loss: 0.9848\n","Train Epoch: 66 -- Batch: 1000 -- Loss: 1.0389\n","Train Epoch: 66 -- Batch: 1500 -- Loss: 1.0329\n","Train Epoch: 67 -- Batch: 000 -- Loss: 1.0183\n","Train Epoch: 67 -- Batch: 500 -- Loss: 0.9921\n","Train Epoch: 67 -- Batch: 1000 -- Loss: 1.0579\n","Train Epoch: 67 -- Batch: 1500 -- Loss: 1.0957\n","Train Epoch: 68 -- Batch: 000 -- Loss: 1.0493\n","Train Epoch: 68 -- Batch: 500 -- Loss: 0.9546\n","Train Epoch: 68 -- Batch: 1000 -- Loss: 1.0538\n","Train Epoch: 68 -- Batch: 1500 -- Loss: 1.0687\n","Train Epoch: 69 -- Batch: 000 -- Loss: 0.9937\n","Train Epoch: 69 -- Batch: 500 -- Loss: 1.0152\n","Train Epoch: 69 -- Batch: 1000 -- Loss: 1.0795\n","Train Epoch: 69 -- Batch: 1500 -- Loss: 1.0401\n","Train Epoch: 70 -- Batch: 000 -- Loss: 1.0467\n","Train Epoch: 70 -- Batch: 500 -- Loss: 0.9641\n","Train Epoch: 70 -- Batch: 1000 -- Loss: 1.0346\n","Train Epoch: 70 -- Batch: 1500 -- Loss: 1.0405\n","Train Epoch: 71 -- Batch: 000 -- Loss: 1.0233\n","Train Epoch: 71 -- Batch: 500 -- Loss: 0.9816\n","Train Epoch: 71 -- Batch: 1000 -- Loss: 1.1011\n","Train Epoch: 71 -- Batch: 1500 -- Loss: 1.0513\n","Train Epoch: 72 -- Batch: 000 -- Loss: 1.0415\n","Train Epoch: 72 -- Batch: 500 -- Loss: 0.9792\n","Train Epoch: 72 -- Batch: 1000 -- Loss: 1.0218\n","Train Epoch: 72 -- Batch: 1500 -- Loss: 1.0412\n","Train Epoch: 73 -- Batch: 000 -- Loss: 1.0610\n","Train Epoch: 73 -- Batch: 500 -- Loss: 1.0060\n","Train Epoch: 73 -- Batch: 1000 -- Loss: 0.9797\n","Train Epoch: 73 -- Batch: 1500 -- Loss: 1.0556\n","Train Epoch: 74 -- Batch: 000 -- Loss: 1.0312\n","Train Epoch: 74 -- Batch: 500 -- Loss: 1.0025\n","Train Epoch: 74 -- Batch: 1000 -- Loss: 1.0373\n","Train Epoch: 74 -- Batch: 1500 -- Loss: 1.0227\n","Train Epoch: 75 -- Batch: 000 -- Loss: 1.0136\n","Train Epoch: 75 -- Batch: 500 -- Loss: 0.9858\n","Train Epoch: 75 -- Batch: 1000 -- Loss: 0.9814\n","Train Epoch: 75 -- Batch: 1500 -- Loss: 1.0523\n","Train Epoch: 76 -- Batch: 000 -- Loss: 1.0506\n","Train Epoch: 76 -- Batch: 500 -- Loss: 0.9945\n","Train Epoch: 76 -- Batch: 1000 -- Loss: 1.0798\n","Train Epoch: 76 -- Batch: 1500 -- Loss: 1.0853\n","Train Epoch: 77 -- Batch: 000 -- Loss: 1.0369\n","Train Epoch: 77 -- Batch: 500 -- Loss: 1.0679\n","Train Epoch: 77 -- Batch: 1000 -- Loss: 1.0585\n","Train Epoch: 77 -- Batch: 1500 -- Loss: 1.0671\n","Train Epoch: 78 -- Batch: 000 -- Loss: 1.0773\n","Train Epoch: 78 -- Batch: 500 -- Loss: 1.0028\n","Train Epoch: 78 -- Batch: 1000 -- Loss: 1.0639\n","Train Epoch: 78 -- Batch: 1500 -- Loss: 1.0464\n","Train Epoch: 79 -- Batch: 000 -- Loss: 1.0017\n","Train Epoch: 79 -- Batch: 500 -- Loss: 1.0036\n","Train Epoch: 79 -- Batch: 1000 -- Loss: 1.1271\n","Train Epoch: 79 -- Batch: 1500 -- Loss: 1.0637\n","Train Epoch: 80 -- Batch: 000 -- Loss: 1.0253\n","Train Epoch: 80 -- Batch: 500 -- Loss: 0.9944\n","Train Epoch: 80 -- Batch: 1000 -- Loss: 1.0464\n","Train Epoch: 80 -- Batch: 1500 -- Loss: 1.0608\n","Train Epoch: 81 -- Batch: 000 -- Loss: 1.0465\n","Train Epoch: 81 -- Batch: 500 -- Loss: 1.0240\n","Train Epoch: 81 -- Batch: 1000 -- Loss: 1.0972\n","Train Epoch: 81 -- Batch: 1500 -- Loss: 1.0936\n","Train Epoch: 82 -- Batch: 000 -- Loss: 1.0556\n","Train Epoch: 82 -- Batch: 500 -- Loss: 0.9531\n","Train Epoch: 82 -- Batch: 1000 -- Loss: 1.0409\n","Train Epoch: 82 -- Batch: 1500 -- Loss: 1.0357\n","Train Epoch: 83 -- Batch: 000 -- Loss: 1.0878\n","Train Epoch: 83 -- Batch: 500 -- Loss: 0.9862\n","Train Epoch: 83 -- Batch: 1000 -- Loss: 1.1013\n","Train Epoch: 83 -- Batch: 1500 -- Loss: 1.0774\n","Train Epoch: 84 -- Batch: 000 -- Loss: 1.0100\n","Train Epoch: 84 -- Batch: 500 -- Loss: 0.9500\n","Train Epoch: 84 -- Batch: 1000 -- Loss: 1.0631\n","Train Epoch: 84 -- Batch: 1500 -- Loss: 1.0400\n","Train Epoch: 85 -- Batch: 000 -- Loss: 1.1012\n","Train Epoch: 85 -- Batch: 500 -- Loss: 1.0076\n","Train Epoch: 85 -- Batch: 1000 -- Loss: 1.0295\n","Train Epoch: 85 -- Batch: 1500 -- Loss: 1.0427\n","Train Epoch: 86 -- Batch: 000 -- Loss: 1.0036\n","Train Epoch: 86 -- Batch: 500 -- Loss: 0.9696\n","Train Epoch: 86 -- Batch: 1000 -- Loss: 1.1048\n","Train Epoch: 86 -- Batch: 1500 -- Loss: 1.0301\n","Train Epoch: 87 -- Batch: 000 -- Loss: 1.0528\n","Train Epoch: 87 -- Batch: 500 -- Loss: 0.9562\n","Train Epoch: 87 -- Batch: 1000 -- Loss: 0.9990\n","Train Epoch: 87 -- Batch: 1500 -- Loss: 1.0812\n","Train Epoch: 88 -- Batch: 000 -- Loss: 1.0771\n","Train Epoch: 88 -- Batch: 500 -- Loss: 1.0191\n","Train Epoch: 88 -- Batch: 1000 -- Loss: 1.0514\n","Train Epoch: 88 -- Batch: 1500 -- Loss: 1.0690\n","Train Epoch: 89 -- Batch: 000 -- Loss: 1.0150\n","Train Epoch: 89 -- Batch: 500 -- Loss: 1.0447\n","Train Epoch: 89 -- Batch: 1000 -- Loss: 1.0978\n","Train Epoch: 89 -- Batch: 1500 -- Loss: 1.0083\n","Train Epoch: 90 -- Batch: 000 -- Loss: 1.0462\n","Train Epoch: 90 -- Batch: 500 -- Loss: 1.0130\n","Train Epoch: 90 -- Batch: 1000 -- Loss: 1.0433\n","Train Epoch: 90 -- Batch: 1500 -- Loss: 1.0607\n","Train Epoch: 91 -- Batch: 000 -- Loss: 1.0297\n","Train Epoch: 91 -- Batch: 500 -- Loss: 0.9808\n","Train Epoch: 91 -- Batch: 1000 -- Loss: 1.0616\n","Train Epoch: 91 -- Batch: 1500 -- Loss: 1.0268\n","Train Epoch: 92 -- Batch: 000 -- Loss: 1.0125\n","Train Epoch: 92 -- Batch: 500 -- Loss: 0.9728\n","Train Epoch: 92 -- Batch: 1000 -- Loss: 1.0043\n","Train Epoch: 92 -- Batch: 1500 -- Loss: 1.0501\n","Train Epoch: 93 -- Batch: 000 -- Loss: 1.0593\n","Train Epoch: 93 -- Batch: 500 -- Loss: 0.9462\n","Train Epoch: 93 -- Batch: 1000 -- Loss: 1.0532\n","Train Epoch: 93 -- Batch: 1500 -- Loss: 1.0340\n","Train Epoch: 94 -- Batch: 000 -- Loss: 1.0338\n","Train Epoch: 94 -- Batch: 500 -- Loss: 1.0209\n","Train Epoch: 94 -- Batch: 1000 -- Loss: 1.0682\n","Train Epoch: 94 -- Batch: 1500 -- Loss: 1.0724\n","Train Epoch: 95 -- Batch: 000 -- Loss: 1.0793\n","Train Epoch: 95 -- Batch: 500 -- Loss: 0.9900\n","Train Epoch: 95 -- Batch: 1000 -- Loss: 1.0230\n","Train Epoch: 95 -- Batch: 1500 -- Loss: 1.0178\n","Train Epoch: 96 -- Batch: 000 -- Loss: 1.0430\n","Train Epoch: 96 -- Batch: 500 -- Loss: 1.0283\n","Train Epoch: 96 -- Batch: 1000 -- Loss: 1.0595\n","Train Epoch: 96 -- Batch: 1500 -- Loss: 0.9825\n","Train Epoch: 97 -- Batch: 000 -- Loss: 1.0025\n","Train Epoch: 97 -- Batch: 500 -- Loss: 0.9964\n","Train Epoch: 97 -- Batch: 1000 -- Loss: 0.9762\n","Train Epoch: 97 -- Batch: 1500 -- Loss: 1.0002\n","Train Epoch: 98 -- Batch: 000 -- Loss: 1.0385\n","Train Epoch: 98 -- Batch: 500 -- Loss: 0.9614\n","Train Epoch: 98 -- Batch: 1000 -- Loss: 1.0648\n","Train Epoch: 98 -- Batch: 1500 -- Loss: 1.0484\n","Train Epoch: 99 -- Batch: 000 -- Loss: 0.9806\n","Train Epoch: 99 -- Batch: 500 -- Loss: 0.9951\n","Train Epoch: 99 -- Batch: 1000 -- Loss: 1.0688\n","Train Epoch: 99 -- Batch: 1500 -- Loss: 1.0209\n","| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6149  \u001b[0m | \u001b[0m 140.1   \u001b[0m | \u001b[0m 0.4391  \u001b[0m | \u001b[0m 0.01074 \u001b[0m | \u001b[0m 134.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6187\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.1088\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.2006\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.0673\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.1098\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.0789\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.0834\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.0528\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.0705\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.0421\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.0590\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.0347\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.0702\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.0183\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0432\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.0220\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.0704\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.0403\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.0666\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.0158\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.0526\n","Train Epoch: 10 -- Batch: 500 -- Loss: 1.0022\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.0514\n","Train Epoch: 11 -- Batch: 500 -- Loss: 0.9896\n","Train Epoch: 12 -- Batch: 000 -- Loss: 1.0336\n","Train Epoch: 12 -- Batch: 500 -- Loss: 1.0045\n","Train Epoch: 13 -- Batch: 000 -- Loss: 1.0325\n","Train Epoch: 13 -- Batch: 500 -- Loss: 0.9937\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.0135\n","Train Epoch: 14 -- Batch: 500 -- Loss: 0.9942\n","Train Epoch: 15 -- Batch: 000 -- Loss: 1.0268\n","Train Epoch: 15 -- Batch: 500 -- Loss: 0.9959\n","Train Epoch: 16 -- Batch: 000 -- Loss: 1.0170\n","Train Epoch: 16 -- Batch: 500 -- Loss: 0.9729\n","Train Epoch: 17 -- Batch: 000 -- Loss: 1.0189\n","Train Epoch: 17 -- Batch: 500 -- Loss: 0.9796\n","Train Epoch: 18 -- Batch: 000 -- Loss: 1.0197\n","Train Epoch: 18 -- Batch: 500 -- Loss: 0.9806\n","Train Epoch: 19 -- Batch: 000 -- Loss: 1.0210\n","Train Epoch: 19 -- Batch: 500 -- Loss: 0.9788\n","Train Epoch: 20 -- Batch: 000 -- Loss: 1.0071\n","Train Epoch: 20 -- Batch: 500 -- Loss: 0.9703\n","Train Epoch: 21 -- Batch: 000 -- Loss: 1.0190\n","Train Epoch: 21 -- Batch: 500 -- Loss: 0.9641\n","Train Epoch: 22 -- Batch: 000 -- Loss: 0.9996\n","Train Epoch: 22 -- Batch: 500 -- Loss: 0.9643\n","Train Epoch: 23 -- Batch: 000 -- Loss: 1.0262\n","Train Epoch: 23 -- Batch: 500 -- Loss: 0.9736\n","Train Epoch: 24 -- Batch: 000 -- Loss: 1.0137\n","Train Epoch: 24 -- Batch: 500 -- Loss: 0.9475\n","Train Epoch: 25 -- Batch: 000 -- Loss: 0.9931\n","Train Epoch: 25 -- Batch: 500 -- Loss: 0.9444\n","Train Epoch: 26 -- Batch: 000 -- Loss: 1.0186\n","Train Epoch: 26 -- Batch: 500 -- Loss: 0.9715\n","Train Epoch: 27 -- Batch: 000 -- Loss: 0.9927\n","Train Epoch: 27 -- Batch: 500 -- Loss: 0.9659\n","Train Epoch: 28 -- Batch: 000 -- Loss: 0.9861\n","Train Epoch: 28 -- Batch: 500 -- Loss: 0.9675\n","Train Epoch: 29 -- Batch: 000 -- Loss: 0.9870\n","Train Epoch: 29 -- Batch: 500 -- Loss: 0.9604\n","Train Epoch: 30 -- Batch: 000 -- Loss: 0.9754\n","Train Epoch: 30 -- Batch: 500 -- Loss: 0.9545\n","Train Epoch: 31 -- Batch: 000 -- Loss: 1.0116\n","Train Epoch: 31 -- Batch: 500 -- Loss: 0.9638\n","Train Epoch: 32 -- Batch: 000 -- Loss: 0.9725\n","Train Epoch: 32 -- Batch: 500 -- Loss: 0.9422\n","Train Epoch: 33 -- Batch: 000 -- Loss: 0.9686\n","Train Epoch: 33 -- Batch: 500 -- Loss: 0.9430\n","Train Epoch: 34 -- Batch: 000 -- Loss: 0.9754\n","Train Epoch: 34 -- Batch: 500 -- Loss: 0.9858\n","Train Epoch: 35 -- Batch: 000 -- Loss: 0.9844\n","Train Epoch: 35 -- Batch: 500 -- Loss: 0.9660\n","Train Epoch: 36 -- Batch: 000 -- Loss: 0.9949\n","Train Epoch: 36 -- Batch: 500 -- Loss: 0.9493\n","Train Epoch: 37 -- Batch: 000 -- Loss: 0.9657\n","Train Epoch: 37 -- Batch: 500 -- Loss: 0.9564\n","Train Epoch: 38 -- Batch: 000 -- Loss: 0.9700\n","Train Epoch: 38 -- Batch: 500 -- Loss: 0.9636\n","Train Epoch: 39 -- Batch: 000 -- Loss: 0.9895\n","Train Epoch: 39 -- Batch: 500 -- Loss: 0.9919\n","Train Epoch: 40 -- Batch: 000 -- Loss: 0.9625\n","Train Epoch: 40 -- Batch: 500 -- Loss: 0.9682\n","Train Epoch: 41 -- Batch: 000 -- Loss: 0.9379\n","Train Epoch: 41 -- Batch: 500 -- Loss: 0.9557\n","Train Epoch: 42 -- Batch: 000 -- Loss: 0.9821\n","Train Epoch: 42 -- Batch: 500 -- Loss: 0.9529\n","Train Epoch: 43 -- Batch: 000 -- Loss: 0.9626\n","Train Epoch: 43 -- Batch: 500 -- Loss: 0.9737\n","Train Epoch: 44 -- Batch: 000 -- Loss: 0.9799\n","Train Epoch: 44 -- Batch: 500 -- Loss: 0.9497\n","Train Epoch: 45 -- Batch: 000 -- Loss: 0.9663\n","Train Epoch: 45 -- Batch: 500 -- Loss: 0.9554\n","Train Epoch: 46 -- Batch: 000 -- Loss: 0.9836\n","Train Epoch: 46 -- Batch: 500 -- Loss: 0.9528\n","Train Epoch: 47 -- Batch: 000 -- Loss: 0.9390\n","Train Epoch: 47 -- Batch: 500 -- Loss: 0.9638\n","Train Epoch: 48 -- Batch: 000 -- Loss: 0.9975\n","Train Epoch: 48 -- Batch: 500 -- Loss: 0.9562\n","Train Epoch: 49 -- Batch: 000 -- Loss: 0.9747\n","Train Epoch: 49 -- Batch: 500 -- Loss: 0.9346\n","Train Epoch: 50 -- Batch: 000 -- Loss: 0.9525\n","Train Epoch: 50 -- Batch: 500 -- Loss: 0.9408\n","Train Epoch: 51 -- Batch: 000 -- Loss: 0.9756\n","Train Epoch: 51 -- Batch: 500 -- Loss: 0.9481\n","Train Epoch: 52 -- Batch: 000 -- Loss: 0.9670\n","Train Epoch: 52 -- Batch: 500 -- Loss: 0.9583\n","Train Epoch: 53 -- Batch: 000 -- Loss: 0.9763\n","Train Epoch: 53 -- Batch: 500 -- Loss: 0.9515\n","Train Epoch: 54 -- Batch: 000 -- Loss: 0.9657\n","Train Epoch: 54 -- Batch: 500 -- Loss: 0.9437\n","Train Epoch: 55 -- Batch: 000 -- Loss: 0.9787\n","Train Epoch: 55 -- Batch: 500 -- Loss: 0.9370\n","Train Epoch: 56 -- Batch: 000 -- Loss: 0.9707\n","Train Epoch: 56 -- Batch: 500 -- Loss: 0.9662\n","Train Epoch: 57 -- Batch: 000 -- Loss: 0.9576\n","Train Epoch: 57 -- Batch: 500 -- Loss: 0.9312\n","Train Epoch: 58 -- Batch: 000 -- Loss: 0.9651\n","Train Epoch: 58 -- Batch: 500 -- Loss: 0.9420\n","Train Epoch: 59 -- Batch: 000 -- Loss: 0.9581\n","Train Epoch: 59 -- Batch: 500 -- Loss: 0.9474\n","Train Epoch: 60 -- Batch: 000 -- Loss: 0.9595\n","Train Epoch: 60 -- Batch: 500 -- Loss: 0.9595\n","Train Epoch: 61 -- Batch: 000 -- Loss: 0.9808\n","Train Epoch: 61 -- Batch: 500 -- Loss: 0.9668\n","Train Epoch: 62 -- Batch: 000 -- Loss: 0.9657\n","Train Epoch: 62 -- Batch: 500 -- Loss: 0.9222\n","Train Epoch: 63 -- Batch: 000 -- Loss: 0.9711\n","Train Epoch: 63 -- Batch: 500 -- Loss: 0.9349\n","Train Epoch: 64 -- Batch: 000 -- Loss: 0.9699\n","Train Epoch: 64 -- Batch: 500 -- Loss: 0.9379\n","Train Epoch: 65 -- Batch: 000 -- Loss: 0.9499\n","Train Epoch: 65 -- Batch: 500 -- Loss: 0.9914\n","Train Epoch: 66 -- Batch: 000 -- Loss: 0.9752\n","Train Epoch: 66 -- Batch: 500 -- Loss: 0.9423\n","Train Epoch: 67 -- Batch: 000 -- Loss: 0.9512\n","Train Epoch: 67 -- Batch: 500 -- Loss: 0.9725\n","Train Epoch: 68 -- Batch: 000 -- Loss: 0.9632\n","Train Epoch: 68 -- Batch: 500 -- Loss: 0.9861\n","Train Epoch: 69 -- Batch: 000 -- Loss: 0.9727\n","Train Epoch: 69 -- Batch: 500 -- Loss: 0.9298\n","Train Epoch: 70 -- Batch: 000 -- Loss: 0.9585\n","Train Epoch: 70 -- Batch: 500 -- Loss: 0.9459\n","Train Epoch: 71 -- Batch: 000 -- Loss: 0.9701\n","Train Epoch: 71 -- Batch: 500 -- Loss: 0.9595\n","Train Epoch: 72 -- Batch: 000 -- Loss: 0.9682\n","Train Epoch: 72 -- Batch: 500 -- Loss: 0.9527\n","Train Epoch: 73 -- Batch: 000 -- Loss: 0.9400\n","Train Epoch: 73 -- Batch: 500 -- Loss: 0.9534\n","Train Epoch: 74 -- Batch: 000 -- Loss: 0.9407\n","Train Epoch: 74 -- Batch: 500 -- Loss: 0.9160\n","Train Epoch: 75 -- Batch: 000 -- Loss: 0.9687\n","Train Epoch: 75 -- Batch: 500 -- Loss: 0.9607\n","Train Epoch: 76 -- Batch: 000 -- Loss: 0.9686\n","Train Epoch: 76 -- Batch: 500 -- Loss: 0.9556\n","Train Epoch: 77 -- Batch: 000 -- Loss: 0.9747\n","Train Epoch: 77 -- Batch: 500 -- Loss: 0.9696\n","Train Epoch: 78 -- Batch: 000 -- Loss: 0.9401\n","Train Epoch: 78 -- Batch: 500 -- Loss: 0.9287\n","Train Epoch: 79 -- Batch: 000 -- Loss: 0.9609\n","Train Epoch: 79 -- Batch: 500 -- Loss: 0.9546\n","Train Epoch: 80 -- Batch: 000 -- Loss: 0.9589\n","Train Epoch: 80 -- Batch: 500 -- Loss: 0.9365\n","Train Epoch: 81 -- Batch: 000 -- Loss: 0.9509\n","Train Epoch: 81 -- Batch: 500 -- Loss: 0.9338\n","Train Epoch: 82 -- Batch: 000 -- Loss: 0.9689\n","Train Epoch: 82 -- Batch: 500 -- Loss: 0.9231\n","Train Epoch: 83 -- Batch: 000 -- Loss: 0.9316\n","Train Epoch: 83 -- Batch: 500 -- Loss: 0.9538\n","Train Epoch: 84 -- Batch: 000 -- Loss: 0.9446\n","Train Epoch: 84 -- Batch: 500 -- Loss: 0.9287\n","Train Epoch: 85 -- Batch: 000 -- Loss: 0.9883\n","Train Epoch: 85 -- Batch: 500 -- Loss: 0.9211\n","Train Epoch: 86 -- Batch: 000 -- Loss: 0.9507\n","Train Epoch: 86 -- Batch: 500 -- Loss: 0.9503\n","Train Epoch: 87 -- Batch: 000 -- Loss: 0.9366\n","Train Epoch: 87 -- Batch: 500 -- Loss: 0.9260\n","Train Epoch: 88 -- Batch: 000 -- Loss: 0.9571\n","Train Epoch: 88 -- Batch: 500 -- Loss: 0.9505\n","Train Epoch: 89 -- Batch: 000 -- Loss: 0.9227\n","Train Epoch: 89 -- Batch: 500 -- Loss: 0.9124\n","Train Epoch: 90 -- Batch: 000 -- Loss: 0.9693\n","Train Epoch: 90 -- Batch: 500 -- Loss: 0.9326\n","Train Epoch: 91 -- Batch: 000 -- Loss: 0.9390\n","Train Epoch: 91 -- Batch: 500 -- Loss: 0.9530\n","Train Epoch: 92 -- Batch: 000 -- Loss: 0.9195\n","Train Epoch: 92 -- Batch: 500 -- Loss: 0.9182\n","Train Epoch: 93 -- Batch: 000 -- Loss: 0.9157\n","Train Epoch: 93 -- Batch: 500 -- Loss: 0.9264\n","Train Epoch: 94 -- Batch: 000 -- Loss: 0.9351\n","Train Epoch: 94 -- Batch: 500 -- Loss: 0.9659\n","Train Epoch: 95 -- Batch: 000 -- Loss: 0.9386\n","Train Epoch: 95 -- Batch: 500 -- Loss: 0.9114\n","Train Epoch: 96 -- Batch: 000 -- Loss: 0.9657\n","Train Epoch: 96 -- Batch: 500 -- Loss: 0.9370\n","Train Epoch: 97 -- Batch: 000 -- Loss: 0.9448\n","Train Epoch: 97 -- Batch: 500 -- Loss: 0.9290\n","Train Epoch: 98 -- Batch: 000 -- Loss: 0.9510\n","Train Epoch: 98 -- Batch: 500 -- Loss: 0.9201\n","Train Epoch: 99 -- Batch: 000 -- Loss: 0.9537\n","Train Epoch: 99 -- Batch: 500 -- Loss: 0.9448\n","| \u001b[0m 9       \u001b[0m | \u001b[0m 0.615   \u001b[0m | \u001b[0m 493.1   \u001b[0m | \u001b[0m 0.2666  \u001b[0m | \u001b[0m 0.0695  \u001b[0m | \u001b[0m 113.1   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6189\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.5863\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5666\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.5281\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.4964\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4685\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4518\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.3832\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.3856\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.3648\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.3596\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.3429\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.3236\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.3156\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.3297\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.2599\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.3426\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.2778\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.2840\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.2651\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.2875\n","Train Epoch: 10 -- Batch: 500 -- Loss: 1.2470\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.2773\n","Train Epoch: 11 -- Batch: 500 -- Loss: 1.2310\n","Train Epoch: 12 -- Batch: 000 -- Loss: 1.2452\n","Train Epoch: 12 -- Batch: 500 -- Loss: 1.2425\n","Train Epoch: 13 -- Batch: 000 -- Loss: 1.2788\n","Train Epoch: 13 -- Batch: 500 -- Loss: 1.2110\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.2251\n","Train Epoch: 14 -- Batch: 500 -- Loss: 1.2060\n","Train Epoch: 15 -- Batch: 000 -- Loss: 1.2286\n","Train Epoch: 15 -- Batch: 500 -- Loss: 1.1830\n","Train Epoch: 16 -- Batch: 000 -- Loss: 1.2219\n","Train Epoch: 16 -- Batch: 500 -- Loss: 1.1803\n","Train Epoch: 17 -- Batch: 000 -- Loss: 1.2205\n","Train Epoch: 17 -- Batch: 500 -- Loss: 1.1744\n","Train Epoch: 18 -- Batch: 000 -- Loss: 1.2320\n","Train Epoch: 18 -- Batch: 500 -- Loss: 1.1729\n","Train Epoch: 19 -- Batch: 000 -- Loss: 1.2151\n","Train Epoch: 19 -- Batch: 500 -- Loss: 1.1568\n","Train Epoch: 20 -- Batch: 000 -- Loss: 1.2174\n","Train Epoch: 20 -- Batch: 500 -- Loss: 1.1814\n","Train Epoch: 21 -- Batch: 000 -- Loss: 1.1696\n","Train Epoch: 21 -- Batch: 500 -- Loss: 1.1489\n","Train Epoch: 22 -- Batch: 000 -- Loss: 1.2024\n","Train Epoch: 22 -- Batch: 500 -- Loss: 1.1639\n","Train Epoch: 23 -- Batch: 000 -- Loss: 1.2004\n","Train Epoch: 23 -- Batch: 500 -- Loss: 1.1566\n","Train Epoch: 24 -- Batch: 000 -- Loss: 1.1860\n","Train Epoch: 24 -- Batch: 500 -- Loss: 1.1795\n","Train Epoch: 25 -- Batch: 000 -- Loss: 1.1952\n","Train Epoch: 25 -- Batch: 500 -- Loss: 1.1423\n","Train Epoch: 26 -- Batch: 000 -- Loss: 1.1903\n","Train Epoch: 26 -- Batch: 500 -- Loss: 1.1188\n","Train Epoch: 27 -- Batch: 000 -- Loss: 1.1936\n","Train Epoch: 27 -- Batch: 500 -- Loss: 1.1311\n","Train Epoch: 28 -- Batch: 000 -- Loss: 1.1630\n","Train Epoch: 28 -- Batch: 500 -- Loss: 1.1546\n","Train Epoch: 29 -- Batch: 000 -- Loss: 1.1508\n","Train Epoch: 29 -- Batch: 500 -- Loss: 1.1414\n","Train Epoch: 30 -- Batch: 000 -- Loss: 1.1713\n","Train Epoch: 30 -- Batch: 500 -- Loss: 1.1277\n","Train Epoch: 31 -- Batch: 000 -- Loss: 1.1626\n","Train Epoch: 31 -- Batch: 500 -- Loss: 1.1458\n","Train Epoch: 32 -- Batch: 000 -- Loss: 1.1803\n","Train Epoch: 32 -- Batch: 500 -- Loss: 1.1634\n","Train Epoch: 33 -- Batch: 000 -- Loss: 1.1411\n","Train Epoch: 33 -- Batch: 500 -- Loss: 1.1265\n","Train Epoch: 34 -- Batch: 000 -- Loss: 1.1765\n","Train Epoch: 34 -- Batch: 500 -- Loss: 1.1191\n","Train Epoch: 35 -- Batch: 000 -- Loss: 1.1736\n","Train Epoch: 35 -- Batch: 500 -- Loss: 1.1158\n","Train Epoch: 36 -- Batch: 000 -- Loss: 1.1498\n","Train Epoch: 36 -- Batch: 500 -- Loss: 1.1281\n","Train Epoch: 37 -- Batch: 000 -- Loss: 1.1366\n","Train Epoch: 37 -- Batch: 500 -- Loss: 1.1213\n","Train Epoch: 38 -- Batch: 000 -- Loss: 1.1437\n","Train Epoch: 38 -- Batch: 500 -- Loss: 1.1047\n","Train Epoch: 39 -- Batch: 000 -- Loss: 1.1400\n","Train Epoch: 39 -- Batch: 500 -- Loss: 1.1266\n","Train Epoch: 40 -- Batch: 000 -- Loss: 1.1176\n","Train Epoch: 40 -- Batch: 500 -- Loss: 1.1378\n","Train Epoch: 41 -- Batch: 000 -- Loss: 1.1324\n","Train Epoch: 41 -- Batch: 500 -- Loss: 1.1111\n","Train Epoch: 42 -- Batch: 000 -- Loss: 1.1455\n","Train Epoch: 42 -- Batch: 500 -- Loss: 1.1293\n","Train Epoch: 43 -- Batch: 000 -- Loss: 1.1484\n","Train Epoch: 43 -- Batch: 500 -- Loss: 1.1164\n","Train Epoch: 44 -- Batch: 000 -- Loss: 1.1781\n","Train Epoch: 44 -- Batch: 500 -- Loss: 1.1043\n","Train Epoch: 45 -- Batch: 000 -- Loss: 1.1336\n","Train Epoch: 45 -- Batch: 500 -- Loss: 1.1055\n","Train Epoch: 46 -- Batch: 000 -- Loss: 1.1330\n","Train Epoch: 46 -- Batch: 500 -- Loss: 1.1074\n","Train Epoch: 47 -- Batch: 000 -- Loss: 1.1247\n","Train Epoch: 47 -- Batch: 500 -- Loss: 1.1298\n","Train Epoch: 48 -- Batch: 000 -- Loss: 1.1519\n","Train Epoch: 48 -- Batch: 500 -- Loss: 1.0999\n","Train Epoch: 49 -- Batch: 000 -- Loss: 1.1221\n","Train Epoch: 49 -- Batch: 500 -- Loss: 1.1337\n","Train Epoch: 50 -- Batch: 000 -- Loss: 1.1381\n","Train Epoch: 50 -- Batch: 500 -- Loss: 1.1192\n","Train Epoch: 51 -- Batch: 000 -- Loss: 1.1133\n","Train Epoch: 51 -- Batch: 500 -- Loss: 1.1107\n","Train Epoch: 52 -- Batch: 000 -- Loss: 1.1285\n","Train Epoch: 52 -- Batch: 500 -- Loss: 1.1183\n","Train Epoch: 53 -- Batch: 000 -- Loss: 1.1289\n","Train Epoch: 53 -- Batch: 500 -- Loss: 1.1056\n","Train Epoch: 54 -- Batch: 000 -- Loss: 1.1150\n","Train Epoch: 54 -- Batch: 500 -- Loss: 1.0723\n","Train Epoch: 55 -- Batch: 000 -- Loss: 1.1110\n","Train Epoch: 55 -- Batch: 500 -- Loss: 1.1279\n","Train Epoch: 56 -- Batch: 000 -- Loss: 1.1189\n","Train Epoch: 56 -- Batch: 500 -- Loss: 1.0978\n","Train Epoch: 57 -- Batch: 000 -- Loss: 1.1167\n","Train Epoch: 57 -- Batch: 500 -- Loss: 1.1041\n","Train Epoch: 58 -- Batch: 000 -- Loss: 1.1247\n","Train Epoch: 58 -- Batch: 500 -- Loss: 1.0890\n","Train Epoch: 59 -- Batch: 000 -- Loss: 1.1288\n","Train Epoch: 59 -- Batch: 500 -- Loss: 1.1046\n","Train Epoch: 60 -- Batch: 000 -- Loss: 1.0911\n","Train Epoch: 60 -- Batch: 500 -- Loss: 1.1188\n","Train Epoch: 61 -- Batch: 000 -- Loss: 1.1084\n","Train Epoch: 61 -- Batch: 500 -- Loss: 1.1064\n","Train Epoch: 62 -- Batch: 000 -- Loss: 1.1471\n","Train Epoch: 62 -- Batch: 500 -- Loss: 1.1047\n","Train Epoch: 63 -- Batch: 000 -- Loss: 1.1190\n","Train Epoch: 63 -- Batch: 500 -- Loss: 1.0923\n","Train Epoch: 64 -- Batch: 000 -- Loss: 1.1067\n","Train Epoch: 64 -- Batch: 500 -- Loss: 1.0975\n","Train Epoch: 65 -- Batch: 000 -- Loss: 1.1389\n","Train Epoch: 65 -- Batch: 500 -- Loss: 1.1193\n","Train Epoch: 66 -- Batch: 000 -- Loss: 1.1232\n","Train Epoch: 66 -- Batch: 500 -- Loss: 1.0783\n","Train Epoch: 67 -- Batch: 000 -- Loss: 1.1245\n","Train Epoch: 67 -- Batch: 500 -- Loss: 1.1007\n","Train Epoch: 68 -- Batch: 000 -- Loss: 1.1130\n","Train Epoch: 68 -- Batch: 500 -- Loss: 1.1129\n","Train Epoch: 69 -- Batch: 000 -- Loss: 1.1530\n","Train Epoch: 69 -- Batch: 500 -- Loss: 1.0883\n","Train Epoch: 70 -- Batch: 000 -- Loss: 1.1256\n","Train Epoch: 70 -- Batch: 500 -- Loss: 1.0902\n","Train Epoch: 71 -- Batch: 000 -- Loss: 1.0946\n","Train Epoch: 71 -- Batch: 500 -- Loss: 1.0778\n","Train Epoch: 72 -- Batch: 000 -- Loss: 1.1294\n","Train Epoch: 72 -- Batch: 500 -- Loss: 1.1219\n","Train Epoch: 73 -- Batch: 000 -- Loss: 1.1164\n","Train Epoch: 73 -- Batch: 500 -- Loss: 1.0821\n","Train Epoch: 74 -- Batch: 000 -- Loss: 1.1148\n","Train Epoch: 74 -- Batch: 500 -- Loss: 1.0993\n","Train Epoch: 75 -- Batch: 000 -- Loss: 1.1177\n","Train Epoch: 75 -- Batch: 500 -- Loss: 1.0932\n","Train Epoch: 76 -- Batch: 000 -- Loss: 1.1219\n","Train Epoch: 76 -- Batch: 500 -- Loss: 1.0715\n","Train Epoch: 77 -- Batch: 000 -- Loss: 1.1090\n","Train Epoch: 77 -- Batch: 500 -- Loss: 1.0913\n","Train Epoch: 78 -- Batch: 000 -- Loss: 1.0992\n","Train Epoch: 78 -- Batch: 500 -- Loss: 1.0914\n","Train Epoch: 79 -- Batch: 000 -- Loss: 1.1305\n","Train Epoch: 79 -- Batch: 500 -- Loss: 1.0720\n","Train Epoch: 80 -- Batch: 000 -- Loss: 1.1007\n","Train Epoch: 80 -- Batch: 500 -- Loss: 1.0805\n","Train Epoch: 81 -- Batch: 000 -- Loss: 1.0986\n","Train Epoch: 81 -- Batch: 500 -- Loss: 1.0740\n","Train Epoch: 82 -- Batch: 000 -- Loss: 1.0927\n","Train Epoch: 82 -- Batch: 500 -- Loss: 1.0635\n","Train Epoch: 83 -- Batch: 000 -- Loss: 1.1102\n","Train Epoch: 83 -- Batch: 500 -- Loss: 1.1112\n","Train Epoch: 84 -- Batch: 000 -- Loss: 1.1038\n","Train Epoch: 84 -- Batch: 500 -- Loss: 1.0644\n","Train Epoch: 85 -- Batch: 000 -- Loss: 1.1076\n","Train Epoch: 85 -- Batch: 500 -- Loss: 1.0990\n","Train Epoch: 86 -- Batch: 000 -- Loss: 1.1026\n","Train Epoch: 86 -- Batch: 500 -- Loss: 1.0926\n","Train Epoch: 87 -- Batch: 000 -- Loss: 1.0965\n","Train Epoch: 87 -- Batch: 500 -- Loss: 1.0751\n","Train Epoch: 88 -- Batch: 000 -- Loss: 1.0930\n","Train Epoch: 88 -- Batch: 500 -- Loss: 1.0666\n","Train Epoch: 89 -- Batch: 000 -- Loss: 1.1143\n","Train Epoch: 89 -- Batch: 500 -- Loss: 1.0690\n","Train Epoch: 90 -- Batch: 000 -- Loss: 1.0843\n","Train Epoch: 90 -- Batch: 500 -- Loss: 1.0658\n","Train Epoch: 91 -- Batch: 000 -- Loss: 1.0923\n","Train Epoch: 91 -- Batch: 500 -- Loss: 1.0705\n","Train Epoch: 92 -- Batch: 000 -- Loss: 1.0782\n","Train Epoch: 92 -- Batch: 500 -- Loss: 1.0568\n","Train Epoch: 93 -- Batch: 000 -- Loss: 1.1159\n","Train Epoch: 93 -- Batch: 500 -- Loss: 1.0914\n","Train Epoch: 94 -- Batch: 000 -- Loss: 1.0760\n","Train Epoch: 94 -- Batch: 500 -- Loss: 1.0815\n","Train Epoch: 95 -- Batch: 000 -- Loss: 1.1191\n","Train Epoch: 95 -- Batch: 500 -- Loss: 1.0796\n","Train Epoch: 96 -- Batch: 000 -- Loss: 1.1119\n","Train Epoch: 96 -- Batch: 500 -- Loss: 1.0875\n","Train Epoch: 97 -- Batch: 000 -- Loss: 1.0802\n","Train Epoch: 97 -- Batch: 500 -- Loss: 1.0714\n","Train Epoch: 98 -- Batch: 000 -- Loss: 1.0998\n","Train Epoch: 98 -- Batch: 500 -- Loss: 1.0634\n","Train Epoch: 99 -- Batch: 000 -- Loss: 1.1059\n","Train Epoch: 99 -- Batch: 500 -- Loss: 1.0566\n","| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5975  \u001b[0m | \u001b[0m 371.6   \u001b[0m | \u001b[0m 0.4173  \u001b[0m | \u001b[0m 0.002811\u001b[0m | \u001b[0m 200.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6375\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.1404\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.0875\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.0827\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.0714\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.0594\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.0397\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0494\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.0282\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.0171\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.0159\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.0068\n","Train Epoch: 12 -- Batch: 000 -- Loss: 0.9957\n","Train Epoch: 13 -- Batch: 000 -- Loss: 0.9969\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.0098\n","Train Epoch: 15 -- Batch: 000 -- Loss: 0.9870\n","Train Epoch: 16 -- Batch: 000 -- Loss: 0.9671\n","Train Epoch: 17 -- Batch: 000 -- Loss: 0.9843\n","Train Epoch: 18 -- Batch: 000 -- Loss: 0.9640\n","Train Epoch: 19 -- Batch: 000 -- Loss: 0.9857\n","Train Epoch: 20 -- Batch: 000 -- Loss: 0.9615\n","Train Epoch: 21 -- Batch: 000 -- Loss: 0.9674\n","Train Epoch: 22 -- Batch: 000 -- Loss: 0.9709\n","Train Epoch: 23 -- Batch: 000 -- Loss: 0.9656\n","Train Epoch: 24 -- Batch: 000 -- Loss: 0.9431\n","Train Epoch: 25 -- Batch: 000 -- Loss: 0.9644\n","Train Epoch: 26 -- Batch: 000 -- Loss: 0.9390\n","Train Epoch: 27 -- Batch: 000 -- Loss: 0.9462\n","Train Epoch: 28 -- Batch: 000 -- Loss: 0.9230\n","Train Epoch: 29 -- Batch: 000 -- Loss: 0.9277\n","Train Epoch: 30 -- Batch: 000 -- Loss: 0.9483\n","Train Epoch: 31 -- Batch: 000 -- Loss: 0.9349\n","Train Epoch: 32 -- Batch: 000 -- Loss: 0.9307\n","Train Epoch: 33 -- Batch: 000 -- Loss: 0.8958\n","Train Epoch: 34 -- Batch: 000 -- Loss: 0.9129\n","Train Epoch: 35 -- Batch: 000 -- Loss: 0.9011\n","Train Epoch: 36 -- Batch: 000 -- Loss: 0.9189\n","Train Epoch: 37 -- Batch: 000 -- Loss: 0.9225\n","Train Epoch: 38 -- Batch: 000 -- Loss: 0.8941\n","Train Epoch: 39 -- Batch: 000 -- Loss: 0.8987\n","Train Epoch: 40 -- Batch: 000 -- Loss: 0.9004\n","Train Epoch: 41 -- Batch: 000 -- Loss: 0.9128\n","Train Epoch: 42 -- Batch: 000 -- Loss: 0.9006\n","Train Epoch: 43 -- Batch: 000 -- Loss: 0.8930\n","Train Epoch: 44 -- Batch: 000 -- Loss: 0.8901\n","Train Epoch: 45 -- Batch: 000 -- Loss: 0.8995\n","Train Epoch: 46 -- Batch: 000 -- Loss: 0.8725\n","Train Epoch: 47 -- Batch: 000 -- Loss: 0.9014\n","Train Epoch: 48 -- Batch: 000 -- Loss: 0.8776\n","Train Epoch: 49 -- Batch: 000 -- Loss: 0.9026\n","Train Epoch: 50 -- Batch: 000 -- Loss: 0.9017\n","Train Epoch: 51 -- Batch: 000 -- Loss: 0.8813\n","Train Epoch: 52 -- Batch: 000 -- Loss: 0.8874\n","Train Epoch: 53 -- Batch: 000 -- Loss: 0.8890\n","Train Epoch: 54 -- Batch: 000 -- Loss: 0.9005\n","Train Epoch: 55 -- Batch: 000 -- Loss: 0.8769\n","Train Epoch: 56 -- Batch: 000 -- Loss: 0.8890\n","Train Epoch: 57 -- Batch: 000 -- Loss: 0.8837\n","Train Epoch: 58 -- Batch: 000 -- Loss: 0.8652\n","Train Epoch: 59 -- Batch: 000 -- Loss: 0.8615\n","Train Epoch: 60 -- Batch: 000 -- Loss: 0.8616\n","Train Epoch: 61 -- Batch: 000 -- Loss: 0.8599\n","Train Epoch: 62 -- Batch: 000 -- Loss: 0.8702\n","Train Epoch: 63 -- Batch: 000 -- Loss: 0.8549\n","Train Epoch: 64 -- Batch: 000 -- Loss: 0.8737\n","Train Epoch: 65 -- Batch: 000 -- Loss: 0.8712\n","Train Epoch: 66 -- Batch: 000 -- Loss: 0.8674\n","Train Epoch: 67 -- Batch: 000 -- Loss: 0.8546\n","Train Epoch: 68 -- Batch: 000 -- Loss: 0.8673\n","Train Epoch: 69 -- Batch: 000 -- Loss: 0.8605\n","Train Epoch: 70 -- Batch: 000 -- Loss: 0.8667\n","Train Epoch: 71 -- Batch: 000 -- Loss: 0.8519\n","Train Epoch: 72 -- Batch: 000 -- Loss: 0.8448\n","Train Epoch: 73 -- Batch: 000 -- Loss: 0.8714\n","Train Epoch: 74 -- Batch: 000 -- Loss: 0.8674\n","Train Epoch: 75 -- Batch: 000 -- Loss: 0.8324\n","Train Epoch: 76 -- Batch: 000 -- Loss: 0.8482\n","Train Epoch: 77 -- Batch: 000 -- Loss: 0.8383\n","Train Epoch: 78 -- Batch: 000 -- Loss: 0.8368\n","Train Epoch: 79 -- Batch: 000 -- Loss: 0.8676\n","Train Epoch: 80 -- Batch: 000 -- Loss: 0.8350\n","Train Epoch: 81 -- Batch: 000 -- Loss: 0.8627\n","Train Epoch: 82 -- Batch: 000 -- Loss: 0.8428\n","Train Epoch: 83 -- Batch: 000 -- Loss: 0.8268\n","Train Epoch: 84 -- Batch: 000 -- Loss: 0.8405\n","Train Epoch: 85 -- Batch: 000 -- Loss: 0.8602\n","Train Epoch: 86 -- Batch: 000 -- Loss: 0.8397\n","Train Epoch: 87 -- Batch: 000 -- Loss: 0.8395\n","Train Epoch: 88 -- Batch: 000 -- Loss: 0.8414\n","Train Epoch: 89 -- Batch: 000 -- Loss: 0.8502\n","Train Epoch: 90 -- Batch: 000 -- Loss: 0.8539\n","Train Epoch: 91 -- Batch: 000 -- Loss: 0.8255\n","Train Epoch: 92 -- Batch: 000 -- Loss: 0.8271\n","Train Epoch: 93 -- Batch: 000 -- Loss: 0.8346\n","Train Epoch: 94 -- Batch: 000 -- Loss: 0.8437\n","Train Epoch: 95 -- Batch: 000 -- Loss: 0.8058\n","Train Epoch: 96 -- Batch: 000 -- Loss: 0.8388\n","Train Epoch: 97 -- Batch: 000 -- Loss: 0.8586\n","Train Epoch: 98 -- Batch: 000 -- Loss: 0.8286\n","Train Epoch: 99 -- Batch: 000 -- Loss: 0.8474\n","| \u001b[95m 11      \u001b[0m | \u001b[95m 0.6175  \u001b[0m | \u001b[95m 511.7   \u001b[0m | \u001b[95m 0.2148  \u001b[0m | \u001b[95m 0.04798 \u001b[0m | \u001b[95m 249.3   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.5958\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.1309\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.9878\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.1419\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.0968\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.9851\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.0739\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.0642\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.9399\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.0929\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.0405\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.9585\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.0807\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.0474\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.9418\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.0489\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.0449\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.9329\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.0528\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.0454\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.9293\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0817\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.0509\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.9397\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.0609\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.0543\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.9354\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.0301\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.0311\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.9352\n","Train Epoch: 10 -- Batch: 000 -- Loss: 1.0460\n","Train Epoch: 10 -- Batch: 500 -- Loss: 1.0305\n","Train Epoch: 10 -- Batch: 1000 -- Loss: 0.9200\n","Train Epoch: 11 -- Batch: 000 -- Loss: 1.0249\n","Train Epoch: 11 -- Batch: 500 -- Loss: 1.0445\n","Train Epoch: 11 -- Batch: 1000 -- Loss: 0.9203\n","Train Epoch: 12 -- Batch: 000 -- Loss: 1.0139\n","Train Epoch: 12 -- Batch: 500 -- Loss: 1.0038\n","Train Epoch: 12 -- Batch: 1000 -- Loss: 0.9066\n","Train Epoch: 13 -- Batch: 000 -- Loss: 1.0340\n","Train Epoch: 13 -- Batch: 500 -- Loss: 1.0392\n","Train Epoch: 13 -- Batch: 1000 -- Loss: 0.9223\n","Train Epoch: 14 -- Batch: 000 -- Loss: 1.0245\n","Train Epoch: 14 -- Batch: 500 -- Loss: 1.0449\n","Train Epoch: 14 -- Batch: 1000 -- Loss: 0.9090\n","Train Epoch: 15 -- Batch: 000 -- Loss: 0.9919\n","Train Epoch: 15 -- Batch: 500 -- Loss: 1.0385\n","Train Epoch: 15 -- Batch: 1000 -- Loss: 0.9182\n","Train Epoch: 16 -- Batch: 000 -- Loss: 1.0450\n","Train Epoch: 16 -- Batch: 500 -- Loss: 1.0544\n","Train Epoch: 16 -- Batch: 1000 -- Loss: 0.9158\n","Train Epoch: 17 -- Batch: 000 -- Loss: 1.0602\n","Train Epoch: 17 -- Batch: 500 -- Loss: 1.0204\n","Train Epoch: 17 -- Batch: 1000 -- Loss: 0.9042\n","Train Epoch: 18 -- Batch: 000 -- Loss: 1.0457\n","Train Epoch: 18 -- Batch: 500 -- Loss: 1.0405\n","Train Epoch: 18 -- Batch: 1000 -- Loss: 0.8939\n","Train Epoch: 19 -- Batch: 000 -- Loss: 1.0695\n","Train Epoch: 19 -- Batch: 500 -- Loss: 0.9968\n","Train Epoch: 19 -- Batch: 1000 -- Loss: 0.9154\n","Train Epoch: 20 -- Batch: 000 -- Loss: 1.0336\n","Train Epoch: 20 -- Batch: 500 -- Loss: 0.9884\n","Train Epoch: 20 -- Batch: 1000 -- Loss: 0.9092\n","Train Epoch: 21 -- Batch: 000 -- Loss: 1.0127\n","Train Epoch: 21 -- Batch: 500 -- Loss: 1.0126\n","Train Epoch: 21 -- Batch: 1000 -- Loss: 0.8889\n","Train Epoch: 22 -- Batch: 000 -- Loss: 1.0035\n","Train Epoch: 22 -- Batch: 500 -- Loss: 1.0046\n","Train Epoch: 22 -- Batch: 1000 -- Loss: 0.8936\n","Train Epoch: 23 -- Batch: 000 -- Loss: 1.0474\n","Train Epoch: 23 -- Batch: 500 -- Loss: 1.0333\n","Train Epoch: 23 -- Batch: 1000 -- Loss: 0.9157\n","Train Epoch: 24 -- Batch: 000 -- Loss: 0.9997\n","Train Epoch: 24 -- Batch: 500 -- Loss: 1.0185\n","Train Epoch: 24 -- Batch: 1000 -- Loss: 0.9109\n","Train Epoch: 25 -- Batch: 000 -- Loss: 1.0101\n","Train Epoch: 25 -- Batch: 500 -- Loss: 1.0106\n","Train Epoch: 25 -- Batch: 1000 -- Loss: 0.9071\n","Train Epoch: 26 -- Batch: 000 -- Loss: 1.0304\n","Train Epoch: 26 -- Batch: 500 -- Loss: 0.9683\n","Train Epoch: 26 -- Batch: 1000 -- Loss: 0.9107\n","Train Epoch: 27 -- Batch: 000 -- Loss: 1.0090\n","Train Epoch: 27 -- Batch: 500 -- Loss: 0.9797\n","Train Epoch: 27 -- Batch: 1000 -- Loss: 0.8810\n","Train Epoch: 28 -- Batch: 000 -- Loss: 1.0495\n","Train Epoch: 28 -- Batch: 500 -- Loss: 0.9823\n","Train Epoch: 28 -- Batch: 1000 -- Loss: 0.9000\n","Train Epoch: 29 -- Batch: 000 -- Loss: 1.0397\n","Train Epoch: 29 -- Batch: 500 -- Loss: 0.9810\n","Train Epoch: 29 -- Batch: 1000 -- Loss: 0.8957\n","Train Epoch: 30 -- Batch: 000 -- Loss: 1.0060\n","Train Epoch: 30 -- Batch: 500 -- Loss: 0.9559\n","Train Epoch: 30 -- Batch: 1000 -- Loss: 0.9336\n","Train Epoch: 31 -- Batch: 000 -- Loss: 1.0504\n","Train Epoch: 31 -- Batch: 500 -- Loss: 0.9569\n","Train Epoch: 31 -- Batch: 1000 -- Loss: 0.8760\n","Train Epoch: 32 -- Batch: 000 -- Loss: 1.0342\n","Train Epoch: 32 -- Batch: 500 -- Loss: 0.9758\n","Train Epoch: 32 -- Batch: 1000 -- Loss: 0.9321\n","Train Epoch: 33 -- Batch: 000 -- Loss: 0.9950\n","Train Epoch: 33 -- Batch: 500 -- Loss: 0.9765\n","Train Epoch: 33 -- Batch: 1000 -- Loss: 0.8995\n","Train Epoch: 34 -- Batch: 000 -- Loss: 1.0005\n","Train Epoch: 34 -- Batch: 500 -- Loss: 0.9726\n","Train Epoch: 34 -- Batch: 1000 -- Loss: 0.8745\n","Train Epoch: 35 -- Batch: 000 -- Loss: 1.0137\n","Train Epoch: 35 -- Batch: 500 -- Loss: 0.9385\n","Train Epoch: 35 -- Batch: 1000 -- Loss: 0.8786\n","Train Epoch: 36 -- Batch: 000 -- Loss: 0.9982\n","Train Epoch: 36 -- Batch: 500 -- Loss: 0.9989\n","Train Epoch: 36 -- Batch: 1000 -- Loss: 0.9001\n","Train Epoch: 37 -- Batch: 000 -- Loss: 0.9818\n","Train Epoch: 37 -- Batch: 500 -- Loss: 0.9660\n","Train Epoch: 37 -- Batch: 1000 -- Loss: 0.9206\n","Train Epoch: 38 -- Batch: 000 -- Loss: 0.9846\n","Train Epoch: 38 -- Batch: 500 -- Loss: 0.9555\n","Train Epoch: 38 -- Batch: 1000 -- Loss: 0.8996\n","Train Epoch: 39 -- Batch: 000 -- Loss: 1.0104\n","Train Epoch: 39 -- Batch: 500 -- Loss: 0.9698\n","Train Epoch: 39 -- Batch: 1000 -- Loss: 0.8981\n","Train Epoch: 40 -- Batch: 000 -- Loss: 0.9813\n","Train Epoch: 40 -- Batch: 500 -- Loss: 0.9739\n","Train Epoch: 40 -- Batch: 1000 -- Loss: 0.8822\n","Train Epoch: 41 -- Batch: 000 -- Loss: 1.0246\n","Train Epoch: 41 -- Batch: 500 -- Loss: 0.9769\n","Train Epoch: 41 -- Batch: 1000 -- Loss: 0.8939\n","Train Epoch: 42 -- Batch: 000 -- Loss: 1.0173\n","Train Epoch: 42 -- Batch: 500 -- Loss: 0.9667\n","Train Epoch: 42 -- Batch: 1000 -- Loss: 0.8986\n","Train Epoch: 43 -- Batch: 000 -- Loss: 1.0238\n","Train Epoch: 43 -- Batch: 500 -- Loss: 0.9995\n","Train Epoch: 43 -- Batch: 1000 -- Loss: 0.8944\n","Train Epoch: 44 -- Batch: 000 -- Loss: 1.0069\n","Train Epoch: 44 -- Batch: 500 -- Loss: 0.9260\n","Train Epoch: 44 -- Batch: 1000 -- Loss: 0.9106\n","Train Epoch: 45 -- Batch: 000 -- Loss: 1.0066\n","Train Epoch: 45 -- Batch: 500 -- Loss: 0.9421\n","Train Epoch: 45 -- Batch: 1000 -- Loss: 0.8914\n","Train Epoch: 46 -- Batch: 000 -- Loss: 1.0152\n","Train Epoch: 46 -- Batch: 500 -- Loss: 0.9650\n","Train Epoch: 46 -- Batch: 1000 -- Loss: 0.9059\n","Train Epoch: 47 -- Batch: 000 -- Loss: 1.0145\n","Train Epoch: 47 -- Batch: 500 -- Loss: 0.9503\n","Train Epoch: 47 -- Batch: 1000 -- Loss: 0.9034\n","Train Epoch: 48 -- Batch: 000 -- Loss: 1.0110\n","Train Epoch: 48 -- Batch: 500 -- Loss: 0.9246\n","Train Epoch: 48 -- Batch: 1000 -- Loss: 0.9160\n","Train Epoch: 49 -- Batch: 000 -- Loss: 1.0005\n","Train Epoch: 49 -- Batch: 500 -- Loss: 0.9793\n","Train Epoch: 49 -- Batch: 1000 -- Loss: 0.8808\n","Train Epoch: 50 -- Batch: 000 -- Loss: 0.9981\n","Train Epoch: 50 -- Batch: 500 -- Loss: 0.9160\n","Train Epoch: 50 -- Batch: 1000 -- Loss: 0.9335\n","Train Epoch: 51 -- Batch: 000 -- Loss: 1.0062\n","Train Epoch: 51 -- Batch: 500 -- Loss: 0.9606\n","Train Epoch: 51 -- Batch: 1000 -- Loss: 0.9189\n","Train Epoch: 52 -- Batch: 000 -- Loss: 1.0161\n","Train Epoch: 52 -- Batch: 500 -- Loss: 0.9465\n","Train Epoch: 52 -- Batch: 1000 -- Loss: 0.9350\n","Train Epoch: 53 -- Batch: 000 -- Loss: 0.9943\n","Train Epoch: 53 -- Batch: 500 -- Loss: 0.9896\n","Train Epoch: 53 -- Batch: 1000 -- Loss: 0.8956\n","Train Epoch: 54 -- Batch: 000 -- Loss: 1.0081\n","Train Epoch: 54 -- Batch: 500 -- Loss: 0.9821\n","Train Epoch: 54 -- Batch: 1000 -- Loss: 0.9190\n","Train Epoch: 55 -- Batch: 000 -- Loss: 1.0009\n","Train Epoch: 55 -- Batch: 500 -- Loss: 0.9436\n","Train Epoch: 55 -- Batch: 1000 -- Loss: 0.8863\n","Train Epoch: 56 -- Batch: 000 -- Loss: 1.0210\n","Train Epoch: 56 -- Batch: 500 -- Loss: 0.9911\n","Train Epoch: 56 -- Batch: 1000 -- Loss: 0.8956\n","Train Epoch: 57 -- Batch: 000 -- Loss: 0.9766\n","Train Epoch: 57 -- Batch: 500 -- Loss: 0.9759\n","Train Epoch: 57 -- Batch: 1000 -- Loss: 0.9112\n","Train Epoch: 58 -- Batch: 000 -- Loss: 1.0075\n","Train Epoch: 58 -- Batch: 500 -- Loss: 0.9877\n","Train Epoch: 58 -- Batch: 1000 -- Loss: 0.9136\n","Train Epoch: 59 -- Batch: 000 -- Loss: 1.0345\n","Train Epoch: 59 -- Batch: 500 -- Loss: 0.9443\n","Train Epoch: 59 -- Batch: 1000 -- Loss: 0.8822\n","Train Epoch: 60 -- Batch: 000 -- Loss: 1.0242\n","Train Epoch: 60 -- Batch: 500 -- Loss: 0.9627\n","Train Epoch: 60 -- Batch: 1000 -- Loss: 0.8845\n","Train Epoch: 61 -- Batch: 000 -- Loss: 1.0186\n","Train Epoch: 61 -- Batch: 500 -- Loss: 0.9257\n","Train Epoch: 61 -- Batch: 1000 -- Loss: 0.8672\n","Train Epoch: 62 -- Batch: 000 -- Loss: 0.9821\n","Train Epoch: 62 -- Batch: 500 -- Loss: 0.9365\n","Train Epoch: 62 -- Batch: 1000 -- Loss: 0.9108\n","Train Epoch: 63 -- Batch: 000 -- Loss: 0.9913\n","Train Epoch: 63 -- Batch: 500 -- Loss: 0.9510\n","Train Epoch: 63 -- Batch: 1000 -- Loss: 0.8854\n","Train Epoch: 64 -- Batch: 000 -- Loss: 0.9637\n","Train Epoch: 64 -- Batch: 500 -- Loss: 0.9907\n","Train Epoch: 64 -- Batch: 1000 -- Loss: 0.8754\n","Train Epoch: 65 -- Batch: 000 -- Loss: 0.9675\n","Train Epoch: 65 -- Batch: 500 -- Loss: 0.9693\n","Train Epoch: 65 -- Batch: 1000 -- Loss: 0.8991\n","Train Epoch: 66 -- Batch: 000 -- Loss: 1.0674\n","Train Epoch: 66 -- Batch: 500 -- Loss: 0.9724\n","Train Epoch: 66 -- Batch: 1000 -- Loss: 0.8953\n","Train Epoch: 67 -- Batch: 000 -- Loss: 1.0313\n","Train Epoch: 67 -- Batch: 500 -- Loss: 0.9835\n","Train Epoch: 67 -- Batch: 1000 -- Loss: 0.8400\n","Train Epoch: 68 -- Batch: 000 -- Loss: 1.0063\n","Train Epoch: 68 -- Batch: 500 -- Loss: 0.9344\n","Train Epoch: 68 -- Batch: 1000 -- Loss: 0.8378\n","Train Epoch: 69 -- Batch: 000 -- Loss: 0.9806\n","Train Epoch: 69 -- Batch: 500 -- Loss: 0.9556\n","Train Epoch: 69 -- Batch: 1000 -- Loss: 0.8847\n","Train Epoch: 70 -- Batch: 000 -- Loss: 0.9750\n","Train Epoch: 70 -- Batch: 500 -- Loss: 0.9224\n","Train Epoch: 70 -- Batch: 1000 -- Loss: 0.8816\n","Train Epoch: 71 -- Batch: 000 -- Loss: 0.9514\n","Train Epoch: 71 -- Batch: 500 -- Loss: 0.9581\n","Train Epoch: 71 -- Batch: 1000 -- Loss: 0.8691\n","Train Epoch: 72 -- Batch: 000 -- Loss: 0.9991\n","Train Epoch: 72 -- Batch: 500 -- Loss: 0.8998\n","Train Epoch: 72 -- Batch: 1000 -- Loss: 0.8800\n","Train Epoch: 73 -- Batch: 000 -- Loss: 1.0085\n","Train Epoch: 73 -- Batch: 500 -- Loss: 0.9681\n","Train Epoch: 73 -- Batch: 1000 -- Loss: 0.8452\n","Train Epoch: 74 -- Batch: 000 -- Loss: 0.9995\n","Train Epoch: 74 -- Batch: 500 -- Loss: 0.9441\n","Train Epoch: 74 -- Batch: 1000 -- Loss: 0.8693\n","Train Epoch: 75 -- Batch: 000 -- Loss: 0.9913\n","Train Epoch: 75 -- Batch: 500 -- Loss: 0.9571\n","Train Epoch: 75 -- Batch: 1000 -- Loss: 0.8492\n","Train Epoch: 76 -- Batch: 000 -- Loss: 1.0115\n","Train Epoch: 76 -- Batch: 500 -- Loss: 1.0090\n","Train Epoch: 76 -- Batch: 1000 -- Loss: 0.8857\n","Train Epoch: 77 -- Batch: 000 -- Loss: 0.9966\n","Train Epoch: 77 -- Batch: 500 -- Loss: 0.9822\n","Train Epoch: 77 -- Batch: 1000 -- Loss: 0.8990\n","Train Epoch: 78 -- Batch: 000 -- Loss: 0.9972\n","Train Epoch: 78 -- Batch: 500 -- Loss: 0.9594\n","Train Epoch: 78 -- Batch: 1000 -- Loss: 0.9371\n","Train Epoch: 79 -- Batch: 000 -- Loss: 1.0191\n","Train Epoch: 79 -- Batch: 500 -- Loss: 0.9660\n","Train Epoch: 79 -- Batch: 1000 -- Loss: 0.8979\n","Train Epoch: 80 -- Batch: 000 -- Loss: 1.0023\n","Train Epoch: 80 -- Batch: 500 -- Loss: 0.9125\n","Train Epoch: 80 -- Batch: 1000 -- Loss: 0.8925\n","Train Epoch: 81 -- Batch: 000 -- Loss: 1.0189\n","Train Epoch: 81 -- Batch: 500 -- Loss: 0.9626\n","Train Epoch: 81 -- Batch: 1000 -- Loss: 0.8808\n","Train Epoch: 82 -- Batch: 000 -- Loss: 1.0048\n","Train Epoch: 82 -- Batch: 500 -- Loss: 0.9428\n","Train Epoch: 82 -- Batch: 1000 -- Loss: 0.9067\n","Train Epoch: 83 -- Batch: 000 -- Loss: 0.9883\n","Train Epoch: 83 -- Batch: 500 -- Loss: 0.9618\n","Train Epoch: 83 -- Batch: 1000 -- Loss: 0.8827\n","Train Epoch: 84 -- Batch: 000 -- Loss: 1.0269\n","Train Epoch: 84 -- Batch: 500 -- Loss: 0.9516\n","Train Epoch: 84 -- Batch: 1000 -- Loss: 0.8731\n","Train Epoch: 85 -- Batch: 000 -- Loss: 1.0250\n","Train Epoch: 85 -- Batch: 500 -- Loss: 0.9578\n","Train Epoch: 85 -- Batch: 1000 -- Loss: 0.8438\n","Train Epoch: 86 -- Batch: 000 -- Loss: 0.9840\n","Train Epoch: 86 -- Batch: 500 -- Loss: 0.9868\n","Train Epoch: 86 -- Batch: 1000 -- Loss: 0.8848\n","Train Epoch: 87 -- Batch: 000 -- Loss: 1.0044\n","Train Epoch: 87 -- Batch: 500 -- Loss: 0.9975\n","Train Epoch: 87 -- Batch: 1000 -- Loss: 0.8717\n","Train Epoch: 88 -- Batch: 000 -- Loss: 1.0250\n","Train Epoch: 88 -- Batch: 500 -- Loss: 0.9652\n","Train Epoch: 88 -- Batch: 1000 -- Loss: 0.8410\n","Train Epoch: 89 -- Batch: 000 -- Loss: 1.0163\n","Train Epoch: 89 -- Batch: 500 -- Loss: 0.9560\n","Train Epoch: 89 -- Batch: 1000 -- Loss: 0.8668\n","Train Epoch: 90 -- Batch: 000 -- Loss: 1.0205\n","Train Epoch: 90 -- Batch: 500 -- Loss: 0.9493\n","Train Epoch: 90 -- Batch: 1000 -- Loss: 0.8492\n","Train Epoch: 91 -- Batch: 000 -- Loss: 1.0104\n","Train Epoch: 91 -- Batch: 500 -- Loss: 0.9596\n","Train Epoch: 91 -- Batch: 1000 -- Loss: 0.8981\n","Train Epoch: 92 -- Batch: 000 -- Loss: 0.9917\n","Train Epoch: 92 -- Batch: 500 -- Loss: 0.9634\n","Train Epoch: 92 -- Batch: 1000 -- Loss: 0.8795\n","Train Epoch: 93 -- Batch: 000 -- Loss: 0.9772\n","Train Epoch: 93 -- Batch: 500 -- Loss: 0.9691\n","Train Epoch: 93 -- Batch: 1000 -- Loss: 0.8909\n","Train Epoch: 94 -- Batch: 000 -- Loss: 0.9846\n","Train Epoch: 94 -- Batch: 500 -- Loss: 0.9386\n","Train Epoch: 94 -- Batch: 1000 -- Loss: 0.8798\n","Train Epoch: 95 -- Batch: 000 -- Loss: 1.0078\n","Train Epoch: 95 -- Batch: 500 -- Loss: 0.9643\n","Train Epoch: 95 -- Batch: 1000 -- Loss: 0.8832\n","Train Epoch: 96 -- Batch: 000 -- Loss: 0.9621\n","Train Epoch: 96 -- Batch: 500 -- Loss: 0.9570\n","Train Epoch: 96 -- Batch: 1000 -- Loss: 0.9041\n","Train Epoch: 97 -- Batch: 000 -- Loss: 1.0042\n","Train Epoch: 97 -- Batch: 500 -- Loss: 0.9580\n","Train Epoch: 97 -- Batch: 1000 -- Loss: 0.8778\n","Train Epoch: 98 -- Batch: 000 -- Loss: 0.9681\n","Train Epoch: 98 -- Batch: 500 -- Loss: 0.9559\n","Train Epoch: 98 -- Batch: 1000 -- Loss: 0.8734\n","Train Epoch: 99 -- Batch: 000 -- Loss: 1.0058\n","Train Epoch: 99 -- Batch: 500 -- Loss: 0.9643\n","Train Epoch: 99 -- Batch: 1000 -- Loss: 0.8556\n","| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6056  \u001b[0m | \u001b[0m 178.2   \u001b[0m | \u001b[0m 0.1297  \u001b[0m | \u001b[0m 0.08986 \u001b[0m | \u001b[0m 50.58   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6138\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RVEAum0ACWw7"},"source":["bayesian_optimizer.max"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WzOB-LZo6kkL"},"source":["**CNN**"]},{"cell_type":"code","metadata":{"id":"kjhJctznPijL"},"source":["x_train = train_df_balanced['preprocessed_selftext'].values\n","y_train = train_df_balanced['label'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMF2MLSUPtwp","executionInfo":{"status":"ok","timestamp":1621600131385,"user_tz":-60,"elapsed":369,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"db9ba4e3-91b4-4dba-8ea7-f53192e7cd0b"},"source":["x_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(170000,)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5J4LAJFP8b2","executionInfo":{"status":"ok","timestamp":1621600166964,"user_tz":-60,"elapsed":35438,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"53e16b59-d172-441c-d87e-db61500e9213"},"source":["vocab = {}\n","idx = 1\n","for i in range(0, len(x_train)):\n","  words = x_train[i].split(' ')\n","  for word in words:\n","    if word in model and word not in vocab:\n","      vocab[word] = idx\n","      idx += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSoLrHjWQo1r","executionInfo":{"status":"ok","timestamp":1621600166965,"user_tz":-60,"elapsed":34299,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"7c32a434-5296-47a2-e70c-fe3070c81bff"},"source":["len(vocab)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["118870"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"5J1Wkp2_Q4tY"},"source":["# convert each training document into list of word indices\n","x_train_numeric = []\n","max_length = 0\n","for i in range(0, len(x_train)):\n","  word_indices = []\n","  words = x_train[i].split(' ')\n","  for word in words:\n","    if word in vocab:\n","      word_indices.append(vocab[word])\n","  \n","  x_train_numeric.append(word_indices)\n","  if len(word_indices) > max_length:\n","    max_length = len(word_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XluuIYPlRYMg","executionInfo":{"status":"ok","timestamp":1621600173563,"user_tz":-60,"elapsed":6581,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"64de4d2c-0f7e-41d7-af22-1607a123215e"},"source":["len(x_train_numeric[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["119"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"YzfOf5iCiFic"},"source":["max_length = 400"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_yTFFkKSCv5"},"source":["# add padding\n","for i in range(0, len(x_train_numeric)):\n","  num_zeros = max_length - len(x_train_numeric[i])\n","  if num_zeros < 0:\n","    x_train_numeric[i] = x_train_numeric[i][ : num_zeros]\n","  else:\n","    for j in range(0, num_zeros):\n","      x_train_numeric[i].append(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1clJ-V5SR6t"},"source":["x_train_numeric = np.array(x_train_numeric)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sPN5Vm5SU7W","executionInfo":{"status":"ok","timestamp":1621600186018,"user_tz":-60,"elapsed":19008,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"dacf4380-df3c-4768-b9d2-76b0d54c42ce"},"source":["x_train_numeric.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(170000, 400)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IekeENEJbAPS","executionInfo":{"status":"ok","timestamp":1621600186706,"user_tz":-60,"elapsed":19689,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"d5544b69-cc93-49c8-f295-2b4f69abd17b"},"source":["weights_matrix = np.zeros((len(vocab) + 1, 300))\n","for word in vocab:\n","  weights_matrix[vocab[word]] = model[word]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"jybqYR5o6pU4"},"source":["class CNN(nn.Module):\n","  def __init__(self, seq_len, num_words, embedding_size, num_filters_1, num_filters_2, num_filters_3, dropout_p):\n","      super().__init__()\n","\n","      self.seq_len = seq_len\n","      self.num_words = num_words\n","      self.embedding_size = embedding_size\n","\n","      self.dropout = nn.Dropout(dropout_p)\n","      self.num_filters_1 = num_filters_1\n","      self.num_filters_2 = num_filters_2\n","      self.num_filters_3 = num_filters_3\n","\n","      # kernel sizes\n","      self.kernel_1 = 2\n","      self.kernel_2 = 3\n","      self.kernel_3 = 4\n","  \n","      self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n","      self.embedding.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n","      self.embedding.weight.requires_grad = False\n","\n","      # Convolution layers definition\n","      self.conv_1 = nn.Conv1d(self.embedding_size, self.num_filters_1, kernel_size=self.kernel_1) # bigrams\n","      self.conv_2 = nn.Conv1d(self.embedding_size, self.num_filters_2, kernel_size=self.kernel_2) # trigrams\n","      self.conv_3 = nn.Conv1d(self.embedding_size, self.num_filters_3, kernel_size=self.kernel_3) # 4-grams\n","\n","      # Max pooling layers definition\n","      self.pool_1 = nn.MaxPool1d(self.seq_len - self.kernel_1 + 1)\n","      self.pool_2 = nn.MaxPool1d(self.seq_len - self.kernel_2 + 1)\n","      self.pool_3 = nn.MaxPool1d(self.seq_len - self.kernel_3 + 1)\n","\n","      self.fc1 = nn.Linear(self.num_filters_1 + self.num_filters_2 + self.num_filters_3, 50)\n","      self.fc2 = nn.Linear(50, 5)\n","\n","  def forward(self, x):\n","      x = x.int()\n","      x = self.embedding(x)\n","      x = x.permute(0, 2, 1)\n","\n","      # Convolution layer 1 is applied\n","      x1 = self.conv_1(x)\n","      x1 = torch.relu(x1)\n","      x1 = self.pool_1(x1)\n","\n","      # Convolution layer 2 is applied\n","      x2 = self.conv_2(x)\n","      x2 = torch.relu((x2))\n","      x2 = self.pool_2(x2)\n","\n","      # Convolution layer 3 is applied\n","      x3 = self.conv_3(x)\n","      x3 = torch.relu(x3)\n","      x3 = self.pool_3(x3)\n","\n","      # The output of each convolutional layer is concatenated into a unique vector\n","      union = torch.cat((x1, x2, x3), 1)\n","      union = union.reshape(union.size(0), -1)\n","\n","      # The \"flattened\" vector is passed through a fully connected layer\n","      h = self.fc1(union)\n","      # Dropout is applied\t\t\n","      h = self.dropout(h)\n","      h = torch.relu(h)\n","\n","      out = self.fc2(h)\n","      # Activation function is applied\n","      #out = torch.softmax(out)\n","\n","      return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rATnjPN6S3rA"},"source":["#one_hot = torch.nn.functional.one_hot(torch.from_numpy(y_train)).long()\n","train = DepressionDataset(x_train_numeric, y_train)\n","train_dataloader = DataLoader(train, batch_size=415)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mor8J8-ZTRPw","executionInfo":{"elapsed":1485513,"status":"ok","timestamp":1617801382320,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"6b4e29bc-d578-440b-b26d-4cb3482642c6"},"source":["classifier = CNN(seq_len=max_length, num_words=len(vocab), embedding_size=300, num_filters_1=600, num_filters_2=500, num_filters_3=300, dropout_p=0.4).to(device)\n","optimizer = optim.Adam(classifier.parameters(), lr=0.0001)\n","# Train-test loop\n","for epoch in range(10):\n","  train_model(classifier, train_dataloader, optimizer, epoch)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Epoch: 00 -- Batch: 000 -- Loss: 1.6789\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6664\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5964\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5762\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5724\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5637\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5304\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4720\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4728\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4479\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XTCIX1a6VBgX"},"source":["x_test = val_df_balanced['preprocessed_selftext'].values\n","y_test = val_df_balanced['label'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"POGwUbSKVMZA"},"source":["# convert each training document into list of word indices\n","x_test_numeric = []\n","\n","for i in range(0, len(x_test)):\n","  word_indices = []\n","  words = x_test[i].split(' ')\n","  for word in words:\n","    if word in vocab:\n","      word_indices.append(vocab[word])\n","  \n","  x_test_numeric.append(word_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xiE8Ae9_VdOo"},"source":["# add padding\n","for i in range(0, len(x_test_numeric)):\n","  num_zeros = max_length - len(x_test_numeric[i])\n","  if num_zeros < 0:\n","    x_test_numeric[i] = x_test_numeric[i][ : num_zeros]\n","  else:\n","    for j in range(0, num_zeros):\n","      x_test_numeric[i].append(0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DJqY8OFQV6JG"},"source":["x_test_numeric = np.array(x_test_numeric)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OnIOYmcNV81J","executionInfo":{"status":"ok","timestamp":1621600196207,"user_tz":-60,"elapsed":865,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"3058169c-ceff-4886-ae33-0b35f638657b"},"source":["x_test_numeric.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20000, 400)"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"YBviVPH3Wk5s"},"source":["test = DepressionDataset(x_test_numeric, y_test)\n","test_dataloader = DataLoader(test, batch_size=415)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EQ3tHS6W35m"},"source":["def get_model_outputs(model, test_loader):\n","    model.eval()\n","    outputs = []\n","    for batch_idx, (inputs, targets) in enumerate(test_loader):\n","        # We need to send our batch to the device we are using. If this is not\n","        # it will default to using the CPU.\n","        inputs = inputs.to(device)\n","        \n","        y_pred = model(inputs)\n","        y_pred = y_pred.detach().cpu().numpy()\n","        for i in range(0, len(y_pred)):\n","          outputs.append(y_pred[i].argmax())\n","\n","\n","    return np.array(outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DwWQbE2pXhdO"},"source":["y_pred = get_model_outputs(classifier, train_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ii1fLk0vYHBI","executionInfo":{"elapsed":766,"status":"ok","timestamp":1617801761511,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"642b09f3-e7b1-459b-9502-b6e008d59ebf"},"source":["len(y_pred)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["170000"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7zOCYPdPYMGq","executionInfo":{"elapsed":496,"status":"ok","timestamp":1617801545909,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"7322fe54-2919-42ea-f583-f91806bd7bb5"},"source":["len(y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20000"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ePpNmHnmYN1I","executionInfo":{"elapsed":813,"status":"ok","timestamp":1617801767520,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"9e0b5bbd-e49d-4c44-9c91-3d73329c8224"},"source":["print(classification_report(y_train, y_pred))\n","#print(precision_score(y_val, y_pred))\n","#print(recall_score(y_val, y_pred))\n","#print(f1_score(y_val, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.98      0.97     34000\n","           1       0.86      0.90      0.88     34000\n","           2       0.92      0.91      0.92     34000\n","           3       0.88      0.89      0.89     34000\n","           4       0.93      0.86      0.90     34000\n","\n","    accuracy                           0.91    170000\n","   macro avg       0.91      0.91      0.91    170000\n","weighted avg       0.91      0.91      0.91    170000\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"id":"G-PStawJfKuI","executionInfo":{"elapsed":1452,"status":"error","timestamp":1617801773212,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"6e4e5297-692f-4afd-d6fd-4ccc77f029d8"},"source":["print(confusion_matrix(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-0c1f547a8d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [20000, 170000]"]}]},{"cell_type":"code","metadata":{"id":"7zxK1EqtS93q"},"source":["def bayesian_train_model(num_filters_1, num_filters_2, num_filters_3, dropout_p, batch_size, lr):\n","  classifier = CNN(seq_len=max_length, num_words=len(vocab), embedding_size=300, num_filters_1=int(num_filters_1), num_filters_2=int(num_filters_2), num_filters_3=int(num_filters_3), dropout_p=dropout_p).to(device)\n","  optimizer = optim.Adam(classifier.parameters(), lr=lr)\n","\n","  train = DepressionDataset(x_train_numeric, y_train)\n","  train_dataloader = DataLoader(train, batch_size=int(batch_size))\n","\n","  # Train-test loop\n","  for epoch in range(10):\n","    train_model(classifier, train_dataloader, optimizer, epoch)\n","  \n","  test = DepressionDataset(x_test_numeric, y_test)\n","  test_dataloader = DataLoader(test, batch_size=int(batch_size))\n","\n","  y_pred = get_model_outputs(classifier, test_dataloader)\n","\n","  return accuracy_score(y_test, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAQlBN50To87"},"source":["bounds = {\n","    'num_filters_1':(50, 600),\n","    'num_filters_2':(50, 600),\n","    'num_filters_3':(50, 600),\n","    'dropout_p': (0, 0.5),\n","    'batch_size':(64, 512),\n","    'lr':(0.0001, 0.01),\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1lvMVyZ2UTzi","executionInfo":{"elapsed":37431314,"status":"ok","timestamp":1617231370550,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"16fe4b03-e261-4725-ac0b-5cf5c3fa1d05"},"source":["bayesian_optimizer = BayesianOptimization(\n","    f=bayesian_train_model,\n","    pbounds=bounds,\n","    random_state=1,\n",")\n","bayesian_optimizer.maximize(init_points=10, n_iter=50)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["|   iter    |  target   | batch_... | dropout_p |    lr     | num_fi... | num_fi... | num_fi... |\n","-------------------------------------------------------------------------------------------------\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.7118\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.6032\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6889\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.5562\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6041\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.4901\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6111\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.4657\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5455\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.4648\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5055\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.4336\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5128\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4171\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4609\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4133\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4315\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.3730\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4004\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.3614\n","| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8135  \u001b[0m | \u001b[0m 250.8   \u001b[0m | \u001b[0m 0.3602  \u001b[0m | \u001b[0m 0.000101\u001b[0m | \u001b[0m 216.3   \u001b[0m | \u001b[0m 130.7   \u001b[0m | \u001b[0m 100.8   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6389\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.7143\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.7085\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7007\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.8046\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.7094\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7949\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5569\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.6458\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6958\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.4792\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.6656\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5696\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.4504\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.6148\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5788\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.6037\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.6506\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5699\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4467\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.5581\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5501\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4600\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.5252\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4691\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4580\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.5779\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6446\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.3616\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.5797\n","| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7602  \u001b[0m | \u001b[0m 147.4   \u001b[0m | \u001b[0m 0.1728  \u001b[0m | \u001b[0m 0.004028\u001b[0m | \u001b[0m 346.3   \u001b[0m | \u001b[0m 280.6   \u001b[0m | \u001b[0m 426.9   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6528\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.6131\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.6034\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7518\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.5606\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.6298\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6233\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5287\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.5583\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6093\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.4255\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.4307\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5568\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.4552\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.3486\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4032\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.3657\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.3574\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4177\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.3298\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.2509\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.3307\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.2820\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.2457\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.3339\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.2325\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.2245\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.3213\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.3077\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.2002\n","| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7844  \u001b[0m | \u001b[0m 155.6   \u001b[0m | \u001b[0m 0.4391  \u001b[0m | \u001b[0m 0.000371\u001b[0m | \u001b[0m 418.8   \u001b[0m | \u001b[0m 279.5   \u001b[0m | \u001b[0m 357.3   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6118\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.9684\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.7889\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7447\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.7964\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.8606\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7713\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.8939\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.6852\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7440\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.8921\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.8533\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.8688\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.9513\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.8612\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5724\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.8986\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.7600\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7736\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.9236\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.5992\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0379\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.8384\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.6486\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.7797\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.9894\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.8525\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.9728\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.1181\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.9136\n","| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7281  \u001b[0m | \u001b[0m 126.9   \u001b[0m | \u001b[0m 0.09905 \u001b[0m | \u001b[0m 0.008027\u001b[0m | \u001b[0m 582.5   \u001b[0m | \u001b[0m 222.4   \u001b[0m | \u001b[0m 430.8   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6447\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7091\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6473\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6668\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5886\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5485\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4130\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4171\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.3443\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.3647\n","| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7944  \u001b[0m | \u001b[0m 456.6   \u001b[0m | \u001b[0m 0.4473  \u001b[0m | \u001b[0m 0.000941\u001b[0m | \u001b[0m 71.48   \u001b[0m | \u001b[0m 143.4   \u001b[0m | \u001b[0m 533.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6121\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.9787\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.9580\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.3094\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.2573\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.2289\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.4320\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.5108\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5611\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.5445\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.4444\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.4707\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5560\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6154\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.5325\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.6070\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6114\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6060\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.6101\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.6106\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6121\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6119\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.5882\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.5957\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6121\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6104\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.6092\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.6107\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6122\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6103\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.6102\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.6107\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6122\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6103\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.6102\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.6107\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6122\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6103\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.6102\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.6107\n","| \u001b[0m 6       \u001b[0m | \u001b[0m 0.2     \u001b[0m | \u001b[0m 108.1   \u001b[0m | \u001b[0m 0.2106  \u001b[0m | \u001b[0m 0.009583\u001b[0m | \u001b[0m 343.2   \u001b[0m | \u001b[0m 430.5   \u001b[0m | \u001b[0m 223.5   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6458\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6953\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6958\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6591\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5659\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5243\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4328\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.3741\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.3319\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.3204\n","| \u001b[0m 7       \u001b[0m | \u001b[0m 0.794   \u001b[0m | \u001b[0m 371.6   \u001b[0m | \u001b[0m 0.4173  \u001b[0m | \u001b[0m 0.000281\u001b[0m | \u001b[0m 462.6   \u001b[0m | \u001b[0m 593.9   \u001b[0m | \u001b[0m 461.5   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6726\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.6845\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7214\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.7674\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6538\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.6297\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6455\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5926\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5227\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.4734\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4617\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.4477\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4447\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4182\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.3470\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.3466\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.3867\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.2931\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.3384\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.2709\n","| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7931  \u001b[0m | \u001b[0m 189.6   \u001b[0m | \u001b[0m 0.3946  \u001b[0m | \u001b[0m 0.001122\u001b[0m | \u001b[0m 296.3   \u001b[0m | \u001b[0m 549.7   \u001b[0m | \u001b[0m 211.5   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6738\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.5676\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.5879\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.4945\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.4690\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.4398\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.4000\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.3655\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.3034\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.2541\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.2134\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.1741\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.1701\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.1470\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.2221\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.1370\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.1305\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.1332\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.1428\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.0556\n","| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7832  \u001b[0m | \u001b[0m 192.9   \u001b[0m | \u001b[0m 0.06501 \u001b[0m | \u001b[0m 0.000291\u001b[0m | \u001b[0m 423.4   \u001b[0m | \u001b[0m 166.4   \u001b[0m | \u001b[0m 196.1   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6558\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.6480\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7122\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.5595\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6895\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5555\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6119\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.4393\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5638\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5355\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4734\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.4313\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7507\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4070\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6206\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.3301\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.3697\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.2744\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4260\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.2746\n","| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7726  \u001b[0m | \u001b[0m 284.2   \u001b[0m | \u001b[0m 0.02668 \u001b[0m | \u001b[0m 0.005784\u001b[0m | \u001b[0m 130.7   \u001b[0m | \u001b[0m 374.1   \u001b[0m | \u001b[0m 434.9   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6990\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7656\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6831\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6701\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6337\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6270\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5885\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5773\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5382\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5275\n","| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8105  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6718\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.6696\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6959\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.5910\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6196\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5779\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5821\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5344\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5499\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5109\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5288\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.4518\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4811\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4380\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4509\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4296\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4255\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4124\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4058\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.3659\n","| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8118  \u001b[0m | \u001b[0m 246.6   \u001b[0m | \u001b[0m 0.354   \u001b[0m | \u001b[0m 0.000112\u001b[0m | \u001b[0m 224.6   \u001b[0m | \u001b[0m 183.7   \u001b[0m | \u001b[0m 55.37   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6700\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7746\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7203\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7131\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6517\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6582\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6220\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5862\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5800\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5503\n","| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8129  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6332\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.6446\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7793\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.6206\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7428\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5898\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6460\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.6596\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6743\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5990\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.7298\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5583\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4936\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5891\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6404\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.6126\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6154\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4502\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5910\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.7338\n","| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7669  \u001b[0m | \u001b[0m 247.5   \u001b[0m | \u001b[0m 0.06115 \u001b[0m | \u001b[0m 0.007327\u001b[0m | \u001b[0m 527.4   \u001b[0m | \u001b[0m 414.6   \u001b[0m | \u001b[0m 267.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6545\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.7110\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7582\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.6420\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6987\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.6276\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7539\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5667\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.4944\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6336\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5669\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5398\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5312\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4928\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6610\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5660\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6761\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4891\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5756\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.3570\n","| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7689  \u001b[0m | \u001b[0m 201.3   \u001b[0m | \u001b[0m 0.09728 \u001b[0m | \u001b[0m 0.004773\u001b[0m | \u001b[0m 421.8   \u001b[0m | \u001b[0m 156.0   \u001b[0m | \u001b[0m 212.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.7210\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7627\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7003\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6678\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6286\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6043\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6005\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5463\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5307\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4950\n","| \u001b[0m 16      \u001b[0m | \u001b[0m 0.8125  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6582\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7787\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7566\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6628\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6246\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6391\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6096\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5774\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4883\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4867\n","| \u001b[0m 17      \u001b[0m | \u001b[0m 0.8131  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.7737\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.8791\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.8753\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.8491\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.8541\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.7760\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.8782\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.7768\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.8224\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6970\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.8037\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.8341\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7443\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.7615\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.7388\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.7016\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.7245\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.7292\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6806\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.6742\n","| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7927  \u001b[0m | \u001b[0m 254.3   \u001b[0m | \u001b[0m 0.4938  \u001b[0m | \u001b[0m 0.003519\u001b[0m | \u001b[0m 227.9   \u001b[0m | \u001b[0m 139.2   \u001b[0m | \u001b[0m 106.6   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6232\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7568\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6708\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6534\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6070\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6067\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5917\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5541\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5063\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4952\n","| \u001b[0m 19      \u001b[0m | \u001b[0m 0.8127  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.5877\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.0010\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.8746\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 0.6888\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 0.6431\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 0.4872\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6069\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.7526\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.7682\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 0.5594\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 0.6750\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 0.3507\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5511\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.6670\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.6354\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 0.5360\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 0.5389\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 0.4655\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.4287\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.6606\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.6250\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 0.4896\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 0.5757\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 0.3425\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.4157\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6084\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.4548\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 0.4772\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 0.5461\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 0.2838\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4593\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5404\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.5660\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 0.3474\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 0.3981\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 0.2885\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.2925\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4803\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.3949\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.3273\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 0.3698\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.1945\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.2804\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4340\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.3273\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.3580\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.2982\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.1923\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.2375\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.3211\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.3254\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.1646\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 0.2709\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.2058\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.1796\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.2374\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.2709\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.2028\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.2149\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.1650\n","| \u001b[0m 20      \u001b[0m | \u001b[0m 0.7945  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6437\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.5419\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6080\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6129\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6159\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 1.6083\n","| \u001b[0m 21      \u001b[0m | \u001b[0m 0.2     \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6725\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.7944\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.7357\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 0.7269\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 0.6081\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 0.4251\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6676\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.7056\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.6826\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 0.6805\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 0.5744\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 0.3228\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6415\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.6629\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.6322\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 0.6068\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 0.6418\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 0.3489\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5230\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.7204\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.6738\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 0.6286\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 0.5313\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 0.3582\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.4670\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.7914\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.5603\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 0.5008\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 0.5865\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 0.3555\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4239\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5340\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.5286\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 0.4896\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 0.5540\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 0.3480\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.3603\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5673\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.5174\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.4212\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 0.4983\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.3041\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.3361\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5224\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.3356\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.3660\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.4416\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.2751\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.3600\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.5029\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.4849\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.3315\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 0.4521\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.1782\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.3305\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.4437\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.3697\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.2840\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.3655\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.2041\n","| \u001b[0m 22      \u001b[0m | \u001b[0m 0.8052  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6423\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7357\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6396\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6695\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5653\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5805\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5400\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4898\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5228\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4608\n","| \u001b[0m 23      \u001b[0m | \u001b[0m 0.7834  \u001b[0m | \u001b[0m 474.2   \u001b[0m | \u001b[0m 0.1986  \u001b[0m | \u001b[0m 0.005338\u001b[0m | \u001b[0m 101.2   \u001b[0m | \u001b[0m 72.29   \u001b[0m | \u001b[0m 56.21   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.7321\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.4620\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.4225\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5267\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6110\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6111\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6111\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.6111\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.6111\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.6111\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.6111\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.6111\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.6111\n","| \u001b[0m 24      \u001b[0m | \u001b[0m 0.2     \u001b[0m | \u001b[0m 131.7   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.009679\u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6698\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6308\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5716\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5356\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5039\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4730\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4384\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4022\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.3604\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.3179\n","| \u001b[0m 25      \u001b[0m | \u001b[0m 0.8094  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 284.0   \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 254.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6200\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.2002\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.2556\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.1972\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.1937\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.2206\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.3247\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.2748\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.3514\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4620\n","| \u001b[0m 26      \u001b[0m | \u001b[0m 0.737   \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.4994  \u001b[0m | \u001b[0m 0.009381\u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 384.5   \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6627\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.8747\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.8294\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 0.6391\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 0.6553\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 0.3791\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.5834\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.8088\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.7992\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 0.7133\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 0.6955\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 0.3966\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5641\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.6609\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.5919\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 0.5820\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 0.5807\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 0.3167\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6150\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.6348\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.6554\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 0.3897\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 0.5294\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 0.3297\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.4558\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6190\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.4393\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 0.4486\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 0.4813\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 0.2243\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4256\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5629\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.4549\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 0.3956\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 0.4814\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 0.2546\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4001\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5054\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.3698\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.3024\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 0.3067\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.2018\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.2800\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4970\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.4718\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.2113\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.3031\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.1533\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.2718\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.3828\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.3210\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.1747\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 0.2028\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.1671\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.2358\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.2292\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.1619\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.1290\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.1370\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.1272\n","| \u001b[0m 27      \u001b[0m | \u001b[0m 0.7996  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6611\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.4905\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.3303\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6018\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6152\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6145\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 1.6083\n","| \u001b[0m 28      \u001b[0m | \u001b[0m 0.2     \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6155\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.6708\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.6515\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 0.5830\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 0.5660\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 0.3408\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.5387\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.5722\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.5525\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 0.4916\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 0.5104\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 0.2677\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.4962\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.4941\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.4751\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 0.3920\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 0.4030\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 0.2350\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.3728\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.4232\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.3891\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 0.2332\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 0.3781\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 0.2455\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.1745\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.4135\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.3651\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 0.1455\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 0.3400\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 0.1298\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.2836\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.2689\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.1488\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 0.1093\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 0.2296\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 0.0546\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.0670\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.2229\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.1579\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.0798\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 0.1383\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.0640\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.0618\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.2302\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.1252\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.1486\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.2306\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.0327\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.3165\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.2432\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.0809\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.2276\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 0.0443\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.0446\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.0708\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.0821\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.1402\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.0445\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.0307\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.0208\n","| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7689  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.00065 \u001b[0m | \u001b[0m 429.2   \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6851\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.8554\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.6849\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6482\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.7602\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.5537\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6259\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5799\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.5472\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6018\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.6978\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.5120\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5442\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5946\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.5284\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5145\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5929\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.4298\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4157\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5292\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.4111\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4628\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4372\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.3801\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.3658\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4629\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.3505\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.3944\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.4354\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.3521\n","| \u001b[0m 30      \u001b[0m | \u001b[0m 0.8082  \u001b[0m | \u001b[0m 137.7   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6889\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.8426\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.8964\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7614\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6372\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6773\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4997\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6374\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4393\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5698\n","| \u001b[0m 31      \u001b[0m | \u001b[0m 0.7599  \u001b[0m | \u001b[0m 362.4   \u001b[0m | \u001b[0m 0.1088  \u001b[0m | \u001b[0m 0.006001\u001b[0m | \u001b[0m 341.4   \u001b[0m | \u001b[0m 277.8   \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.7872\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6101\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6101\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6102\n","| \u001b[0m 32      \u001b[0m | \u001b[0m 0.2     \u001b[0m | \u001b[0m 357.4   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 353.2   \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6955\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7299\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7433\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7631\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6835\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5956\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6865\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6059\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5403\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6048\n","| \u001b[0m 33      \u001b[0m | \u001b[0m 0.7724  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.1077  \u001b[0m | \u001b[0m 0.009671\u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 421.5   \u001b[0m | \u001b[0m 286.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6889\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.5683\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.4787\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.3730\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.2804\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.1791\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.1385\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.0863\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.1182\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.2132\n","| \u001b[0m 34      \u001b[0m | \u001b[0m 0.784   \u001b[0m | \u001b[0m 345.3   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.000618\u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 323.8   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6448\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7056\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6363\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6230\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6609\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5448\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5449\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5035\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6336\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4297\n","| \u001b[0m 35      \u001b[0m | \u001b[0m 0.7703  \u001b[0m | \u001b[0m 379.6   \u001b[0m | \u001b[0m 0.06692 \u001b[0m | \u001b[0m 0.008002\u001b[0m | \u001b[0m 399.9   \u001b[0m | \u001b[0m 389.4   \u001b[0m | \u001b[0m 343.4   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6809\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.4749\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.5804\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.4154\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5217\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.3663\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.4489\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.3118\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.3646\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.2498\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.2763\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.2049\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.2553\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.1838\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.1513\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.1643\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.1625\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.1554\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.1064\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.1218\n","| \u001b[0m 36      \u001b[0m | \u001b[0m 0.7923  \u001b[0m | \u001b[0m 282.1   \u001b[0m | \u001b[0m 1.249e-0\u001b[0m | \u001b[0m 0.000394\u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 239.4   \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6963\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6108\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6080\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6159\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6084\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 1.6083\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.6130\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 1.6160\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 1.6083\n","| \u001b[0m 37      \u001b[0m | \u001b[0m 0.2     \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 322.1   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.8385\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.2931\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.4491\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.2179\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.3610\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.2052\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.2757\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.2258\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.2773\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.2862\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.3796\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.2172\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2762\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.3281\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.3109\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.2982\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.3700\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.5463\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5997\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.5483\n","| \u001b[0m 38      \u001b[0m | \u001b[0m 0.3126  \u001b[0m | \u001b[0m 185.4   \u001b[0m | \u001b[0m 0.4653  \u001b[0m | \u001b[0m 0.006268\u001b[0m | \u001b[0m 290.2   \u001b[0m | \u001b[0m 550.0   \u001b[0m | \u001b[0m 197.6   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6350\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.5835\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.5657\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.5306\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5054\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.4896\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.4584\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.4543\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.4108\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.4115\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.3583\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.3597\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.3006\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.2991\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.2426\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.2348\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.1840\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.1767\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.1326\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.1242\n","| \u001b[0m 39      \u001b[0m | \u001b[0m 0.7994  \u001b[0m | \u001b[0m 234.4   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 330.3   \u001b[0m | \u001b[0m 564.5   \u001b[0m | \u001b[0m 315.3   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6720\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.4818\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.4875\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.4541\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.4394\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4628\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4512\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.4413\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4172\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4575\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4130\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.3917\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4625\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4321\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4605\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.3803\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4473\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.3643\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4378\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.3858\n","| \u001b[0m 40      \u001b[0m | \u001b[0m 0.3617  \u001b[0m | \u001b[0m 315.5   \u001b[0m | \u001b[0m 0.4055  \u001b[0m | \u001b[0m 0.00811 \u001b[0m | \u001b[0m 280.5   \u001b[0m | \u001b[0m 466.0   \u001b[0m | \u001b[0m 249.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6429\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.6062\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.5714\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.5389\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5021\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.4972\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.4490\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.4496\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.3914\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.3924\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.3247\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.3325\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.2589\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.2703\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.1938\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.2074\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.1403\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.1473\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.1001\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.1034\n","| \u001b[0m 41      \u001b[0m | \u001b[0m 0.8011  \u001b[0m | \u001b[0m 182.6   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 418.0   \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 236.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6635\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.5993\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6390\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.5573\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6181\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5120\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5620\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.3873\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.4499\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.3499\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4578\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.3460\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4454\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.3191\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.3128\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.3157\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.2967\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.2217\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.2829\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.2297\n","| \u001b[0m 42      \u001b[0m | \u001b[0m 0.7758  \u001b[0m | \u001b[0m 224.8   \u001b[0m | \u001b[0m 0.2271  \u001b[0m | \u001b[0m 0.001073\u001b[0m | \u001b[0m 315.8   \u001b[0m | \u001b[0m 215.5   \u001b[0m | \u001b[0m 547.8   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6451\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.5279\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6058\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.4524\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5288\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.4688\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5119\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.4467\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5329\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.3178\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5430\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.2327\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.2400\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.2950\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.2450\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.2462\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.1829\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.2759\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.2673\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.1616\n","| \u001b[0m 43      \u001b[0m | \u001b[0m 0.7423  \u001b[0m | \u001b[0m 209.7   \u001b[0m | \u001b[0m 0.02129 \u001b[0m | \u001b[0m 0.002636\u001b[0m | \u001b[0m 205.1   \u001b[0m | \u001b[0m 538.2   \u001b[0m | \u001b[0m 258.5   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6457\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.1542\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.1152\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.0693\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.0447\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.9757\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.9719\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0489\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.7828\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.8460\n","| \u001b[0m 44      \u001b[0m | \u001b[0m 0.7133  \u001b[0m | \u001b[0m 379.5   \u001b[0m | \u001b[0m 0.1282  \u001b[0m | \u001b[0m 0.007869\u001b[0m | \u001b[0m 528.4   \u001b[0m | \u001b[0m 218.6   \u001b[0m | \u001b[0m 410.7   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6623\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.0986\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.1106\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.0331\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.0292\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.0790\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.9977\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0446\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.0537\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.0228\n","| \u001b[0m 45      \u001b[0m | \u001b[0m 0.775   \u001b[0m | \u001b[0m 462.9   \u001b[0m | \u001b[0m 0.4881  \u001b[0m | \u001b[0m 0.004559\u001b[0m | \u001b[0m 263.5   \u001b[0m | \u001b[0m 572.2   \u001b[0m | \u001b[0m 370.1   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6933\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7601\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7178\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6679\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6382\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5943\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5447\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5243\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5081\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4867\n","| \u001b[95m 46      \u001b[0m | \u001b[95m 0.8141  \u001b[0m | \u001b[95m 512.0   \u001b[0m | \u001b[95m 0.4539  \u001b[0m | \u001b[95m 0.0001  \u001b[0m | \u001b[95m 583.6   \u001b[0m | \u001b[95m 510.1   \u001b[0m | \u001b[95m 298.4   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6319\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6294\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5575\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.4890\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.4214\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.3429\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.2436\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.2861\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.2002\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.2878\n","| \u001b[0m 47      \u001b[0m | \u001b[0m 0.741   \u001b[0m | \u001b[0m 507.4   \u001b[0m | \u001b[0m 0.08297 \u001b[0m | \u001b[0m 0.000483\u001b[0m | \u001b[0m 152.5   \u001b[0m | \u001b[0m 512.1   \u001b[0m | \u001b[0m 335.5   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6870\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6929\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6783\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6072\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6867\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.7503\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6304\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.7084\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6521\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5185\n","| \u001b[0m 48      \u001b[0m | \u001b[0m 0.7762  \u001b[0m | \u001b[0m 423.2   \u001b[0m | \u001b[0m 0.1142  \u001b[0m | \u001b[0m 0.00597 \u001b[0m | \u001b[0m 433.6   \u001b[0m | \u001b[0m 336.2   \u001b[0m | \u001b[0m 254.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6888\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.0199\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.9721\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.1540\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.0023\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.0759\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.0495\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.1697\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.0576\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.1417\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.9658\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.0639\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.0378\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.9170\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.5185\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.0878\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.2734\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.3222\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2690\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.1230\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.2775\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.3191\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.4504\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.4000\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4906\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.3226\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.5421\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5761\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.5544\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.6034\n","| \u001b[0m 49      \u001b[0m | \u001b[0m 0.2713  \u001b[0m | \u001b[0m 147.7   \u001b[0m | \u001b[0m 0.3578  \u001b[0m | \u001b[0m 0.007107\u001b[0m | \u001b[0m 479.3   \u001b[0m | \u001b[0m 337.2   \u001b[0m | \u001b[0m 199.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6541\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6519\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5601\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.4825\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.4785\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5175\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.3362\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.2914\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.2189\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.1575\n","| \u001b[0m 50      \u001b[0m | \u001b[0m 0.7757  \u001b[0m | \u001b[0m 508.9   \u001b[0m | \u001b[0m 0.1427  \u001b[0m | \u001b[0m 0.001009\u001b[0m | \u001b[0m 452.3   \u001b[0m | \u001b[0m 542.0   \u001b[0m | \u001b[0m 541.6   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6169\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6500\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5917\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5552\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5245\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4942\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4631\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4324\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4003\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.3676\n","| \u001b[0m 51      \u001b[0m | \u001b[0m 0.8126  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 284.0   \u001b[0m | \u001b[0m 55.63   \u001b[0m | \u001b[0m 346.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6512\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6357\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5708\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5113\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.4478\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.3559\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.2705\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.2602\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.1638\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.1352\n","| \u001b[0m 52      \u001b[0m | \u001b[0m 0.784   \u001b[0m | \u001b[0m 471.2   \u001b[0m | \u001b[0m 0.2881  \u001b[0m | \u001b[0m 0.000450\u001b[0m | \u001b[0m 72.21   \u001b[0m | \u001b[0m 562.9   \u001b[0m | \u001b[0m 574.9   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6420\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.7011\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.8574\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.6593\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7913\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.6320\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7349\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.7302\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6891\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5733\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.7020\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.7167\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.8667\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.6692\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.8794\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5855\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6027\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.6684\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6189\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.6370\n","| \u001b[0m 53      \u001b[0m | \u001b[0m 0.7683  \u001b[0m | \u001b[0m 254.9   \u001b[0m | \u001b[0m 0.2385  \u001b[0m | \u001b[0m 0.006614\u001b[0m | \u001b[0m 524.3   \u001b[0m | \u001b[0m 126.7   \u001b[0m | \u001b[0m 542.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6252\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.5726\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.5986\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.5132\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5331\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.4790\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.4906\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.4474\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.4539\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.4131\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4154\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.3794\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.3742\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.3418\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.3286\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.3032\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.2810\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.2630\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.2377\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.2211\n","| \u001b[0m 54      \u001b[0m | \u001b[0m 0.7993  \u001b[0m | \u001b[0m 265.7   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 367.1   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6328\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6274\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5726\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5417\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5150\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4896\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4637\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4384\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4122\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.3853\n","| \u001b[0m 55      \u001b[0m | \u001b[0m 0.8129  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 332.8   \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6453\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.7733\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7124\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6588\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6402\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6487\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5982\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5542\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5610\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5221\n","| \u001b[0m 56      \u001b[0m | \u001b[0m 0.8131  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 320.9   \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.7002\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6500\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5981\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5687\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5455\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5253\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5067\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4877\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4686\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4488\n","| \u001b[0m 57      \u001b[0m | \u001b[0m 0.8113  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 357.1   \u001b[0m | \u001b[0m 141.9   \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.7413\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.7114\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 0.7427\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 0.5892\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 0.5320\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 0.3249\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.4778\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.6138\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.5635\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 0.5038\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 0.4905\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 0.2808\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.4085\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5691\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.4782\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 0.4391\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 0.4354\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 0.2444\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.3458\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5082\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.4147\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 0.3618\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 0.3615\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 0.1991\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.2763\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.4224\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.3498\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 0.2573\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 0.2709\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 0.1532\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.2124\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.3265\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.2762\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 0.1542\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 0.1679\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 0.1136\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.1544\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.2166\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.1548\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.0856\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 0.0921\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.0692\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.0818\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.1144\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.0759\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.0610\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.0404\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.0325\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.0362\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.0359\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.0462\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.0379\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 0.0443\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.1186\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.0820\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.0506\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.0496\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.0518\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.0217\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.0174\n","| \u001b[0m 58      \u001b[0m | \u001b[0m 0.779   \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 243.6   \u001b[0m | \u001b[0m 406.7   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6654\n","Train Epoch: 00 -- Batch: 500 -- Loss: 0.5027\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.5790\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.4449\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.4878\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.3973\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.3957\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.3679\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.3851\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.2917\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.3675\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.4306\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.3338\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.1732\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.2406\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.3642\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.2160\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.2172\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.1162\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.0843\n","| \u001b[0m 59      \u001b[0m | \u001b[0m 0.7669  \u001b[0m | \u001b[0m 241.7   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.002623\u001b[0m | \u001b[0m 600.0   \u001b[0m | \u001b[0m 410.0   \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6603\n","Train Epoch: 01 -- Batch: 000 -- Loss: 0.6327\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.5768\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5232\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5907\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.4909\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4015\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.2904\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.2532\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4274\n","| \u001b[0m 60      \u001b[0m | \u001b[0m 0.7767  \u001b[0m | \u001b[0m 429.8   \u001b[0m | \u001b[0m 0.06288 \u001b[0m | \u001b[0m 0.001388\u001b[0m | \u001b[0m 438.5   \u001b[0m | \u001b[0m 328.4   \u001b[0m | \u001b[0m 253.6   \u001b[0m |\n","=================================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9i5x6k3NeIBL","executionInfo":{"elapsed":37428836,"status":"ok","timestamp":1617231370551,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"55cd127c-4e56-4875-8f06-ae686d605410"},"source":["bayesian_optimizer.max"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'params': {'batch_size': 512.0,\n","  'dropout_p': 0.45390976039702363,\n","  'lr': 0.0001,\n","  'num_filters_1': 583.5689343238934,\n","  'num_filters_2': 510.1024441228828,\n","  'num_filters_3': 298.41850117229467},\n"," 'target': 0.8141}"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"DG_EFJRZuKRA"},"source":["**Train my own word2vec embeddings**"]},{"cell_type":"code","metadata":{"id":"x_0EXjSAuNWT"},"source":["posts = train_df_balanced['preprocessed_selftext'].values\n","word2vec_corpus = []\n","for i in range(0, len(posts)):\n","  word2vec_corpus.append(posts[i].split(' '))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"33KxqaQSuSAr","executionInfo":{"status":"ok","timestamp":1621599696811,"user_tz":-60,"elapsed":1622,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"c0d30320-6ef3-4bdf-de63-c320096ab0bd"},"source":["len(word2vec_corpus)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["170000"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"zm6AgfR2uZm_"},"source":["model = gensim.models.Word2Vec(sentences=word2vec_corpus, size=300, window=5, min_count=1, workers=4, iter=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWPM70N3b6JG"},"source":["**LSTM**"]},{"cell_type":"code","metadata":{"id":"_lmNJr0lb5Ii"},"source":["class LSTM(nn.Module):\n","  def __init__(self, num_words, embedding_size, hidden_dim, num_lstm_layers, dropout_p):\n","    super().__init__()\n","\n","    self.num_words = num_words\n","    self.embedding_size = embedding_size\n","    self.hidden_dim = hidden_dim\n","    self.num_lstm_layers = num_lstm_layers\n","\n","    self.dropout = nn.Dropout(dropout_p)\n","\n","    self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n","    self.embedding.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n","    self.embedding.weight.requires_grad = False\n","\n","    self.lstm = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_dim, num_layers=self.num_lstm_layers, batch_first=True)\n","    self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=5)\n","    #self.fc2 = nn.Linear(in_features=self.hidden_dim // 2, out_features=5)\n","\n","  def forward(self, x):\n","    h = torch.zeros((self.num_lstm_layers, x.size(0), self.hidden_dim)).to(device)\n","    c = torch.zeros((self.num_lstm_layers, x.size(0), self.hidden_dim)).to(device)\n","\n","    #torch.nn.init.xavier_normal_(h)\n","    #torch.nn.init.xavier_normal_(c)\n","    \n","    x = x.int()\n","    out = self.embedding(x)\n","    print(out.size())\n","    out, (hidden, cell) = self.lstm(out, (h,c))\n","    out = self.dropout(out)\n","\n","    #out = torch.relu_(self.fc1(out[:,-1,:]))\n","    #out = self.dropout(out)\n","    #out = self.fc2(out)\n","\n","    out = self.fc1(out[:,-1,:])\n","\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vewLQ4zphfBy","executionInfo":{"elapsed":5855,"status":"error","timestamp":1621334913796,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"b5dc3e1e-d934-4011-a43b-a96851cc3df1"},"source":["classifier = LSTM(num_words=len(vocab), embedding_size=300, hidden_dim=75, num_lstm_layers=2, dropout_p=0).to(device)\n","optimizer = optim.Adam(classifier.parameters(), lr=0.003)\n","# Train-test loop\n","for epoch in range(10):\n","  train_model(classifier, train_dataloader, optimizer, epoch)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([300, 400, 300])\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6108\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n","torch.Size([300, 400, 300])\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-82055b572813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train-test loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-40-ec5cfdd47b2b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, epoch, log_interval)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# We need to send our batch to the device we are using. If this is not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# it will default to using the CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"K4oM48K3yDe4"},"source":["y_pred = get_model_outputs(classifier, test_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVrZ7sNXyFpq","executionInfo":{"elapsed":2506,"status":"ok","timestamp":1621333706497,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"2e5092b0-ff19-47f2-ac7b-eb2afb8295dd"},"source":["print(classification_report(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.93      0.88      0.90      4000\n","           1       0.67      0.80      0.73      4000\n","           2       0.81      0.81      0.81      4000\n","           3       0.77      0.74      0.76      4000\n","           4       0.83      0.76      0.79      4000\n","\n","    accuracy                           0.80     20000\n","   macro avg       0.80      0.80      0.80     20000\n","weighted avg       0.80      0.80      0.80     20000\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gVe3oKeAEEnO"},"source":["def bayesian_train_model(hidden_dim, dropout_p, batch_size, lr):\n","  classifier = LSTM(num_words=len(vocab), embedding_size=300, hidden_dim=int(hidden_dim), num_lstm_layers=2, dropout_p=dropout_p).to(device)\n","  optimizer = optim.Adam(classifier.parameters(), lr=lr)\n","\n","  train = DepressionDataset(x_train_numeric, y_train)\n","  train_dataloader = DataLoader(train, batch_size=int(batch_size))\n","\n","  # Train-test loop\n","  for epoch in range(10):\n","    train_model(classifier, train_dataloader, optimizer, epoch)\n","  \n","  test = DepressionDataset(x_test_numeric, y_test)\n","  test_dataloader = DataLoader(test, batch_size=int(batch_size))\n","\n","  y_pred = get_model_outputs(classifier, test_dataloader)\n","\n","  return accuracy_score(y_test, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pctASr0Est-"},"source":["bounds = {\n","    'hidden_dim':(20, 150),\n","    'dropout_p': (0, 0.3),\n","    'batch_size':(64, 512),\n","    'lr':(0.0001, 0.01),\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_tFip_omE-f8","executionInfo":{"elapsed":29856974,"status":"ok","timestamp":1617942096129,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"bd3857e4-0baa-4827-a2d6-c6f6202fd94d"},"source":["bayesian_optimizer = BayesianOptimization(\n","    f=bayesian_train_model,\n","    pbounds=bounds,\n","    random_state=1,\n",")\n","bayesian_optimizer.maximize(init_points=10, n_iter=50)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["|   iter    |  target   | batch_... | dropout_p | hidden... |    lr     |\n","-------------------------------------------------------------------------\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6228\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6117\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6169\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6105\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6167\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6135\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6076\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6149\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6153\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6120\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.5294\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.5426\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6113\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6100\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6118\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6063\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6110\n","| \u001b[0m 1       \u001b[0m | \u001b[0m 0.2003  \u001b[0m | \u001b[0m 250.8   \u001b[0m | \u001b[0m 0.2161  \u001b[0m | \u001b[0m 20.01   \u001b[0m | \u001b[0m 0.003093\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6072\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6086\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6133\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6135\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.5780\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.5878\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6176\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4943\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.2128\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.0549\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.7978\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.7152\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6169\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6100\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.6120\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5982\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.4465\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.4696\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5807\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4167\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.5014\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5280\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.3690\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.4644\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5292\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.3495\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.4397\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4913\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.3508\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.4088\n","| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8018  \u001b[0m | \u001b[95m 129.7   \u001b[0m | \u001b[95m 0.0277  \u001b[0m | \u001b[95m 44.21   \u001b[0m | \u001b[95m 0.003521\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6207\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.5495\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5699\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.3164\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.9026\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5735\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5864\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5134\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5519\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5195\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5896\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.4929\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5458\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4899\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5047\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4462\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5317\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4682\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5049\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.4864\n","| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8089  \u001b[0m | \u001b[95m 241.8   \u001b[0m | \u001b[95m 0.1616  \u001b[0m | \u001b[95m 74.5    \u001b[0m | \u001b[95m 0.006884\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6080\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6097\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6119\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6096\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6096\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6112\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.2513\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.9728\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.9121\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.7437\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.7504\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7912\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.8017\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.6587\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.7062\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5563\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.0989\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2239\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.9514\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.0614\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0044\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.8781\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.9050\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.8849\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.7460\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.0435\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.8709\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.8064\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.8865\n","| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7376  \u001b[0m | \u001b[0m 155.6   \u001b[0m | \u001b[0m 0.2634  \u001b[0m | \u001b[0m 23.56   \u001b[0m | \u001b[0m 0.006738\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6275\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6123\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6125\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.4924\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6112\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4964\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6859\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.3578\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.2422\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.9844\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.8630\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.9126\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.8022\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.7747\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6107\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.6967\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5774\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.7275\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5841\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.7096\n","| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7983  \u001b[0m | \u001b[0m 251.0   \u001b[0m | \u001b[0m 0.1676  \u001b[0m | \u001b[0m 38.25   \u001b[0m | \u001b[0m 0.002061\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6227\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6034\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5862\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5392\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5057\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.3902\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7084\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6423\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6171\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6105\n","| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7989  \u001b[0m | \u001b[0m 422.7   \u001b[0m | \u001b[0m 0.2905  \u001b[0m | \u001b[0m 60.75   \u001b[0m | \u001b[0m 0.006954\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6113\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6096\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6097\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5055\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5348\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5500\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5291\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4984\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.5756\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5015\n","| \u001b[0m 7       \u001b[0m | \u001b[0m 0.2853  \u001b[0m | \u001b[0m 456.6   \u001b[0m | \u001b[0m 0.2684  \u001b[0m | \u001b[0m 31.06   \u001b[0m | \u001b[0m 0.000486\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6195\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6100\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6119\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.4407\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6093\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6128\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.3429\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.5008\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4549\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.3595\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.3658\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.1997\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.0582\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.8873\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.8703\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.8719\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.7505\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7495\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.7723\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.9517\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.8693\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.7789\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.7532\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.7436\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.7667\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.7227\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6862\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.7645\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.6757\n","| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7307  \u001b[0m | \u001b[0m 140.1   \u001b[0m | \u001b[0m 0.2634  \u001b[0m | \u001b[0m 32.79   \u001b[0m | \u001b[0m 0.004269\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6103\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6188\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.0870\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6973\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5561\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5303\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4874\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4465\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4310\n","| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8005  \u001b[0m | \u001b[0m 493.1   \u001b[0m | \u001b[0m 0.1599  \u001b[0m | \u001b[0m 109.9   \u001b[0m | \u001b[0m 0.003224\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6161\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6111\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6114\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5017\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4704\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4818\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5184\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.5301\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.5348\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4797\n","| \u001b[0m 10      \u001b[0m | \u001b[0m 0.4576  \u001b[0m | \u001b[0m 371.6   \u001b[0m | \u001b[0m 0.2504  \u001b[0m | \u001b[0m 22.38   \u001b[0m | \u001b[0m 0.007526\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6064\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6069\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6160\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6052\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6101\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.2856\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.1402\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.7342\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7327\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6539\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6255\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5970\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6323\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5860\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6646\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.6339\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6180\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.5527\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5572\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5890\n","| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8005  \u001b[0m | \u001b[0m 243.9   \u001b[0m | \u001b[0m 0.286   \u001b[0m | \u001b[0m 72.97   \u001b[0m | \u001b[0m 0.008844\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6070\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6085\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6103\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6095\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6104\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6077\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6086\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6098\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6093\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6099\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.6065\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5109\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.5073\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.4903\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.5889\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.4972\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.4350\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5115\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5810\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.5718\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.5979\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.5709\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.5822\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5803\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.5316\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.4254\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.4407\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.4663\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.4493\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4907\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.4923\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.3473\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.4611\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.4666\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.3329\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5006\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4301\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.3724\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.3714\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.4263\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 1.4741\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4665\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.5367\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.3185\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.3801\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 1.5299\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 1.3250\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4478\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4156\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.4494\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.3390\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.4505\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 1.4491\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4823\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.5098\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.5131\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.5047\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 1.5671\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 1.4061\n","| \u001b[0m 12      \u001b[0m | \u001b[0m 0.4365  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6096\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4569\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5623\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5611\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.5469\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.5435\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5453\n","| \u001b[0m 13      \u001b[0m | \u001b[0m 0.3317  \u001b[0m | \u001b[0m 383.7   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6097\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6096\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6097\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4189\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5320\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.4847\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5174\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4816\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5070\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.4402\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4878\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.3921\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4950\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.3848\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4838\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4100\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4822\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4076\n","| \u001b[0m 14      \u001b[0m | \u001b[0m 0.3261  \u001b[0m | \u001b[0m 170.7   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6523\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6118\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6100\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6089\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6092\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6069\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6090\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6099\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6091\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6091\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.6065\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6090\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6099\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6091\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.6091\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.6057\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6090\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6099\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.6091\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.6090\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.6047\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6090\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.6098\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.6091\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.6090\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.6036\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6090\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.6098\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.6091\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.6089\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.6035\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6098\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6091\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.6098\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.6091\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.6088\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 1.6035\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6097\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6091\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.6098\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.6090\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 1.6087\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 1.5939\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6096\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6091\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.6097\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.6090\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.6086\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 1.5909\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6095\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6091\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.6097\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.6090\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 1.6087\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 1.5899\n","| \u001b[0m 15      \u001b[0m | \u001b[0m 0.2008  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6127\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6095\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6102\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6076\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.5193\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4944\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4846\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4804\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.4140\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4110\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4092\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4104\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4422\n","| \u001b[0m 16      \u001b[0m | \u001b[0m 0.3073  \u001b[0m | \u001b[0m 285.5   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6107\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6101\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.0176\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6501\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5508\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5133\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4957\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4612\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4460\n","| \u001b[0m 17      \u001b[0m | \u001b[0m 0.7823  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6093\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5110\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.5399\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5001\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.5218\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5162\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.3573\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.2975\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.1969\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.9568\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5775\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5963\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4859\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5354\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4539\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4874\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4175\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4631\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.3918\n","| \u001b[95m 18      \u001b[0m | \u001b[95m 0.8095  \u001b[0m | \u001b[95m 173.8   \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 77.52   \u001b[0m | \u001b[95m 0.000847\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6080\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6083\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6084\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4587\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6087\n","| \u001b[0m 19      \u001b[0m | \u001b[0m 0.324   \u001b[0m | \u001b[0m 452.0   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6197\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6084\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.5018\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.5595\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4937\n","| \u001b[0m 20      \u001b[0m | \u001b[0m 0.3417  \u001b[0m | \u001b[0m 346.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 84.12   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6066\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6134\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6099\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6108\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.3246\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.4503\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.3030\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.2564\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 0.9561\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.1044\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 0.5942\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.8544\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.7852\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.8334\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 0.8951\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 0.6141\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 0.5382\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7235\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.7558\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.8440\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 0.8881\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 0.6462\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 0.4936\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.8299\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.8451\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.9643\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 0.8186\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 0.6694\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 0.5411\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.9499\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.7814\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.9349\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 0.8336\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 0.5721\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 0.9431\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.0063\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.8739\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.0428\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.8986\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 0.9654\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.6769\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.8076\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.7727\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.9979\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.8125\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.9026\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.5805\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.7772\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.8201\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.2634\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.3351\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.4123\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 1.4350\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4363\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.2726\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.3506\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.4842\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 1.3879\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 1.3742\n","| \u001b[0m 21      \u001b[0m | \u001b[0m 0.4125  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 88.12   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6132\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6107\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6076\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.9882\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.7601\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6521\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6151\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6061\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6490\n","| \u001b[0m 22      \u001b[0m | \u001b[0m 0.7777  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 53.76   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6177\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6091\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6092\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6092\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6092\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6093\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6070\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6056\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6046\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6038\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4485\n","| \u001b[0m 23      \u001b[0m | \u001b[0m 0.2898  \u001b[0m | \u001b[0m 207.7   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 46.62   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6156\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6004\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.5765\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5465\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.3085\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.5694\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6129\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5625\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.5289\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6205\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5402\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.5245\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5870\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5234\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.4699\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5911\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5079\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.4449\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5557\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4998\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.4309\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5484\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4687\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.4531\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5449\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4350\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.4124\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5096\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.3735\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.4318\n","| \u001b[0m 24      \u001b[0m | \u001b[0m 0.7763  \u001b[0m | \u001b[0m 121.9   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 108.2   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6152\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6097\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6095\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6095\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6081\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6079\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6093\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6078\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6092\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6076\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6090\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6073\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6086\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6070\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6066\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6070\n","| \u001b[0m 25      \u001b[0m | \u001b[0m 0.2014  \u001b[0m | \u001b[0m 303.4   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 37.76   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6167\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6040\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5507\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6000\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.4140\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.7141\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6593\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5582\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5797\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5019\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5495\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.4741\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5188\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4656\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5015\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4464\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4710\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4224\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4250\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.3987\n","| \u001b[0m 26      \u001b[0m | \u001b[0m 0.792   \u001b[0m | \u001b[0m 228.6   \u001b[0m | \u001b[0m 0.0813  \u001b[0m | \u001b[0m 149.2   \u001b[0m | \u001b[0m 0.001169\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6066\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6110\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4784\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.5165\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.5080\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5065\n","| \u001b[0m 27      \u001b[0m | \u001b[0m 0.3476  \u001b[0m | \u001b[0m 412.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 101.8   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6025\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6085\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6125\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.4070\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.1153\n","Train Epoch: 02 -- Batch: 500 -- Loss: nan\n","Train Epoch: 03 -- Batch: 000 -- Loss: nan\n","Train Epoch: 03 -- Batch: 500 -- Loss: nan\n","Train Epoch: 04 -- Batch: 000 -- Loss: nan\n","Train Epoch: 04 -- Batch: 500 -- Loss: nan\n","Train Epoch: 05 -- Batch: 000 -- Loss: nan\n","Train Epoch: 05 -- Batch: 500 -- Loss: nan\n","Train Epoch: 06 -- Batch: 000 -- Loss: nan\n","Train Epoch: 06 -- Batch: 500 -- Loss: nan\n","Train Epoch: 07 -- Batch: 000 -- Loss: nan\n","Train Epoch: 07 -- Batch: 500 -- Loss: nan\n","Train Epoch: 08 -- Batch: 000 -- Loss: nan\n","Train Epoch: 08 -- Batch: 500 -- Loss: nan\n","Train Epoch: 09 -- Batch: 000 -- Loss: nan\n","Train Epoch: 09 -- Batch: 500 -- Loss: nan\n","| \u001b[0m 28      \u001b[0m | \u001b[0m 0.2     \u001b[0m | \u001b[0m 207.1   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 112.9   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6145\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6097\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6099\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6083\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6114\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6100\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6090\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6081\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6106\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.3828\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.3563\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6091\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6106\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.3142\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.4100\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.4959\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5105\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.3552\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.3726\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.4081\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4479\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.3746\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.3916\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.5260\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5168\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4222\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.3903\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.3992\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4773\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.4030\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.3632\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.4251\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.5315\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.5147\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.4207\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.4146\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4710\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4050\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.4304\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.4180\n","| \u001b[0m 29      \u001b[0m | \u001b[0m 0.3189  \u001b[0m | \u001b[0m 113.7   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6084\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6100\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6096\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6059\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6066\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.0746\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.9944\n","| \u001b[0m 30      \u001b[0m | \u001b[0m 0.4528  \u001b[0m | \u001b[0m 511.0   \u001b[0m | \u001b[0m 0.1078  \u001b[0m | \u001b[0m 21.09   \u001b[0m | \u001b[0m 0.005986\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6109\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5056\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.3449\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6776\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6181\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5669\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5601\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5186\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5245\n","| \u001b[0m 31      \u001b[0m | \u001b[0m 0.8002  \u001b[0m | \u001b[0m 464.7   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 78.85   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6049\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6083\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6112\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6084\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6100\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6081\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6086\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6101\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6083\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6086\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6100\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6081\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.5999\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6140\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6129\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.5979\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6098\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.5951\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.4094\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.4756\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4854\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4245\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.4660\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5058\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4201\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.4892\n","| \u001b[0m 32      \u001b[0m | \u001b[0m 0.3246  \u001b[0m | \u001b[0m 140.1   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 77.01   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6019\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6128\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6109\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6083\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6115\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6118\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6106\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6082\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.5564\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.4632\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.4173\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5607\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5521\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.5409\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.4157\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4880\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.5391\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.5077\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.4373\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5179\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.5517\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.6127\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.6082\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6101\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.5821\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.5698\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.5577\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6233\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.5508\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.5356\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.4843\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6083\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.5014\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.4621\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.4774\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5126\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.5248\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.5010\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.4452\n","| \u001b[0m 33      \u001b[0m | \u001b[0m 0.2721  \u001b[0m | \u001b[0m 107.6   \u001b[0m | \u001b[0m 0.02055 \u001b[0m | \u001b[0m 21.49   \u001b[0m | \u001b[0m 0.001098\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6056\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6080\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6080\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6072\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5902\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5686\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6086\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.4738\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4556\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.5620\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.5027\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.4753\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4525\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4851\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4522\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.5500\n","| \u001b[0m 34      \u001b[0m | \u001b[0m 0.2367  \u001b[0m | \u001b[0m 337.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6134\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6098\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6095\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6078\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6097\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6078\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6092\n","| \u001b[0m 35      \u001b[0m | \u001b[0m 0.3201  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 90.64   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6213\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6106\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5289\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4942\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5511\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5160\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5384\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6112\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6104\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6088\n","| \u001b[0m 36      \u001b[0m | \u001b[0m 0.2001  \u001b[0m | \u001b[0m 411.6   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6188\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6090\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6077\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6067\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6063\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.5057\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4808\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.4576\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4622\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4590\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4672\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.4729\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4393\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4984\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5278\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4810\n","| \u001b[0m 37      \u001b[0m | \u001b[0m 0.3224  \u001b[0m | \u001b[0m 284.9   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 100.7   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6166\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6108\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6110\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5889\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.8570\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6964\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6143\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5808\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5523\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5278\n","| \u001b[0m 38      \u001b[0m | \u001b[0m 0.7996  \u001b[0m | \u001b[0m 384.4   \u001b[0m | \u001b[0m 0.2392  \u001b[0m | \u001b[0m 62.35   \u001b[0m | \u001b[0m 0.008065\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6113\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6100\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.5840\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6090\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6150\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6107\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.5778\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.7212\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5001\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4219\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.4800\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.4303\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4502\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.4784\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.3602\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.3171\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.3064\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4027\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.4053\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.2920\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.2327\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.3435\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.2984\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.1279\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.1761\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.1246\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.0855\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.9429\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0520\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.2619\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.0484\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.9468\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.5969\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.5778\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.5359\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.5586\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5431\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6453\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.6645\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.4835\n","| \u001b[0m 39      \u001b[0m | \u001b[0m 0.2747  \u001b[0m | \u001b[0m 91.03   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 114.4   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6161\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6169\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6138\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6055\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6124\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6092\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.4436\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.5212\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.4034\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.2288\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.0828\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 0.9581\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.8008\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.8912\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.9044\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 0.7758\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6672\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.7505\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.6630\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 0.7003\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5773\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.6664\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.6472\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 0.6707\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5715\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5965\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.5583\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.6682\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5544\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5184\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.5376\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.6270\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5089\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.5395\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.5092\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.6322\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5430\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5070\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.4849\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.6055\n","| \u001b[0m 40      \u001b[0m | \u001b[0m 0.798   \u001b[0m | \u001b[0m 92.28   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 61.48   \u001b[0m | \u001b[0m 0.001951\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6134\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.2526\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6576\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6205\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5474\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5268\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4855\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4742\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4547\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4202\n","| \u001b[0m 41      \u001b[0m | \u001b[0m 0.8044  \u001b[0m | \u001b[0m 461.8   \u001b[0m | \u001b[0m 0.2101  \u001b[0m | \u001b[0m 110.8   \u001b[0m | \u001b[0m 0.005451\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6131\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.1118\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6805\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5982\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5247\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4977\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4585\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4269\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.3941\n","| \u001b[0m 42      \u001b[0m | \u001b[0m 0.7994  \u001b[0m | \u001b[0m 487.2   \u001b[0m | \u001b[0m 0.1053  \u001b[0m | \u001b[0m 148.6   \u001b[0m | \u001b[0m 0.004542\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6129\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6107\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6115\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6084\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6110\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6067\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.4331\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4635\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4036\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4517\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.4042\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4798\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4328\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4801\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.4109\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4788\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.3958\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4627\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.3961\n","| \u001b[0m 43      \u001b[0m | \u001b[0m 0.3157  \u001b[0m | \u001b[0m 250.8   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 124.5   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6121\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6036\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6122\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5715\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.3947\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.6874\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6705\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.5663\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.4636\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5966\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5473\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.4247\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5933\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5188\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.4279\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5568\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5300\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.4133\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5202\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5157\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.4069\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5165\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5038\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.4019\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5329\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4983\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.4589\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5707\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5164\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.4120\n","| \u001b[0m 44      \u001b[0m | \u001b[0m 0.7952  \u001b[0m | \u001b[0m 150.7   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 117.9   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6121\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6078\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6075\n","| \u001b[0m 45      \u001b[0m | \u001b[0m 0.2008  \u001b[0m | \u001b[0m 487.1   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 54.21   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6135\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6066\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6127\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6100\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6657\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6047\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6066\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.5999\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6158\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6182\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6154\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.6107\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6133\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6013\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6155\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.3342\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.0814\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 0.9179\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.1196\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.9145\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.7844\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 0.8522\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 0.7271\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 0.6084\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7698\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.7914\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.7790\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 0.8026\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 0.6457\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 0.4878\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.7450\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.7141\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.7511\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 0.7551\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 0.6134\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 0.5464\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7326\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.8399\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.7918\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.7259\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 0.5181\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.5126\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.7115\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.6902\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.7415\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.7185\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.5131\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.4199\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6638\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.7058\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.7543\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.7162\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 0.5675\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.4165\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6930\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.6683\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.8076\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.6949\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.5622\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.3845\n","| \u001b[0m 46      \u001b[0m | \u001b[0m 0.7973  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 54.53   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6126\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6127\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6109\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4687\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7356\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6594\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6073\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5766\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5446\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5196\n","| \u001b[0m 47      \u001b[0m | \u001b[0m 0.7966  \u001b[0m | \u001b[0m 442.0   \u001b[0m | \u001b[0m 0.04081 \u001b[0m | \u001b[0m 82.57   \u001b[0m | \u001b[0m 0.009859\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6057\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6041\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.5009\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.4812\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.4831\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6084\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6116\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6018\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.5080\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5320\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5279\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.5164\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5288\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4340\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.4517\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4359\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.4913\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.4288\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.3925\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.3257\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.4187\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.3051\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.3272\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.3938\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.3422\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.2718\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.3147\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.1587\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.9408\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.9415\n","| \u001b[0m 48      \u001b[0m | \u001b[0m 0.6276  \u001b[0m | \u001b[0m 167.0   \u001b[0m | \u001b[0m 0.1104  \u001b[0m | \u001b[0m 47.92   \u001b[0m | \u001b[0m 0.003102\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6169\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5498\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.2777\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.8587\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6119\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5294\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5130\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4997\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4678\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4587\n","| \u001b[0m 49      \u001b[0m | \u001b[0m 0.8037  \u001b[0m | \u001b[0m 352.7   \u001b[0m | \u001b[0m 0.01434 \u001b[0m | \u001b[0m 48.94   \u001b[0m | \u001b[0m 0.003692\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6049\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6102\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6098\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6087\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6080\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6080\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6059\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6007\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.3763\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4926\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4228\n","| \u001b[0m 50      \u001b[0m | \u001b[0m 0.3305  \u001b[0m | \u001b[0m 270.6   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 53.48   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6170\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6126\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6104\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6130\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6107\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6130\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6100\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6063\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5831\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.8710\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.8361\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.7002\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6126\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.6288\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5762\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.6263\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5319\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.6208\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5327\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5906\n","| \u001b[0m 51      \u001b[0m | \u001b[0m 0.7988  \u001b[0m | \u001b[0m 334.4   \u001b[0m | \u001b[0m 0.06438 \u001b[0m | \u001b[0m 20.36   \u001b[0m | \u001b[0m 0.00652 \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6114\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.3232\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.7930\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6782\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6060\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5950\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6216\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5538\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5257\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5168\n","| \u001b[0m 52      \u001b[0m | \u001b[0m 0.7915  \u001b[0m | \u001b[0m 501.2   \u001b[0m | \u001b[0m 0.1169  \u001b[0m | \u001b[0m 130.8   \u001b[0m | \u001b[0m 0.005269\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6101\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6038\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6110\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6083\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.3617\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.7455\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6527\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5421\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5748\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5368\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5606\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5212\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5421\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5051\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5484\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4837\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5231\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4548\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5132\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.4803\n","| \u001b[0m 53      \u001b[0m | \u001b[0m 0.8056  \u001b[0m | \u001b[0m 236.3   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 55.24   \u001b[0m | \u001b[0m 0.008679\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6149\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6128\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6137\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.5584\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5816\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.5326\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4989\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.4784\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4967\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.0819\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.8097\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.7431\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6017\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.6777\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6018\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.6150\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5706\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.5996\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5311\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5455\n","| \u001b[0m 54      \u001b[0m | \u001b[0m 0.8044  \u001b[0m | \u001b[0m 198.3   \u001b[0m | \u001b[0m 0.07978 \u001b[0m | \u001b[0m 78.82   \u001b[0m | \u001b[0m 0.000715\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6074\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.4849\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.2078\n","Train Epoch: 01 -- Batch: 500 -- Loss: 0.8972\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.8730\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.7876\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7784\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.7191\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7179\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6441\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.7188\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.6513\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7257\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5889\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6418\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.6501\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6755\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.6453\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.9321\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.7712\n","| \u001b[0m 55      \u001b[0m | \u001b[0m 0.7429  \u001b[0m | \u001b[0m 247.8   \u001b[0m | \u001b[0m 0.239   \u001b[0m | \u001b[0m 149.8   \u001b[0m | \u001b[0m 0.009596\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6121\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6076\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6095\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6155\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6874\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6126\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6114\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6272\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6206\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6174\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6131\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6162\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6138\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.3682\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.3739\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.9791\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.8492\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.8775\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.8165\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.7438\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.8391\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.8198\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.6482\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.8183\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.7335\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.6460\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.7472\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.7431\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.6515\n","| \u001b[0m 56      \u001b[0m | \u001b[0m 0.7541  \u001b[0m | \u001b[0m 132.7   \u001b[0m | \u001b[0m 0.239   \u001b[0m | \u001b[0m 128.3   \u001b[0m | \u001b[0m 0.008119\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6106\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.6985\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.5974\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5382\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5125\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5080\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4902\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4760\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4606\n","| \u001b[0m 57      \u001b[0m | \u001b[0m 0.7963  \u001b[0m | \u001b[0m 481.0   \u001b[0m | \u001b[0m 0.07713 \u001b[0m | \u001b[0m 128.4   \u001b[0m | \u001b[0m 0.005632\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6106\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6084\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6110\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.5187\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5084\n","| \u001b[0m 58      \u001b[0m | \u001b[0m 0.3574  \u001b[0m | \u001b[0m 473.9   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 97.57   \u001b[0m | \u001b[0m 0.0001  \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6056\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6108\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5771\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5344\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6149\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5256\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5046\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4991\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4920\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5065\n","| \u001b[0m 59      \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 449.6   \u001b[0m | \u001b[0m 0.2594  \u001b[0m | \u001b[0m 59.73   \u001b[0m | \u001b[0m 0.005901\u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5821\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.2656\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6658\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5745\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5465\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5067\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5035\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5623\n","| \u001b[0m 60      \u001b[0m | \u001b[0m 0.8067  \u001b[0m | \u001b[0m 368.0   \u001b[0m | \u001b[0m 0.2732  \u001b[0m | \u001b[0m 117.1   \u001b[0m | \u001b[0m 0.007199\u001b[0m |\n","=========================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H5n-KilLFBkC","executionInfo":{"elapsed":29849159,"status":"ok","timestamp":1617942096131,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"},"user_tz":-60},"outputId":"b7fe5463-c7f1-4901-a3ef-164fb34f2af0"},"source":["bayesian_optimizer.max"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'params': {'batch_size': 173.83987621924035,\n","  'dropout_p': 0.0,\n","  'hidden_dim': 77.5165200461393,\n","  'lr': 0.0008469775389387133},\n"," 'target': 0.8095}"]},"metadata":{"tags":[]},"execution_count":202}]},{"cell_type":"markdown","metadata":{"id":"qrBajGlUBsTw"},"source":["**CNN-LSTM**"]},{"cell_type":"code","metadata":{"id":"djCw2tBiBnq4"},"source":["class CNN_LSTM(nn.Module):\n","  def __init__(self, num_words, embedding_size, num_filters_1, num_filters_2, num_filters_3, hidden_dim, num_lstm_layers, dropout_p):\n","    super().__init__()\n","\n","    self.num_words = num_words\n","    self.embedding_size = embedding_size\n","    self.hidden_dim = hidden_dim\n","    self.num_lstm_layers = num_lstm_layers\n","\n","    self.num_filters_1 = num_filters_1\n","    self.num_filters_2 = num_filters_2\n","    self.num_filters_3 = num_filters_3\n","\n","    # kernel sizes\n","    self.kernel_1 = 2\n","    self.kernel_2 = 3\n","    self.kernel_3 = 4\n","\n","    self.dropout = nn.Dropout(dropout_p)\n","\n","    self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n","    self.embedding.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n","    self.embedding.weight.requires_grad = False\n","\n","    # Convolution layers definition\n","    #self.conv_1 = nn.Conv1d(self.embedding_size, self.num_filters_1, kernel_size=self.kernel_1) # bigrams\n","    self.conv_2 = nn.Conv1d(self.embedding_size, self.num_filters_2, kernel_size=self.kernel_2) # trigrams\n","    #self.conv_3 = nn.Conv1d(self.embedding_size, self.num_filters_3, kernel_size=self.kernel_3) # 4-grams\n","\n","    self.lstm = nn.LSTM(input_size=num_filters_2, hidden_size=self.hidden_dim, num_layers=self.num_lstm_layers, batch_first=True)\n","    self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=5)\n","    #self.fc2 = nn.Linear(in_features=self.hidden_dim // 2, out_features=5)\n","\n","  def forward(self, x):\n","    h = torch.zeros((self.num_lstm_layers, x.size(0), self.hidden_dim)).to(device)\n","    c = torch.zeros((self.num_lstm_layers, x.size(0), self.hidden_dim)).to(device)\n","\n","    #torch.nn.init.xavier_normal_(h)\n","    #torch.nn.init.xavier_normal_(c)\n","\n","    x = x.int()\n","    x = self.embedding(x)\n","    x = x.permute(0, 2, 1)\n","\n","    # Convolution layer 1 is applied\n","    #x1 = self.conv_1(x)\n","    #x1 = torch.relu(x1)\n","    #print(x1.size())\n","\n","    # Convolution layer 2 is applied\n","    x2 = self.conv_2(x)\n","    x2 = torch.relu((x2))\n","    #print(x2.size())\n","\n","    # Convolution layer 3 is applied\n","    #x3 = self.conv_3(x)\n","    #x3 = torch.relu(x3)\n","    #print(x3.size()[2])\n","\n","    #x1 = x1[ : , : , : x3.size()[2]]\n","    #x2 = x2[ : , : , : x3.size()[2]]\n","\n","    #print(x1.size())\n","    #print(x2.size())\n","    #print(x3.size())\n","\n","    # The output of each convolutional layer is concatenated into a unique vector\n","    #union = torch.cat((x1, x2, x3), 1)\n","    #union = union.reshape(union.size(0), -1)\n","\n","    out = x2.permute(0, 2, 1)\n","\n","    \n","    #x = x.int()\n","    #out = self.embedding(x)\n","    out, (hidden, cell) = self.lstm(out, (h,c))\n","    out = self.dropout(out)\n","\n","    #out = torch.relu_(self.fc1(out[:,-1,:]))\n","    #out = self.dropout(out)\n","    #out = self.fc2(out)\n","    \n","    out = self.fc1(out[:,-1,:])\n","\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205},"id":"eiQ6l6SuDDLR","executionInfo":{"status":"ok","timestamp":1621605105742,"user_tz":-60,"elapsed":429903,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"7f7f623c-955a-4137-82bd-c063056d7ed7"},"source":["classifier = CNN_LSTM(num_words=len(vocab), embedding_size=300, num_filters_1=50, num_filters_2=215, num_filters_3=50, hidden_dim=160, num_lstm_layers=2, dropout_p=0.02).to(device)\n","optimizer = optim.Adam(classifier.parameters(), lr=0.005)\n","# Train-test loop\n","for epoch in range(10):\n","  train_model(classifier, train_dataloader, optimizer, epoch)\n","\"\"\"\n","|  39       |  0.8096   |  416.5    |  0.02754  |  159.9    |  0.003682 |  214.3    |\n","|   iter    |  target   | batch_... | dropout_p | hidden... |    lr     | num_fi... |\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Epoch: 00 -- Batch: 000 -- Loss: 1.6106\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6097\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6097\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.8902\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6725\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6210\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6149\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5791\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5745\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n|  39       |  0.8096   |  416.5    |  0.02754  |  159.9    |  0.003682 |  214.3    |\\n|   iter    |  target   | batch_... | dropout_p | hidden... |    lr     | num_fi... |\\n'"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"7OLS0w-tIYje"},"source":["y_pred = get_model_outputs(classifier, test_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhrY_E9AIi4y","executionInfo":{"status":"ok","timestamp":1621605107891,"user_tz":-60,"elapsed":429743,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"047e205c-acfb-4a83-cc97-12f906592a52"},"source":["print(classification_report(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.89      0.92      0.90      4000\n","           1       0.70      0.76      0.73      4000\n","           2       0.78      0.84      0.81      4000\n","           3       0.81      0.67      0.73      4000\n","           4       0.79      0.77      0.78      4000\n","\n","    accuracy                           0.79     20000\n","   macro avg       0.79      0.79      0.79     20000\n","weighted avg       0.79      0.79      0.79     20000\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFEW1P1g9Cj5","executionInfo":{"status":"ok","timestamp":1621601044493,"user_tz":-60,"elapsed":660,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"6c568ca7-d3f3-4269-ae0b-5342ac029c82"},"source":["print(confusion_matrix(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[3716  100   55   58   71]\n"," [ 102 2993  246  390  269]\n"," [ 130  293 3263  153  161]\n"," [  94  388  174 3014  330]\n"," [ 100  334  153  283 3130]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2kaiFEheSH0v"},"source":["def bayesian_train_model(num_filters_2, hidden_dim, dropout_p, batch_size, lr, kernel_2):\n","  classifier = CNN_LSTM(num_words=len(vocab), embedding_size=300, num_filters_1=10, num_filters_2=int(num_filters_2), num_filters_3=10, hidden_dim=int(hidden_dim), dropout_p=dropout_p, num_lstm_layers=2, kernel_2=int(kernel_2)).to(device)\n","  optimizer = optim.Adam(classifier.parameters(), lr=lr)\n","\n","  train = DepressionDataset(x_train_numeric, y_train)\n","  train_dataloader = DataLoader(train, batch_size=int(batch_size))\n","\n","  # Train-test loop\n","  for epoch in range(10):\n","    train_model(classifier, train_dataloader, optimizer, epoch)\n","  \n","  test = DepressionDataset(x_test_numeric, y_test)\n","  test_dataloader = DataLoader(test, batch_size=int(batch_size))\n","\n","  y_pred = get_model_outputs(classifier, test_dataloader)\n","\n","  return accuracy_score(y_test, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-RkDTAwSX6D"},"source":["bounds = {\n","    'hidden_dim':(50, 200),\n","    'dropout_p': (0, 0.3),\n","    'batch_size':(64, 512),\n","    'lr':(0.0001, 0.01),\n","    'num_filters_2': (50, 300),\n","    'kernel_2': (2, 5)\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1X6YOHvsS4Id","executionInfo":{"status":"ok","timestamp":1621472611544,"user_tz":-60,"elapsed":34722212,"user":{"displayName":"Zimu Zeng","photoUrl":"https://lh4.googleusercontent.com/-cna7EXAdO8Y/AAAAAAAAAAI/AAAAAAAAC7A/mpXdR4qvIUQ/s64/photo.jpg","userId":"11200670789617191744"}},"outputId":"048f16ba-c8e4-4f1e-ca0c-48ea9c02caf1"},"source":["bayesian_optimizer = BayesianOptimization(\n","    f=bayesian_train_model,\n","    pbounds=bounds,\n","    random_state=1,\n",")\n","bayesian_optimizer.maximize(init_points=10, n_iter=50)\n","|  50       |  0.8079   |  310.3    |  0.2355   |  123.1    |  3.047    |  0.002983 |  104.0    |\n","|   iter    |  target   | batch_... | dropout_p | hidden... | kernel_2  |    lr     | num_fi... |"],"execution_count":null,"outputs":[{"output_type":"stream","text":["|   iter    |  target   | batch_... | dropout_p | hidden... | kernel_2  |    lr     | num_fi... |\n","-------------------------------------------------------------------------------------------------\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6108\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6112\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6106\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5075\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4977\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5954\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.3998\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4372\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.3330\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.2378\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.1091\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.0491\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.7913\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.7850\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6558\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.6591\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5556\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.6771\n","| \u001b[0m 1       \u001b[0m | \u001b[0m 0.793   \u001b[0m | \u001b[0m 250.8   \u001b[0m | \u001b[0m 0.2161  \u001b[0m | \u001b[0m 50.02   \u001b[0m | \u001b[0m 2.907   \u001b[0m | \u001b[0m 0.001553\u001b[0m | \u001b[0m 73.08   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6106\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6096\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6152\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6086\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.2879\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.2849\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.9516\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.8515\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7730\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.7971\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.7892\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7529\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.7445\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.7216\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6827\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.7270\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.7134\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.9014\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.6768\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.6923\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.7029\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.7201\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.6334\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6975\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.6611\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.5791\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6775\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.6570\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.6297\n","| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7903  \u001b[0m | \u001b[0m 147.4   \u001b[0m | \u001b[0m 0.1037  \u001b[0m | \u001b[0m 109.5   \u001b[0m | \u001b[0m 3.616   \u001b[0m | \u001b[0m 0.00425 \u001b[0m | \u001b[0m 221.3   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6164\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6090\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6104\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6077\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6106\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6099\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6110\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6095\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6078\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6091\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6090\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6106\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.6094\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6080\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6106\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.6093\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6080\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6105\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.4281\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.3733\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.2247\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.3095\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6047\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6100\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.6116\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6072\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.3648\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.4033\n","| \u001b[0m 3       \u001b[0m | \u001b[0m 0.432   \u001b[0m | \u001b[0m 155.6   \u001b[0m | \u001b[0m 0.2634  \u001b[0m | \u001b[0m 54.11   \u001b[0m | \u001b[0m 4.011   \u001b[0m | \u001b[0m 0.004231\u001b[0m | \u001b[0m 189.7   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6100\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6173\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6097\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6077\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6162\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6100\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.3804\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.1062\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.8974\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.8232\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.6716\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.7761\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7236\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6534\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.6846\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6850\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5570\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.6638\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6603\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5713\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.6692\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6923\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5858\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.6338\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6974\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4987\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.5904\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6546\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5198\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.5645\n","| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7868  \u001b[0m | \u001b[0m 126.9   \u001b[0m | \u001b[0m 0.05943 \u001b[0m | \u001b[0m 170.1   \u001b[0m | \u001b[0m 4.905   \u001b[0m | \u001b[0m 0.003203\u001b[0m | \u001b[0m 223.1   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6136\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6105\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5129\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4953\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5271\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.3566\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.1990\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.1770\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.9337\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.7107\n","| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7097  \u001b[0m | \u001b[0m 456.6   \u001b[0m | \u001b[0m 0.2684  \u001b[0m | \u001b[0m 62.76   \u001b[0m | \u001b[0m 2.117   \u001b[0m | \u001b[0m 0.001781\u001b[0m | \u001b[0m 269.5   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6116\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6104\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6098\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6100\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6077\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6115\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6098\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6100\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6077\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6192\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6143\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6133\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6078\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6188\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6114\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.4181\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5073\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4360\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.2667\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.3829\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4615\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.5073\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.2150\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.3593\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4152\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4468\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.2517\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.2939\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.5005\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.3931\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.2577\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.2535\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4183\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4307\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.2212\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.2720\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.3167\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.3560\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.2939\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.3049\n","| \u001b[0m 6       \u001b[0m | \u001b[0m 0.4259  \u001b[0m | \u001b[0m 108.1   \u001b[0m | \u001b[0m 0.1263  \u001b[0m | \u001b[0m 193.7   \u001b[0m | \u001b[0m 3.599   \u001b[0m | \u001b[0m 0.00695 \u001b[0m | \u001b[0m 128.9   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6163\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.3533\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.2044\n","| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5663  \u001b[0m | \u001b[0m 371.6   \u001b[0m | \u001b[0m 0.2504  \u001b[0m | \u001b[0m 52.74   \u001b[0m | \u001b[0m 4.25    \u001b[0m | \u001b[0m 0.00989 \u001b[0m | \u001b[0m 237.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6104\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6097\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5005\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.2057\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.1563\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.9399\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.8952\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.9417\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.8355\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.8706\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7676\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.8287\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.7654\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.8347\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.7473\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.8486\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.7582\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.7626\n","| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7708  \u001b[0m | \u001b[0m 189.6   \u001b[0m | \u001b[0m 0.2368  \u001b[0m | \u001b[0m 65.48   \u001b[0m | \u001b[0m 3.344   \u001b[0m | \u001b[0m 0.009095\u001b[0m | \u001b[0m 123.4   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6148\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6088\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5433\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.5540\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5545\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.5542\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5364\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5633\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5360\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.5832\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5144\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.5844\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2833\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.0536\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.9582\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.7924\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.7518\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.7299\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.7058\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.6458\n","| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7843  \u001b[0m | \u001b[0m 192.9   \u001b[0m | \u001b[0m 0.03901 \u001b[0m | \u001b[0m 52.91   \u001b[0m | \u001b[0m 4.037   \u001b[0m | \u001b[0m 0.002195\u001b[0m | \u001b[0m 116.4   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6135\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6142\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.2882\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.0810\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.7860\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.9313\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.7143\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6496\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6957\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6275\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.6893\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5914\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.6423\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5788\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.6231\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5723\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.6390\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5775\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.6305\n","| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7914  \u001b[0m | \u001b[0m 284.2   \u001b[0m | \u001b[0m 0.01601 \u001b[0m | \u001b[0m 136.1   \u001b[0m | \u001b[0m 2.44    \u001b[0m | \u001b[0m 0.005934\u001b[0m | \u001b[0m 224.9   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6112\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5283\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4857\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5200\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5569\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5551\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.5513\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4681\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4745\n","| \u001b[0m 11      \u001b[0m | \u001b[0m 0.314   \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6101\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6068\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5068\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4537\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4767\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5177\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6097\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6091\n","| \u001b[0m 12      \u001b[0m | \u001b[0m 0.2622  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6126\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5349\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.1915\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.0666\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.9302\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.8024\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7394\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6320\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5951\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5773\n","| \u001b[0m 13      \u001b[0m | \u001b[0m 0.784   \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6131\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.4980\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.3554\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.2644\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.0418\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.8468\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6704\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.6498\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5847\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6199\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5418\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5974\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5449\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5677\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5034\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5706\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4899\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.5470\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4806\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5749\n","| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7897  \u001b[0m | \u001b[0m 250.9   \u001b[0m | \u001b[0m 0.04486 \u001b[0m | \u001b[0m 51.77   \u001b[0m | \u001b[0m 4.316   \u001b[0m | \u001b[0m 0.002851\u001b[0m | \u001b[0m 67.65   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6055\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6096\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6049\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6112\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6033\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6137\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4878\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.9657\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.8801\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.6556\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.7035\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5932\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6866\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.5875\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5856\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5906\n","| \u001b[95m 15      \u001b[0m | \u001b[95m 0.7936  \u001b[0m | \u001b[95m 214.2   \u001b[0m | \u001b[95m 0.3     \u001b[0m | \u001b[95m 200.0   \u001b[0m | \u001b[95m 2.0     \u001b[0m | \u001b[95m 0.002899\u001b[0m | \u001b[95m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6240\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6110\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6097\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6097\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6162\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6097\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6077\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6089\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6093\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6099\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6159\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.6072\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6074\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6083\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6090\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6099\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.6158\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.6095\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6076\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.1946\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.1092\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 0.9041\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 0.8617\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 0.9528\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.8094\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.7311\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.7351\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 0.7472\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 0.6918\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 0.7591\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.8101\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.7436\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.6090\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 0.6280\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 0.7365\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 0.7628\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7544\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.7554\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.5538\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.6121\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 0.6427\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.7314\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.7558\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.7663\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.5338\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.6612\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.7450\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.6139\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6152\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.7521\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.5250\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.6898\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 0.7289\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.7212\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.7435\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.6782\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.5016\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.6402\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.6256\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.6614\n","| \u001b[0m 16      \u001b[0m | \u001b[0m 0.776   \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.01025 \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.004105\u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6095\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6077\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6085\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6099\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6139\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6093\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6083\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6088\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.4139\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.4364\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.4072\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6029\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.2519\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.3672\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.5541\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.5818\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.6075\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6077\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6114\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6109\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.5994\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.6182\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.6134\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6317\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6107\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.3354\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.4713\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.4512\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.2269\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4966\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.1365\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.1204\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.3731\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.3548\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.2275\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2519\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.9098\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.8329\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.1170\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.0636\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.9622\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.1070\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.2997\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.7304\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.0749\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.8974\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.8904\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.0602\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.9349\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.6850\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.9010\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.0706\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.7595\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.9268\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.7943\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.5941\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.8503\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.7645\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.7426\n","| \u001b[0m 17      \u001b[0m | \u001b[0m 0.7302  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 126.9   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.000100\u001b[0m | \u001b[0m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6060\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6114\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6121\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6068\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6136\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6129\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6069\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6125\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.4640\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5076\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.2959\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.2681\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.2630\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.3431\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.2087\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.1799\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.2641\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.9821\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.0735\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.0792\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.8734\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.8948\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.9371\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.7572\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.8392\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.9426\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.7004\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.8560\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.9814\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.7116\n","| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7248  \u001b[0m | \u001b[0m 131.2   \u001b[0m | \u001b[0m 0.1101  \u001b[0m | \u001b[0m 173.4   \u001b[0m | \u001b[0m 2.671   \u001b[0m | \u001b[0m 0.008217\u001b[0m | \u001b[0m 224.4   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5104\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4914\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.3907\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.1133\n","| \u001b[0m 19      \u001b[0m | \u001b[0m 0.6823  \u001b[0m | \u001b[0m 352.2   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.000214\u001b[0m | \u001b[0m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6120\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6095\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4270\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.2947\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.1989\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.1298\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.9439\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.8990\n","| \u001b[0m 20      \u001b[0m | \u001b[0m 0.6704  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 171.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6107\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6092\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6092\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4694\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5405\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5947\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5895\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4344\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4627\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.3978\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4404\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4395\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4576\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.5212\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.5203\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6055\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6008\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.5981\n","| \u001b[0m 21      \u001b[0m | \u001b[0m 0.214   \u001b[0m | \u001b[0m 333.8   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.000147\u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6116\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6091\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6081\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6075\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6069\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6089\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.2097\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2468\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.0866\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0625\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.9770\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.9464\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.8412\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.8380\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.8355\n","| \u001b[0m 22      \u001b[0m | \u001b[0m 0.6863  \u001b[0m | \u001b[0m 262.8   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 89.07   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6246\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6076\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6089\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6099\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6138\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6088\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6079\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6085\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6092\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.4698\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.5134\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.2817\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.4648\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4525\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6107\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6096\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.6180\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.6093\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6107\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.3335\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.4528\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.7686\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.5972\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5870\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4890\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.4777\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.4491\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.4558\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.3601\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4585\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.3986\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.3569\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.4100\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.4357\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.3562\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4647\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4313\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.3314\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.6042\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.4095\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 1.3639\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4681\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.2899\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.3217\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.3957\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 1.4298\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 1.3359\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4494\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.2151\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.3744\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.3583\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.3269\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 1.3038\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4178\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.5359\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.2784\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.3134\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 1.3137\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 1.4094\n","| \u001b[0m 23      \u001b[0m | \u001b[0m 0.3484  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 116.6   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 180.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6116\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6117\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6098\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6094\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6162\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6104\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6080\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6103\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.2885\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.3627\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.4243\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.2143\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.3283\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.9578\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.9351\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 0.7024\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 0.7361\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 0.7484\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.9117\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.6779\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.6089\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 0.6009\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 0.6188\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 0.6189\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7879\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6346\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.4719\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 0.5854\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 0.5959\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 0.6395\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.7479\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5276\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.4473\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 0.5591\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 0.5134\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 0.7104\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7797\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5564\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.3737\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.5295\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 0.5540\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.6392\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.7487\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4620\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.2976\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.5075\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.5036\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.6288\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6808\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4184\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.3534\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.5084\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 0.4784\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.5955\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6911\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.4374\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.3025\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.5337\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.4730\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.5310\n","| \u001b[95m 24      \u001b[0m | \u001b[95m 0.8014  \u001b[0m | \u001b[95m 64.42   \u001b[0m | \u001b[95m 0.164   \u001b[0m | \u001b[95m 126.3   \u001b[0m | \u001b[95m 4.827   \u001b[0m | \u001b[95m 0.001149\u001b[0m | \u001b[95m 298.8   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6101\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6218\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6095\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6106\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4196\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4260\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.2563\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.1192\n","| \u001b[0m 25      \u001b[0m | \u001b[0m 0.5941  \u001b[0m | \u001b[0m 397.1   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6078\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6105\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6138\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6109\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6114\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6084\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.3823\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4954\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6112\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6089\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6070\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.6089\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6058\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.4888\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4744\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.3708\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.4207\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4270\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.5129\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.3453\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4502\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4408\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.3754\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4940\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4256\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.5056\n","| \u001b[0m 26      \u001b[0m | \u001b[0m 0.3451  \u001b[0m | \u001b[0m 147.1   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 124.8   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.2707\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.8667\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7941\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.8862\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.7594\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.7598\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6958\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6806\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6287\n","| \u001b[0m 27      \u001b[0m | \u001b[0m 0.7704  \u001b[0m | \u001b[0m 411.7   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.009021\u001b[0m | \u001b[0m 181.4   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6137\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6074\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6081\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6146\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.3817\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.4983\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.5326\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.3506\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.4705\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.5030\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.3375\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5424\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.5661\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6033\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6087\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.5923\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.5902\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5883\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6006\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.6165\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.5964\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.2945\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5222\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.5397\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.2631\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.3688\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.4363\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.0909\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4068\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.9727\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.0021\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.0910\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.1979\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.0925\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2853\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.8877\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.9793\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.0180\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 0.9654\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 1.1153\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.1129\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.6771\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.7211\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.8141\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.6788\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.8117\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.8533\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.5693\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.6246\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.5138\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 0.5467\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.6583\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.8137\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5100\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.6012\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.5875\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.6177\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.6168\n","| \u001b[0m 28      \u001b[0m | \u001b[0m 0.7911  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6095\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6077\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.2880\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 0.9033\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.8850\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.7054\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 0.6201\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7181\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5672\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 0.5212\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6803\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5848\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 0.5683\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6513\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5628\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.5249\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6426\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4886\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.5015\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6104\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5352\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.5214\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6145\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4985\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.4972\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6161\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5131\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.4712\n","| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7943  \u001b[0m | \u001b[0m 153.1   \u001b[0m | \u001b[0m 0.2684  \u001b[0m | \u001b[0m 104.9   \u001b[0m | \u001b[0m 4.52    \u001b[0m | \u001b[0m 0.002159\u001b[0m | \u001b[0m 299.3   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6143\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6187\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6128\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6078\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6078\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6080\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5697\n","| \u001b[0m 30      \u001b[0m | \u001b[0m 0.4306  \u001b[0m | \u001b[0m 511.8   \u001b[0m | \u001b[0m 0.2573  \u001b[0m | \u001b[0m 53.09   \u001b[0m | \u001b[0m 3.74    \u001b[0m | \u001b[0m 0.000272\u001b[0m | \u001b[0m 294.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6096\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6066\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6062\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6060\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5276\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.7273\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4801\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4474\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.3808\n","| \u001b[0m 31      \u001b[0m | \u001b[0m 0.3635  \u001b[0m | \u001b[0m 433.2   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 116.7   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 150.5   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6064\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6081\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6049\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6084\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4206\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5474\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.4058\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5058\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4115\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6083\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.5988\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6016\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6013\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6043\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.5271\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.3874\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.2299\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.2670\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.1301\n","| \u001b[0m 32      \u001b[0m | \u001b[0m 0.5061  \u001b[0m | \u001b[0m 276.9   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 173.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.3760\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7055\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6033\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5861\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5795\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5756\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5451\n","| \u001b[0m 33      \u001b[0m | \u001b[0m 0.7988  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.005279\u001b[0m | \u001b[0m 181.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6122\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6063\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5109\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4044\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4501\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4791\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4139\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0586\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.7255\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6497\n","| \u001b[0m 34      \u001b[0m | \u001b[0m 0.7802  \u001b[0m | \u001b[0m 388.4   \u001b[0m | \u001b[0m 0.1797  \u001b[0m | \u001b[0m 123.7   \u001b[0m | \u001b[0m 3.326   \u001b[0m | \u001b[0m 0.000964\u001b[0m | \u001b[0m 299.7   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6039\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6210\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6259\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6048\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6355\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6116\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6118\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6181\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6214\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6007\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6464\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.6056\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6152\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6093\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6174\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6038\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.6399\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.6088\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6134\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6119\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6109\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.6083\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.6357\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.5963\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6226\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6061\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.3038\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.4055\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.3756\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.2404\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.3844\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.1056\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.1642\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.1791\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.2572\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.1829\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2828\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.1174\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.9795\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.9480\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.0023\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 0.8985\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0585\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.7842\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.8650\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.7754\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 0.9866\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 0.8901\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.8618\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.7696\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.8250\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.7834\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 0.9460\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 0.9518\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.9196\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.7799\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.7677\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.6368\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 0.7537\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 0.8496\n","| \u001b[0m 35      \u001b[0m | \u001b[0m 0.7145  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6120\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6112\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6098\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6097\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6100\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4702\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4535\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.5702\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5517\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4803\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.3828\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.4453\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4252\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4035\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.3604\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4479\n","| \u001b[0m 36      \u001b[0m | \u001b[0m 0.3647  \u001b[0m | \u001b[0m 250.8   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 76.8    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 175.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6066\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6086\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6075\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6141\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6107\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6111\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6074\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6095\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6082\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6123\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6106\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.6108\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6093\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6082\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6125\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.6107\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.6108\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6082\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.6128\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.6105\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.6107\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6076\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6093\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.6122\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.6105\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.6109\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6073\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.6082\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.6127\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.6105\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.6109\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6077\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4488\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.2691\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.1257\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.1923\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 1.2666\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.3731\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.1331\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.1983\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.1437\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 1.2904\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 1.3125\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4248\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.1631\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.1193\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.1488\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.1741\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 1.2019\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.2921\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.0282\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.0953\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.9903\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 1.1083\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 1.1507\n","| \u001b[0m 37      \u001b[0m | \u001b[0m 0.5477  \u001b[0m | \u001b[0m 65.91   \u001b[0m | \u001b[0m 0.131   \u001b[0m | \u001b[0m 55.38   \u001b[0m | \u001b[0m 4.762   \u001b[0m | \u001b[0m 0.007531\u001b[0m | \u001b[0m 297.7   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6130\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6089\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6071\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6072\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.3836\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4887\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5963\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5893\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4107\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4614\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.4849\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4560\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6010\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.5901\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.5501\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.5642\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4016\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.5654\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.5599\n","| \u001b[0m 38      \u001b[0m | \u001b[0m 0.2731  \u001b[0m | \u001b[0m 280.9   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 159.3   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.3865\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.2772\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.9923\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7012\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6770\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6253\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.6176\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.6313\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.6146\n","| \u001b[0m 39      \u001b[0m | \u001b[0m 0.7784  \u001b[0m | \u001b[0m 422.9   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 254.4   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6130\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6014\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6061\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.4660\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5265\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.5223\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.4454\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5085\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.4335\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.4216\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5559\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4600\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.4658\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4866\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.4316\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.5188\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5498\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.4780\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.4905\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.3808\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.4687\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.4178\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4194\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.4002\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.4272\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4739\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4766\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.3965\n","| \u001b[0m 40      \u001b[0m | \u001b[0m 0.3426  \u001b[0m | \u001b[0m 138.3   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.000100\u001b[0m | \u001b[0m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6102\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6098\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6096\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.2328\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.1156\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.0236\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.9403\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.8597\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.8696\n","| \u001b[0m 41      \u001b[0m | \u001b[0m 0.6821  \u001b[0m | \u001b[0m 371.3   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6133\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6115\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6097\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6100\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6164\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6101\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6072\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6093\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6018\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6085\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6297\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.6151\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6069\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6066\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6119\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6290\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.6083\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.5937\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5933\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.5830\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.3257\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.3642\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.4693\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.1119\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4941\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.2060\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.2262\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.4496\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.3337\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.1026\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4336\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.1095\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.1806\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.3535\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.2320\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.2810\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4385\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.9740\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.2435\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.3333\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.2692\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 1.1148\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4219\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.9970\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.2119\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.3695\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 1.1326\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 1.1524\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.3161\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.0893\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.2892\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.3151\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.3389\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 1.0437\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4183\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.0599\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.3078\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.2179\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 1.1422\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 1.0482\n","| \u001b[0m 42      \u001b[0m | \u001b[0m 0.5147  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.005495\u001b[0m | \u001b[0m 220.9   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6099\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6098\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6096\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6074\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6095\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.3267\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.3603\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.4356\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.2605\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.1450\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.1633\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.1571\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.1419\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.0812\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.0972\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.9750\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.9950\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.9632\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.0351\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.9841\n","| \u001b[0m 43      \u001b[0m | \u001b[0m 0.6425  \u001b[0m | \u001b[0m 200.2   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6130\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6090\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6095\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6097\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6094\n","| \u001b[0m 44      \u001b[0m | \u001b[0m 0.2004  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 127.2   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 219.6   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6113\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6095\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6089\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4660\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4801\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5862\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4699\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4970\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4953\n","| \u001b[0m 45      \u001b[0m | \u001b[0m 0.2017  \u001b[0m | \u001b[0m 347.8   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 226.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6131\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6111\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6126\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6134\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6134\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6116\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6128\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6124\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6108\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6117\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6140\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6066\n","| \u001b[0m 46      \u001b[0m | \u001b[0m 0.2005  \u001b[0m | \u001b[0m 212.8   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6067\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6084\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.8927\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7953\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.5773\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.7442\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.5509\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6472\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5234\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.6275\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.4866\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5692\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.4562\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5687\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.4298\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5560\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.4650\n","| \u001b[95m 47      \u001b[0m | \u001b[95m 0.8044  \u001b[0m | \u001b[95m 216.5   \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 154.3   \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 0.003254\u001b[0m | \u001b[95m 227.1   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6116\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6092\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6038\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6084\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6095\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6021\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6084\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6089\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6037\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6084\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6094\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.3297\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4396\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.2662\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.1460\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.2166\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.0039\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 0.9025\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.8984\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.8492\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.7614\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.1790\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.8169\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.8997\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.9611\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.7987\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.9523\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.9507\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.8196\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.7985\n","| \u001b[0m 48      \u001b[0m | \u001b[0m 0.6869  \u001b[0m | \u001b[0m 153.4   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6117\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6097\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6091\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6087\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5639\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4204\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.3164\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.3243\n","| \u001b[0m 49      \u001b[0m | \u001b[0m 0.5012  \u001b[0m | \u001b[0m 458.0   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 124.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6120\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6086\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.4895\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.4251\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.8094\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.8331\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.6655\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6229\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6005\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6098\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5777\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5702\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5760\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5340\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5398\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5122\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.5084\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5078\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5009\n","| \u001b[95m 50      \u001b[0m | \u001b[95m 0.8079  \u001b[0m | \u001b[95m 310.3   \u001b[0m | \u001b[95m 0.2355  \u001b[0m | \u001b[95m 123.1   \u001b[0m | \u001b[95m 3.047   \u001b[0m | \u001b[95m 0.002983\u001b[0m | \u001b[95m 104.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6177\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.4794\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4525\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4539\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5483\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.5544\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.5413\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4693\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6300\n","| \u001b[0m 51      \u001b[0m | \u001b[0m 0.2016  \u001b[0m | \u001b[0m 356.5   \u001b[0m | \u001b[0m 0.1891  \u001b[0m | \u001b[0m 51.17   \u001b[0m | \u001b[0m 4.109   \u001b[0m | \u001b[0m 0.000919\u001b[0m | \u001b[0m 120.8   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6189\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6077\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6088\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5359\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4853\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.5332\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.4785\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.5195\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.4618\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5220\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.4042\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4416\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.3898\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4697\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.3481\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.7343\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6066\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6039\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.5940\n","| \u001b[0m 52      \u001b[0m | \u001b[0m 0.2323  \u001b[0m | \u001b[0m 237.2   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 142.4   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 122.2   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6146\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6086\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6082\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.3595\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.3551\n","Train Epoch: 02 -- Batch: 500 -- Loss: 0.8055\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.7029\n","Train Epoch: 03 -- Batch: 500 -- Loss: 0.6458\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5642\n","Train Epoch: 04 -- Batch: 500 -- Loss: 0.6071\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5463\n","Train Epoch: 05 -- Batch: 500 -- Loss: 0.5885\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5045\n","Train Epoch: 06 -- Batch: 500 -- Loss: 0.5650\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4956\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.5447\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4976\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.5128\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5036\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.5011\n","| \u001b[0m 53      \u001b[0m | \u001b[0m 0.8048  \u001b[0m | \u001b[0m 326.8   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 98.33   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001862\u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6086\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6093\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6097\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6095\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6107\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4975\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.3480\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.2236\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.1926\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.0252\n","| \u001b[0m 54      \u001b[0m | \u001b[0m 0.6759  \u001b[0m | \u001b[0m 422.5   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 300.0   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6142\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6084\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6062\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6116\n","Train Epoch: 00 -- Batch: 2000 -- Loss: 1.6149\n","Train Epoch: 00 -- Batch: 2500 -- Loss: 1.6090\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6083\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6098\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6094\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6107\n","Train Epoch: 01 -- Batch: 2000 -- Loss: 1.6168\n","Train Epoch: 01 -- Batch: 2500 -- Loss: 1.4059\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.4926\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.4415\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.3140\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.4220\n","Train Epoch: 02 -- Batch: 2000 -- Loss: 1.4921\n","Train Epoch: 02 -- Batch: 2500 -- Loss: 1.5492\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4667\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.3457\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.4445\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.4465\n","Train Epoch: 03 -- Batch: 2000 -- Loss: 1.4545\n","Train Epoch: 03 -- Batch: 2500 -- Loss: 1.5451\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.4852\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.3819\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.3909\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.4265\n","Train Epoch: 04 -- Batch: 2000 -- Loss: 1.5022\n","Train Epoch: 04 -- Batch: 2500 -- Loss: 1.5046\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.5482\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.5669\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.4109\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.4036\n","Train Epoch: 05 -- Batch: 2000 -- Loss: 1.4742\n","Train Epoch: 05 -- Batch: 2500 -- Loss: 1.3916\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4358\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.2370\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.2749\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.4373\n","Train Epoch: 06 -- Batch: 2000 -- Loss: 1.3924\n","Train Epoch: 06 -- Batch: 2500 -- Loss: 1.3526\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4476\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.2514\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.3684\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.3866\n","Train Epoch: 07 -- Batch: 2000 -- Loss: 1.3998\n","Train Epoch: 07 -- Batch: 2500 -- Loss: 1.2728\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4443\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.2469\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.2889\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.3900\n","Train Epoch: 08 -- Batch: 2000 -- Loss: 1.5394\n","Train Epoch: 08 -- Batch: 2500 -- Loss: 1.4080\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4956\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.4824\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.3770\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.5442\n","Train Epoch: 09 -- Batch: 2000 -- Loss: 1.5413\n","Train Epoch: 09 -- Batch: 2500 -- Loss: 1.3722\n","| \u001b[0m 55      \u001b[0m | \u001b[0m 0.296   \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 122.1   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6055\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6083\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6075\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6106\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6074\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.4892\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.4628\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.4680\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.5317\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.5317\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.4011\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.3905\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.4694\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.2687\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.2967\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.3177\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.3528\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.2649\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.2282\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.2619\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.3140\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.1589\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.1539\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.3336\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.2786\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.0920\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 0.9586\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 0.9120\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.9028\n","Train Epoch: 07 -- Batch: 500 -- Loss: 0.6273\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 0.6990\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 0.7531\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.8010\n","Train Epoch: 08 -- Batch: 500 -- Loss: 0.5411\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 0.6292\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 0.7328\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.7542\n","Train Epoch: 09 -- Batch: 500 -- Loss: 0.4616\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 0.6035\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 0.6828\n","| \u001b[0m 56      \u001b[0m | \u001b[0m 0.7987  \u001b[0m | \u001b[0m 93.6    \u001b[0m | \u001b[0m 0.2229  \u001b[0m | \u001b[0m 50.06   \u001b[0m | \u001b[0m 2.702   \u001b[0m | \u001b[0m 0.000878\u001b[0m | \u001b[0m 102.4   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6111\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6094\n","Train Epoch: 02 -- Batch: 000 -- Loss: 0.8808\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6679\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.6340\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.6024\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.5946\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.5983\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.5802\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.5653\n","| \u001b[0m 57      \u001b[0m | \u001b[0m 0.7901  \u001b[0m | \u001b[0m 434.5   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 120.3   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6153\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.5936\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.3162\n","Train Epoch: 03 -- Batch: 000 -- Loss: 0.6422\n","Train Epoch: 04 -- Batch: 000 -- Loss: 0.5657\n","Train Epoch: 05 -- Batch: 000 -- Loss: 0.5230\n","Train Epoch: 06 -- Batch: 000 -- Loss: 0.4994\n","Train Epoch: 07 -- Batch: 000 -- Loss: 0.4746\n","Train Epoch: 08 -- Batch: 000 -- Loss: 0.4728\n","Train Epoch: 09 -- Batch: 000 -- Loss: 0.4410\n","| \u001b[0m 58      \u001b[0m | \u001b[0m 0.7993  \u001b[0m | \u001b[0m 511.5   \u001b[0m | \u001b[0m 0.2767  \u001b[0m | \u001b[0m 104.1   \u001b[0m | \u001b[0m 3.26    \u001b[0m | \u001b[0m 0.00216 \u001b[0m | \u001b[0m 101.7   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6135\n","Train Epoch: 00 -- Batch: 500 -- Loss: 1.6081\n","Train Epoch: 00 -- Batch: 1000 -- Loss: 1.6082\n","Train Epoch: 00 -- Batch: 1500 -- Loss: 1.6110\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 01 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 01 -- Batch: 1000 -- Loss: 1.6083\n","Train Epoch: 01 -- Batch: 1500 -- Loss: 1.6110\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 02 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 02 -- Batch: 1000 -- Loss: 1.6082\n","Train Epoch: 02 -- Batch: 1500 -- Loss: 1.6110\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 03 -- Batch: 500 -- Loss: 1.6084\n","Train Epoch: 03 -- Batch: 1000 -- Loss: 1.6082\n","Train Epoch: 03 -- Batch: 1500 -- Loss: 1.6110\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 04 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 04 -- Batch: 1000 -- Loss: 1.6088\n","Train Epoch: 04 -- Batch: 1500 -- Loss: 1.6110\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 05 -- Batch: 500 -- Loss: 1.6084\n","Train Epoch: 05 -- Batch: 1000 -- Loss: 1.6081\n","Train Epoch: 05 -- Batch: 1500 -- Loss: 1.6110\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.6077\n","Train Epoch: 06 -- Batch: 500 -- Loss: 1.6083\n","Train Epoch: 06 -- Batch: 1000 -- Loss: 1.6105\n","Train Epoch: 06 -- Batch: 1500 -- Loss: 1.6110\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 07 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 07 -- Batch: 1000 -- Loss: 1.6083\n","Train Epoch: 07 -- Batch: 1500 -- Loss: 1.6110\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 08 -- Batch: 500 -- Loss: 1.6083\n","Train Epoch: 08 -- Batch: 1000 -- Loss: 1.6082\n","Train Epoch: 08 -- Batch: 1500 -- Loss: 1.6110\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.6075\n","Train Epoch: 09 -- Batch: 500 -- Loss: 1.6082\n","Train Epoch: 09 -- Batch: 1000 -- Loss: 1.6082\n","Train Epoch: 09 -- Batch: 1500 -- Loss: 1.6110\n","| \u001b[0m 59      \u001b[0m | \u001b[0m 0.2004  \u001b[0m | \u001b[0m 97.01   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 121.8   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 255.6   \u001b[0m |\n","Train Epoch: 00 -- Batch: 000 -- Loss: 1.6085\n","Train Epoch: 01 -- Batch: 000 -- Loss: 1.6098\n","Train Epoch: 02 -- Batch: 000 -- Loss: 1.6092\n","Train Epoch: 03 -- Batch: 000 -- Loss: 1.6078\n","Train Epoch: 04 -- Batch: 000 -- Loss: 1.6070\n","Train Epoch: 05 -- Batch: 000 -- Loss: 1.4623\n","Train Epoch: 06 -- Batch: 000 -- Loss: 1.4993\n","Train Epoch: 07 -- Batch: 000 -- Loss: 1.4867\n","Train Epoch: 08 -- Batch: 000 -- Loss: 1.4610\n","Train Epoch: 09 -- Batch: 000 -- Loss: 1.4857\n","| \u001b[0m 60      \u001b[0m | \u001b[0m 0.3174  \u001b[0m | \u001b[0m 512.0   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 115.5   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.0001  \u001b[0m | \u001b[0m 50.0    \u001b[0m |\n","=================================================================================================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ORw_5n0Fpte4"},"source":["**n-gram language model**\n"]},{"cell_type":"code","metadata":{"id":"8ejo5ByFphUm"},"source":["from nltk import bigrams, trigrams\n","from collections import Counter, defaultdict\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Sp1omBYpyNf"},"source":["def build_bigram_language_model(subreddit):\n","  model = defaultdict(lambda: defaultdict(lambda: 0))\n","  posts = train_df_balanced[train_df_balanced['subreddit'] == subreddit]['preprocessed_selftext'].values\n","  tokenised_posts = []\n","  for i in range(0, len(posts)):\n","    words = posts[i].split(' ')\n","    tokenised_posts.append(words)\n","\n","  vocabulary = {}\n","  for post in tokenised_posts:\n","    for word in post:\n","      if word not in vocabulary:\n","        vocabulary[word] = 1\n","\n","  for post in tokenised_posts:\n","    for w1, w2 in bigrams(post, pad_right=True, pad_left=True):\n","      model[w1][w2] += 1\n","\n","  for w1 in model:\n","      total_count = float(sum(model[w1].values()))\n","      for w2 in model[w1]:\n","        model[w1][w2] = (model[w1][w2] + 1) / (total_count + len(vocabulary))\n","\n","  return model, vocabulary\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-LR2aq1pp0Ee"},"source":["def build_trigram_language_model(subreddit):\n","  model = defaultdict(lambda: defaultdict(lambda: 0))\n","  posts = train_df_balanced[train_df_balanced['subreddit'] == subreddit]['preprocessed_selftext'].values\n","  tokenised_posts = []\n","  for i in range(0, len(posts)):\n","    words = posts[i].split(' ')\n","    tokenised_posts.append(words)\n","\n","  vocabulary = {}\n","  for post in tokenised_posts:\n","    for word in post:\n","      if word not in vocabulary:\n","        vocabulary[word] = 1\n","\n","  for post in tokenised_posts:\n","    for w1, w2, w3 in trigrams(post, pad_right=True, pad_left=True):\n","      model[(w1, w2)][w3] += 1\n","\n","  for w1_w2 in model:\n","      total_count = float(sum(model[w1_w2].values()))\n","      for w3 in model[w1_w2]:\n","        model[w1_w2][w3] = (model[w1_w2][w3] + 1) / (total_count + len(vocabulary))\n","\n","  return model, vocabulary\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hlmv5l7yp1KC"},"source":["control_model_b, control_vocab_b = build_bigram_language_model('control')\n","depression_model_b, depression_vocab_b = build_bigram_language_model('depression')\n","anxiety_model_b, anxiety_vocab_b = build_bigram_language_model('anxiety')\n","bpd_model_b, bpd_vocab_b = build_bigram_language_model('bpd')\n","bipolar_model_b, bipolar_vocab_b = build_bigram_language_model('bipolar')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"we__MVj5p1Of"},"source":["control_model_t, control_vocab_t = build_trigram_language_model('control')\n","depression_model_t, depression_vocab_t = build_trigram_language_model('depression')\n","anxiety_model_t, anxiety_vocab_t = build_trigram_language_model('anxiety')\n","bpd_model_t, bpd_vocab_t = build_trigram_language_model('bpd')\n","bipolar_model_t, bipolar_vocab_t = build_trigram_language_model('bipolar')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cKApMnsMp3Jq"},"source":["import math\n","\n","def get_log_prob_b(post, model, vocabulary):\n","  log_prob = 0\n","  words = post.split(' ')\n","  if len(words) < 2:\n","    return 0\n","  for i in range(1, len(words)):\n","    if words[i - 1] in model and words[i] in model[words[i - 1]]:\n","      log_prob += math.log(model[words[i - 1]][words[i]])\n","    else:\n","      log_prob += math.log(1 / len(vocabulary))\n","\n","  return log_prob"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Snj-Cm7Ip4Fp"},"source":["def get_log_prob_t(post, model, vocabulary):\n","  log_prob = 0\n","  words = post.split(' ')\n","  if len(words) < 3:\n","    return 0\n","  for i in range(2, len(words)):\n","    if (words[i - 2], words[i - 1]) in model and words[i] in model[(words[i - 2], words[i - 1])]:\n","      log_prob += math.log(model[(words[i - 2], words[i - 1])][words[i]])\n","    else:\n","      log_prob += math.log(1 / len(vocabulary))\n","\n","  return log_prob"],"execution_count":null,"outputs":[]}]}